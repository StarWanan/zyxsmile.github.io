{"title":"一类嵌入的反向蒸馏","uid":"082602022df518a2faf6c62dcd303051","slug":"一类嵌入反向蒸馏","date":"2022-05-04T03:30:00.000Z","updated":"2022-05-04T03:30:26.007Z","comments":true,"path":"api/articles/一类嵌入反向蒸馏.json","keywords":null,"cover":null,"content":"<h1\nid=\"anomaly-detection-via-reverse-distillation-from-one-class-embedding\">Anomaly\nDetection via Reverse Distillation from One-Class Embedding</h1>\n<p>通过一类嵌入的反向蒸馏进行异常检测</p>\n<span id=\"more\"></span>\n<h2 id=\"abstract\">Abstract</h2>\n<p>Knowledge distillation (KD) achieves promising results on the\nchallenging problem of unsupervised anomaly detection (AD). The\nrepresentation discrepancy of anomalies in the teacher-student (T-S)\nmodel provides essential evidence for AD.</p>\n<p>However, using similar or identical architectures to build the\nteacher and student models in previous stud- ies hinders the diversity\nof anomalous representations. To tackle this problem, we propose a novel\nT-S model consisting of a teacher encoder and a student decoder and\nintroduce a simple yet effective <strong>“reverse distillation”\nparadigm</strong> accordingly.</p>\n<p>Instead of receiving raw images directly, the student network takes\nteacher model’s one-class embedding as input and targets to restore the\nteacher’s multi-scale representations.</p>\n<p>Inherently, knowledge distillation in this study starts from\nabstract, high-level presentations to low-level features.</p>\n<p>In addition, we introduce a trainable <strong>one-class bottleneck\nembedding (OCBE) module</strong> in our T-S model.</p>\n<p>The obtained compact embedding effectively preserves essential\ninformation on normal patterns, but abandons anomaly perturbations.</p>\n<p>Extensive experimentation on AD and one-class novelty detection\nbenchmarks shows that our method surpasses SOTA performance,\ndemonstrating our proposed approach’s effectiveness and\ngeneralizability.</p>\n<hr />\n<p>知识蒸馏（KD）在无监督异常检测（AD）的挑战性问题上取得了可喜的成果。\n师生（T-S）模型中异常的表示差异为AD提供了必要的证据。</p>\n<p>然而，在以前的研究中使用相似或相同的架构来构建教师和学生模型阻碍了异常表示的多样性。\n为了解决这个问题，我们提出了一种由教师编码器和学生解码器组成的新型 T-S\n模型，并相应地引入了一种简单而有效的“逆向蒸馏”范式。</p>\n<p>学生网络不是直接接收原始图像，而是将教师模型的一类嵌入作为输入和目标，以恢复教师的多尺度表示。</p>\n<p>本质上，本研究中的知识蒸馏从抽象的高级表示开始到低级特征。</p>\n<p>此外，我们在 T-S 模型中引入了可训练的一类瓶颈嵌入 (OCBE) 模块。</p>\n<p>获得的紧凑嵌入有效地保留了正常模式的基本信息，但放弃了异常扰动。</p>\n<p>对 AD 和一类新颖性检测基准的广泛实验表明，我们的方法超越了 SOTA\n性能，证明了我们提出的方法的有效性和普遍性。</p>\n<h2 id=\"introduction\">1. Introduction</h2>\n<p>Anomaly detection (AD) refers to <strong>identifying and localizing\nanomalies</strong> with limited, even no, prior knowledge of\nabnormality.</p>\n<p>The wide applications of AD, such as indus- trial defect detection\n[3], medical out-of-distribution detection [50], and video surveillance\n[24], makes it a critical task as well as a spotlight. In the context of\n<strong>unsupervised AD</strong>, no prior information on anomalies is\navailable. Instead, a set of normal samples is provided for\nreference.</p>\n<p>To tackle this problem, previous efforts attempt to construct various\nself-supervision tasks on those anomaly-free samples. These tasks\ninclude, but not limited to, sample reconstruction [2, 5, 11, 16, 26,\n34, 38, 48], pseudo-outlier augmentation [23, 42, 46], knowledge\ndistillation [4, 33, 39], etc.</p>\n<p>In this study, we tackle the problem of unsupervised anomaly\ndetection from the knowledge distillation-based point of view.</p>\n<p>In <strong>knowledge distillation (KD)</strong> [6, 15], knowledge is\ntransferred within a teacher-student (T-S) pair. In the context of\nunsupervised AD, since the student experiences only normal samples\nduring training, it is likely to generate discrepant representations\nfrom the teacher when a query is anomalous. This hypothesis forms the\nbasis of KD-based methods for anomaly detection.</p>\n<p>However, this hypothesis is not always true in practice due to</p>\n<ol type=\"1\">\n<li><p>the identical or similar architectures of the teacher and student\nnetworks (i.e., non-distinguishing filters [33])</p></li>\n<li><p>the same data flow in the T-S model during knowledge trans-\nfer/distillation.</p></li>\n</ol>\n<p>Though the use of a smaller student network partially addresses this\nissue [33, 39], the weaker represen- tation capability of shallow\narchitectures hinders the model from precisely detecting and localizing\nanomalies.</p>\n<hr />\n<p>异常检测 (AD)\n是指在对异常的先验知识有限甚至没有的情况下识别和定位异常。</p>\n<p>AD 的广泛应用，如工业缺陷检测 [3]、医疗分布外检测 [50] 和视频监控\n[24]，使其成为一项关键任务和聚光灯。 在无监督 AD\n的背景下，没有关于异常的先验信息可用。\n相反，提供了一组正常样本以供参考。</p>\n<p>为了解决这个问题，以前的努力试图在那些无异常的样本上构建各种自我监督任务。\n这些任务包括但不限于样本重建[2、5、11、16、26、34、38、48]、伪异常值增强[23、42、46]、知识蒸馏[4、33、\n39]等。</p>\n<p>在这项研究中，我们从基于知识蒸馏的角度解决了无监督异常检测的问题。</p>\n<p>在知识蒸馏 (KD) [6, 15] 中，知识在师生 (T-S) 对中转移。 在无监督 AD\n的背景下，由于学生在训练期间只体验到正常样本，因此当查询异常时，它可能会从教师那里产生不一致的表示。\n该假设构成了基于 KD 的异常检测方法的基础。\n然而，这个假设在实践中并不总是正确的，因为</p>\n<ol type=\"1\">\n<li>教师和学生网络的相同或相似架构（即非区分过滤器 [33]）</li>\n<li>T-S 模型中的相同数据流 知识转移/蒸馏。</li>\n</ol>\n<p>尽管使用较小的学生网络部分解决了这个问题 [33,\n39]，但浅层架构较弱的表示能力阻碍了模型精确检测和定位异常。</p>\n<hr />\n<p>To holistically address the issue mentioned above, we propose a new\nparadigm of knowledge distillation, namely Reverse Distillation, for\nanomaly detection. We use sim- ple diagrams in Fig. 2 to highlight the\nsystematic differ- ence between conventional knowledge distillation and\nthe proposed reverse distillation. First, unlike the conventional\nknowledge distillation framework where both teacher and student adopt\nthe encoder structure, the T-S model in our reverse distillation\nconsists of heterogeneous architectures: a teacher encoder and a student\ndecoder. Second, instead of directly feeding the raw data to the T-S\nmodel simulta- neously, the student decoder takes the low-dimensional\nem- bedding as input, targeting to mimic the teacher’s behavior by\nrestoring the teacher model’s representations in different scales. From\nthe regression perspective, our reverse distil- lation uses the student\nnetwork to predict the representa- tion of the teacher model. Therefore,\n”reverse” here indi- cates both the reverse shapes of teacher encoder\nand stu- dent decoder and the distinct knowledge distillation order\nwhere high-level representation is first distilled, followed by\nlow-level features. It is noteworthy that our reverse distilla- tion\npresents two significant advantages: i) <em>Non-similarity</em>\nstructure. In the proposed T-S model, one can consider the teacher\nencoder as a down-sampling filter and the stu- dent decoder as an\nup-sampling filter. The ”reverse struc- tures” avoid the confusion\ncaused by non-distinguishing fil- ters [33] as we discussed above. ii)\n<em>Compactness embed- ding</em>. The low-dimensional embedding fed to\nthe student decoder acts as an information bottleneck for normal pat-\ntern restoration. Let’s formulate anomaly features as pertur- bations on\nnormal patterns. Then the compact embedding helps to prohibit the\npropagation of such unusual perturba- tions to the student model and\nthus boosts the T-S model’s representation discrepancy on anomalies.\nNotably, tradi- tional AE-based methods [5, 11, 16, 26] detect anomalies\nutilising pixel differences, whereas we perform discrimi- nation with\ndense descriptive features. Deep features as region-aware descriptors\nprovide more effective discrimi- native information than per-pixel in\nimages.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>为了全面解决上述问题，我们提出了一种新的知识蒸馏范式，即反向蒸馏，用于异常检测。\n我们使用图 2\n中的简单图表来突出传统知识蒸馏和提出的逆向蒸馏之间的系统差异。\n首先，与教师和学生都采用编码器结构的传统知识蒸馏框架不同，我们的逆向蒸馏中的\nT-S 模型由异构架构组成：教师编码器和学生解码器。\n其次，学生解码器不是直接将原始数据同时馈送到 T-S\n模型，而是将低维嵌入作为输入，旨在通过恢复教师模型在不同尺度上的表示来模仿教师的行为。\n从回归的角度来看，我们的反向蒸馏使用学生网络来预测教师模型的表示。\n因此，这里的“反向”表示教师编码器和学生解码器的反向形状以及不同的知识蒸馏顺序，其中首先蒸馏高级表示，然后是低级特征。\n值得注意的是，我们的逆向蒸馏具有两个显着优势：i）<em>非相似性</em>结构。\n在提出的 T-S\n模型中，可以将教师编码器视为下采样滤波器，将学生解码器视为上采样滤波器。\n正如我们上面讨论的，“反向结构”避免了由非区分过滤器[33]引起的混淆。 ii)\n<em>紧凑性嵌入</em>。\n馈送到学生解码器的低维嵌入充当了正常模式恢复的信息瓶颈。\n让我们将异常特征表述为对正常模式的扰动。\n然后紧凑嵌入有助于禁止这种不寻常的扰动传播到学生模型，从而提高 T-S\n模型对异常的表示差异。 值得注意的是，传统的基于 AE 的方法\n[5、11、16、26]\n利用像素差异检测异常，而我们使用密集的描述性特征进行区分。\n作为区域感知描述符的深度特征比图像中的每个像素提供更有效的判别信息。</p></blockquote>\n<p>In addition, since the compactness of the bottleneck em- bedding is\nvital for anomaly detection (as discussed above), we introduce a\none-class bottleneck embedding (OCBE) module to condense the feature\ncodes further. Our OCBE module consists of a multi-scale feature fusion\n(MFF) block and one-class embedding (OCE) block, both jointly opti-\nmized with the student decoder. Notably, the former aggre- gates low-\nand high-level features to construct a rich embed- ding for normal\npattern reconstruction. The latter targets to retain essential\ninformation favorable for the student to de- code out the teacher’s\nresponse. We perform extensive experiments on public bench- marks. The\nexperimental results indicate that our re- verse distillation paradigm\nachieves comparable perfor- mance with prior arts. The proposed OCBE\nmodule further improves the performance to a new state-of-the-art (SOTA)\nrecord. Our main contributions are summarized as follows:</p>\n<ul>\n<li><p>We introduce a simple, yet effective Reverse Distilla- tion\nparadigm for anomaly detection. The encoder- decoder structure and\nreverse knowledge distillation strategy holistically address the\nnon-distinguishing fil- ter problem in conventional KD models, boosting\nthe T-S model’s discrimination capability on anomalies.</p></li>\n<li><p>We propose a one-class bottleneck embedding mod- ule to project\nthe teacher’s high-dimensional features to a compact one-class embedding\nspace. This inno- vation facilitates retaining rich yet compact codes\nfor anomaly-free representation restoration at the student.</p></li>\n<li><p>We perform extensive experiments and show that our approach\nachieves new SOTA performance.</p></li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>此外，由于瓶颈嵌入的紧凑性对于异常检测至关重要（如上所述），我们引入了一类瓶颈嵌入（OCBE）模块来进一步压缩特征代码。\n我们的 OCBE 模块由多尺度特征融合 (MFF) 块和一类嵌入 (OCE)\n块组成，两者都与学生解码器联合优化。\n值得注意的是，前者聚合了低级和高级特征以构建用于正常模式重建的丰富嵌入。\n后者的目标是保留有利于学生解码教师反应的基本信息。\n我们在公共基准上进行了广泛的实验。\n实验结果表明，我们的反向蒸馏范式实现了与现有技术相当的性能。 所提出的\nOCBE 模块进一步将性能提高到新的最先进 (SOTA) 记录。\n我们的主要贡献总结如下：</p>\n<ul>\n<li><p>我们为异常检测引入了一种简单而有效的逆向蒸馏范式。\n编码器-解码器结构和反向知识蒸馏策略整体解决了传统 KD\n模型中的非区分过滤器问题，提高了 T-S 模型对异常的判别能力。</p>\n<ul>\n<li>我们提出了一类瓶颈嵌入模块，将教师的高维特征投影到紧凑的一类嵌入空间。\n这项创新有助于保留丰富而紧凑的代码，以便在学生处进行无异常表示恢复。</li>\n<li>我们进行了广泛的实验并表明我们的方法实现了新的 SOTA 性能。</li>\n</ul></li>\n</ul></blockquote>\n<h2 id=\"related-work\">2. Related Work</h2>\n<p>This section briefly reviews previous efforts on unsuper- vised\nanomaly detection. We will highlight the similarity and difference\nbetween the proposed method and prior arts. Classical anomaly detection\nmethods focus on defining a compact closed one-class distribution using\nnormal sup- port vectors. The pioneer studies include one-class support\nvector machine (OC-SVM) [35] and support vector data description (SVDD)\n[36]. To cope with high-dimensional data, DeepSVDD [31] and PatchSVDD\n[43] estimate data representations through deep networks. Another\nunsupervised AD prototype is the use of gener- ative models, such as\nAutoEncoder (AE) [19] and Genera- tive Adversarial Nets (GAN) [12], for\nsample reconstruc- tion. These methods rely on the hypothesis that\ngenera- tive models trained on normal samples only can success- fully\nreconstruct anomaly-free regions, but fail for anoma- lous regions [2,\n5, 34]. However, recent studies show that deep models generalize so well\nthat even anomalous re- gions can be well-restored [46]. To address this\nissue, memory mechanism [11, 16, 26] , image masking strat- egy [42, 46]\nand pseudo-anomaly [28, 45] are incorporated in reconstruction-based\nmethods. However, these meth- ods still lack a strong discriminating\nability for real-world anomaly detection [3, 5]. Recently, Metaformer\n(MF) [40] proposes the use of meta-learning [9] to bridge model adap-\ntation and reconstruction gap for reconstruction-based ap- proaches.\nNotably, the proposed reverse knowledge distil- lation also adopts the\nencoder-decoder architecture, but it differs from construction-based\nmethods in two-folds. First, the encoder in a generative model is\njointly trained with the decoder, while our reverse distillation freezes\na pre-trained model as the teacher. Second, instead of pixel-level\nrecon- struction error, it performs anomaly detection on the seman- tic\nfeature space.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>本节简要回顾了以前在无监督异常检测方面的努力。\n我们将强调所提出的方法与现有技术之间的相似之处和不同之处。\n经典的异常检测方法侧重于使用正态支持向量定义紧凑的封闭一类分布。\n先驱研究包括一类支持向量机（OC-SVM）[35]和支持向量数据描述（SVDD）[36]。\n为了处理高维数据，DeepSVDD [31] 和 PatchSVDD [43]\n通过深度网络估计数据表示。 另一个无监督的 AD\n原型是使用生成模型，例如自动编码器 (AE) [19] 和生成对抗网络 (GAN)\n[12]，用于样本重建。\n这些方法依赖于这样一个假设，即在正常样本上训练的生成模型只能成功地重建无异常区域，但对于异常区域则失败\n[2, 5, 34]。\n然而，最近的研究表明，深度模型的泛化能力非常好，即使是异常区域也可以很好地恢复\n[46]。\n为了解决这个问题，记忆机制[11、16、26]、图像掩蔽策略[42、46]和伪异常[28、45]被纳入基于重建的方法中。\n然而，这些方法对于现实世界的异常检测仍然缺乏很强的辨别能力[3, 5]。\n最近，Metaformer (MF) [40] 提出使用元学习 [9]\n来弥合基于重建的方法的模型适应和重建差距。\n值得注意的是，所提出的反向知识蒸馏也采用了编码器-解码器架构，但它与基于构造的方法有两方面的不同。\n首先，生成模型中的编码器与解码器联合训练，而我们的逆向蒸馏将预先训练的模型冻结为教师。\n其次，它不是像素级的重建错误，而是对语义特征空间进行异常检测。</p></blockquote>\n<p>Data augmentation strategy is also widely used. By adding pseudo\nanomalies in the provided anomaly-free samples, the unsupervised task is\nconverted to a supervised learning task [23, 42, 46]. However, these\napproaches are prone to bias towards pseudo outliers and fail to detect\na large variety of anomaly types. For example, CutPaste [23] generates\npseudo outliers by adding small patches onto nor- mal images and trains\na model to detect these anomalous regions. Since the model focuses on\ndetecting local fea- tures such as edge discontinuity and texture\nperturbations, it fails to detect and localize large defects and global\nstruc- tural anomalies as shown in Fig. 6. Recently, networks\npre-trained on the large dataset are proven to be capable of extracting\ndiscriminative features for anomaly detection [7,8,23,25,29,30]. With a\npre-trained model, memorizing its anomaly-free features helps to iden-\ntify anomalous samples [7, 29]. The studies in [8, 30] show that using\nthe Mahalanobis distance to measure the simi- larity between anomalies\nand anomaly-free features leads to accurate anomaly detection. Since\nthese methods re- quire memorizing all features from training samples,\nthey are computationally expensive. Knowledge distillation from\npre-trained models is an- other potential solution to anomaly detection.\nIn the con- text of unsupervised AD, since the student model is ex-\nposed to anomaly-free samples in knowledge distillation, the T-S model\nis expected to generate discrepant features on anomalies in inference\n[4,33,39]. To further increase the discrimnating capability of the T-S\nmodel on various types of abnormalities, different strategies are\nintroduced. For in- stance, in order to capture multi-scale anomaly, US\n[4] en- sembles several models trained on normal data at different\nscales, and MKD [33] propose to use multi-level features alignment. It\nshould be noted that though the proposed method is also based on\nknowledge distillation, our reverse distillation is the first to adopt\nan encoder and a decoder to construct the T-S model. The heterogeneity\nof the teacher and student networks and reverse data flow in knowledge\ndistillation distinguishes our method from prior arts.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>数据增强策略也被广泛使用。\n通过在提供的无异常样本中添加伪异常，将无监督任务转换为监督学习任务\n[23,42,46]。\n然而，这些方法容易偏向伪异常值，并且无法检测到多种异常类型。\n例如，CutPaste [23]\n通过在正常图像上添加小块来生成伪异常值，并训练模型来检测这些异常区域。\n由于该模型侧重于检测局部特征，例如边缘不连续性和纹理扰动，因此无法检测和定位大缺陷和全局结构异常，如图\n6 所示。\n最近，在大型数据集上预训练的网络被证明能够提取用于异常检测的判别特征\n[7,8,23,25,29,30]。 使用预训练模型，记住其无异常特征有助于识别异常样本\n[7, 29]。 [8, 30]\n中的研究表明，使用马氏距离来测量异常和无异常特征之间的相似性可以实现准确的异常检测。\n由于这些方法需要记住训练样本的所有特征，因此它们的计算成本很高。\n来自预训练模型的知识蒸馏是异常检测的另一个潜在解决方案。 在无监督 AD\n的背景下，由于学生模型在知识蒸馏中暴露于无异常样本，因此 T-S\n模型预计会在推理异常上产生差异特征 [4,33,39]。 为了进一步提高 T-S\n模型对各类异常的判别能力，引入了不同的策略。\n例如，为了捕获多尺度异常，US [4]\n集成了几个在不同尺度的正常数据上训练的模型，MKD [33]\n建议使用多级特征对齐。\n需要注意的是，虽然所提出的方法也是基于知识蒸馏的，但我们的逆向蒸馏是第一个采用编码器和解码器来构建\nT-S 模型的方法。\n教师和学生网络的异质性以及知识蒸馏中的反向数据流将我们的方法与现有技术区分开来。</p></blockquote>\n","feature":true,"text":"Anomaly Detection via Reverse Distillation from One-Class Embedding 通过一类嵌入的反向蒸馏进行异常检测 Abstract Knowledge distillation (KD) achieves promisin...","link":"","photos":[],"count_time":{"symbolsCount":"15k","symbolsTime":"13 mins."},"categories":[],"tags":[{"name":"深度学习","slug":"深度学习","count":2,"path":"api/tags/深度学习.json"},{"name":"异常分割","slug":"异常分割","count":2,"path":"api/tags/异常分割.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#anomaly-detection-via-reverse-distillation-from-one-class-embedding\"><span class=\"toc-text\">Anomaly\nDetection via Reverse Distillation from One-Class Embedding</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#abstract\"><span class=\"toc-text\">Abstract</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#introduction\"><span class=\"toc-text\">1. Introduction</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#related-work\"><span class=\"toc-text\">2. Related Work</span></a></li></ol></li></ol>","author":{"name":"Star","slug":"blog-author","avatar":"https://gitee.com/zyxstar/Pic_bed/raw/master/image/C06BE6D9-B862-4B4B-A025-F273AE06FAF0.jpeg","link":"/","description":"有棱有角，还会发光；认真生活，星之煌煌","socials":{"github":"https://github.com/ZYXsmile","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"Pytorch & python","uid":"4346091fabb18c4f718acfd51c087897","slug":"python&pytorch使用指南","date":"2022-05-03T12:10:00.000Z","updated":"2022-05-04T03:39:30.478Z","comments":true,"path":"api/articles/python&pytorch使用指南.json","keywords":null,"cover":null,"text":"pytorch 参数 &amp; 命令行 &amp; 辅助 logger logger模块解释 —— CSDN logger使用案例 logging模块是Python内置的标准模块，主要用于输出运行日志，可以设置输出日志的等级、日志保存路径、日志文件回滚等 yacs.config...","link":"","photos":[],"count_time":{"symbolsCount":"7.4k","symbolsTime":"7 mins."},"categories":[],"tags":[],"author":{"name":"Star","slug":"blog-author","avatar":"https://gitee.com/zyxstar/Pic_bed/raw/master/image/C06BE6D9-B862-4B4B-A025-F273AE06FAF0.jpeg","link":"/","description":"有棱有角，还会发光；认真生活，星之煌煌","socials":{"github":"https://github.com/ZYXsmile","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}