{"title":"一类嵌入的反向蒸馏","uid":"082602022df518a2faf6c62dcd303051","slug":"一类嵌入反向蒸馏","date":"2022-05-03T12:07:00.000Z","updated":"2022-05-04T00:49:04.263Z","comments":true,"path":"api/articles/一类嵌入反向蒸馏.json","keywords":null,"cover":null,"content":"<h1 id=\"Anomaly-Detection-via-Reverse-Distillation-from-One-Class-Embedding\"><a href=\"#Anomaly-Detection-via-Reverse-Distillation-from-One-Class-Embedding\" class=\"headerlink\" title=\"Anomaly Detection via Reverse Distillation from One-Class Embedding\"></a>Anomaly Detection via Reverse Distillation from One-Class Embedding</h1><p>通过一类嵌入的反向蒸馏进行异常检测</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. </p>\n<p>However, using similar or identical architectures to build the teacher and student models in previous stud- ies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective <strong>“reverse distillation” paradigm</strong> accordingly. </p>\n<p>Instead of receiving raw images directly, the student network takes teacher model’s one-class embedding as input and targets to restore the teacher’s multi-scale representations. </p>\n<p>Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. </p>\n<p>In addition, we introduce a trainable <strong>one-class bottleneck embedding (OCBE) module</strong> in our T-S model. </p>\n<p>The obtained compact embedding effectively preserves essential information on normal patterns, but abandons anomaly perturbations. </p>\n<p>Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach’s effectiveness and generalizability.</p>\n<hr>\n<p>知识蒸馏（KD）在无监督异常检测（AD）的挑战性问题上取得了可喜的成果。 师生（T-S）模型中异常的表示差异为AD提供了必要的证据。 </p>\n<p>然而，在以前的研究中使用相似或相同的架构来构建教师和学生模型阻碍了异常表示的多样性。 为了解决这个问题，我们提出了一种由教师编码器和学生解码器组成的新型 T-S 模型，并相应地引入了一种简单而有效的“逆向蒸馏”范式。</p>\n<p>学生网络不是直接接收原始图像，而是将教师模型的一类嵌入作为输入和目标，以恢复教师的多尺度表示。 </p>\n<p>本质上，本研究中的知识蒸馏从抽象的高级表示开始到低级特征。 </p>\n<p>此外，我们在 T-S 模型中引入了可训练的一类瓶颈嵌入 (OCBE) 模块。 </p>\n<p>获得的紧凑嵌入有效地保留了正常模式的基本信息，但放弃了异常扰动。 </p>\n<p>对 AD 和一类新颖性检测基准的广泛实验表明，我们的方法超越了 SOTA 性能，证明了我们提出的方法的有效性和普遍性。</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><p>Anomaly detection (AD) refers to <strong>identifying and localizing anomalies</strong> with limited, even no, prior knowledge of abnormality. </p>\n<p>The wide applications of AD, such as indus- trial defect detection [3], medical out-of-distribution detection [50], and video surveillance [24], makes it a critical task as well as a spotlight. In the context of <strong>unsupervised AD</strong>, no prior information on anomalies is available. Instead, a set of normal samples is provided for reference. </p>\n<p>To tackle this problem, previous efforts attempt to construct various self-supervision tasks on those anomaly-free samples. These tasks include, but not limited to, sample reconstruction [2, 5, 11, 16, 26, 34, 38, 48], pseudo-outlier augmentation [23, 42, 46], knowledge distillation [4, 33, 39], etc.</p>\n<p>In this study, we tackle the problem of unsupervised anomaly detection from the knowledge distillation-based point of view. </p>\n<p>In <strong>knowledge distillation (KD)</strong> [6, 15], knowledge is transferred within a teacher-student (T-S) pair. In the context of unsupervised AD, since the student experiences only normal samples during training, it is likely to generate discrepant representations from the teacher when a query is anomalous. This hypothesis forms the basis of KD-based methods for anomaly detection. </p>\n<p>However, this hypothesis is not always true in practice due to </p>\n<p>(1) the identical or similar architectures of the teacher and student networks (i.e., non-distinguishing filters [33]) </p>\n<p>(2) the same data flow in the T-S model during knowledge trans- fer&#x2F;distillation. </p>\n<p>Though the use of a smaller student network partially addresses this issue [33, 39], the weaker represen- tation capability of shallow architectures hinders the model from precisely detecting and localizing anomalies.</p>\n<hr>\n<p>异常检测 (AD) 是指在对异常的先验知识有限甚至没有的情况下识别和定位异常。  </p>\n<p>AD 的广泛应用，如工业缺陷检测 [3]、医疗分布外检测 [50] 和视频监控 [24]，使其成为一项关键任务和聚光灯。 在无监督 AD 的背景下，没有关于异常的先验信息可用。 相反，提供了一组正常样本以供参考。 </p>\n<p>为了解决这个问题，以前的努力试图在那些无异常的样本上构建各种自我监督任务。 这些任务包括但不限于样本重建[2、5、11、16、26、34、38、48]、伪异常值增强[23、42、46]、知识蒸馏[4、33、  39]等。</p>\n<p>在这项研究中，我们从基于知识蒸馏的角度解决了无监督异常检测的问题。 </p>\n<p>在知识蒸馏 (KD) [6, 15] 中，知识在师生 (T-S) 对中转移。 在无监督 AD 的背景下，由于学生在训练期间只体验到正常样本，因此当查询异常时，它可能会从教师那里产生不一致的表示。 该假设构成了基于 KD 的异常检测方法的基础。 然而，这个假设在实践中并不总是正确的，因为 </p>\n<ol>\n<li>教师和学生网络的相同或相似架构（即非区分过滤器 [33]）</li>\n<li>T-S 模型中的相同数据流 知识转移&#x2F;蒸馏。</li>\n</ol>\n<p>尽管使用较小的学生网络部分解决了这个问题 [33, 39]，但浅层架构较弱的表示能力阻碍了模型精确检测和定位异常。</p>\n<hr>\n<p>To holistically address the issue mentioned above, we propose a new paradigm of knowledge distillation, namely Reverse Distillation, for anomaly detection. We use sim- ple diagrams in Fig. 2 to highlight the systematic differ- ence between conventional knowledge distillation and the proposed reverse distillation. First, unlike the conventional knowledge distillation framework where both teacher and student adopt the encoder structure, the T-S model in our reverse distillation consists of heterogeneous architectures: a teacher encoder and a student decoder. Second, instead of directly feeding the raw data to the T-S model simulta- neously, the student decoder takes the low-dimensional em- bedding as input, targeting to mimic the teacher’s behavior by restoring the teacher model’s representations in different scales. From the regression perspective, our reverse distil- lation uses the student network to predict the representa- tion of the teacher model. Therefore, ”reverse” here indi- cates both the reverse shapes of teacher encoder and stu- dent decoder and the distinct knowledge distillation order where high-level representation is first distilled, followed by low-level features. It is noteworthy that our reverse distilla- tion presents two significant advantages: i) <em>Non-similarity</em> structure. In the proposed T-S model, one can consider the teacher encoder as a down-sampling filter and the stu- dent decoder as an up-sampling filter. The ”reverse struc- tures” avoid the confusion caused by non-distinguishing fil- ters [33] as we discussed above. ii) <em>Compactness embed- ding</em>. The low-dimensional embedding fed to the student decoder acts as an information bottleneck for normal pat- tern restoration. Let’s formulate anomaly features as pertur- bations on normal patterns. Then the compact embedding helps to prohibit the propagation of such unusual perturba- tions to the student model and thus boosts the T-S model’s representation discrepancy on anomalies. Notably, tradi- tional AE-based methods [5, 11, 16, 26] detect anomalies utilising pixel differences, whereas we perform discrimi- nation with dense descriptive features. Deep features as region-aware descriptors provide more effective discrimi- native information than per-pixel in images.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>为了全面解决上述问题，我们提出了一种新的知识蒸馏范式，即反向蒸馏，用于异常检测。 我们使用图 2 中的简单图表来突出传统知识蒸馏和提出的逆向蒸馏之间的系统差异。 首先，与教师和学生都采用编码器结构的传统知识蒸馏框架不同，我们的逆向蒸馏中的 T-S 模型由异构架构组成：教师编码器和学生解码器。 其次，学生解码器不是直接将原始数据同时馈送到 T-S 模型，而是将低维嵌入作为输入，旨在通过恢复教师模型在不同尺度上的表示来模仿教师的行为。 从回归的角度来看，我们的反向蒸馏使用学生网络来预测教师模型的表示。 因此，这里的“反向”表示教师编码器和学生解码器的反向形状以及不同的知识蒸馏顺序，其中首先蒸馏高级表示，然后是低级特征。 值得注意的是，我们的逆向蒸馏具有两个显着优势：i）<em>非相似性</em>结构。 在提出的 T-S 模型中，可以将教师编码器视为下采样滤波器，将学生解码器视为上采样滤波器。 正如我们上面讨论的，“反向结构”避免了由非区分过滤器[33]引起的混淆。  ii) <em>紧凑性嵌入</em>。 馈送到学生解码器的低维嵌入充当了正常模式恢复的信息瓶颈。 让我们将异常特征表述为对正常模式的扰动。 然后紧凑嵌入有助于禁止这种不寻常的扰动传播到学生模型，从而提高 T-S 模型对异常的表示差异。 值得注意的是，传统的基于 AE 的方法 [5、11、16、26] 利用像素差异检测异常，而我们使用密集的描述性特征进行区分。 作为区域感知描述符的深度特征比图像中的每个像素提供更有效的判别信息。</p></blockquote>\n<p>In addition, since the compactness of the bottleneck em- bedding is vital for anomaly detection (as discussed above), we introduce a one-class bottleneck embedding (OCBE) module to condense the feature codes further. Our OCBE module consists of a multi-scale feature fusion (MFF) block and one-class embedding (OCE) block, both jointly opti- mized with the student decoder. Notably, the former aggre- gates low- and high-level features to construct a rich embed- ding for normal pattern reconstruction. The latter targets to retain essential information favorable for the student to de- code out the teacher’s response.<br> We perform extensive experiments on public bench- marks. The experimental results indicate that our re- verse distillation paradigm achieves comparable perfor- mance with prior arts. The proposed OCBE module further improves the performance to a new state-of-the-art (SOTA) record. Our main contributions are summarized as follows:  </p>\n<ul>\n<li><p>We introduce a simple, yet effective Reverse Distilla- tion paradigm for anomaly detection. The encoder- decoder structure and reverse knowledge distillation strategy holistically address the non-distinguishing fil- ter problem in conventional KD models, boosting the T-S model’s discrimination capability on anomalies.</p>\n</li>\n<li><p>We propose a one-class bottleneck embedding mod- ule to project the teacher’s high-dimensional features to a compact one-class embedding space. This inno- vation facilitates retaining rich yet compact codes for anomaly-free representation restoration at the student.</p>\n</li>\n<li><p>We perform extensive experiments and show that our approach achieves new SOTA performance.</p>\n</li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>此外，由于瓶颈嵌入的紧凑性对于异常检测至关重要（如上所述），我们引入了一类瓶颈嵌入（OCBE）模块来进一步压缩特征代码。 我们的 OCBE 模块由多尺度特征融合 (MFF) 块和一类嵌入 (OCE) 块组成，两者都与学生解码器联合优化。 值得注意的是，前者聚合了低级和高级特征以构建用于正常模式重建的丰富嵌入。 后者的目标是保留有利于学生解码教师反应的基本信息。<br>    我们在公共基准上进行了广泛的实验。 实验结果表明，我们的反向蒸馏范式实现了与现有技术相当的性能。 所提出的 OCBE 模块进一步将性能提高到新的最先进 (SOTA) 记录。 我们的主要贡献总结如下： </p>\n<ul>\n<li><p>我们为异常检测引入了一种简单而有效的逆向蒸馏范式。 编码器-解码器结构和反向知识蒸馏策略整体解决了传统 KD 模型中的非区分过滤器问题，提高了 T-S 模型对异常的判别能力。</p>\n<ul>\n<li>我们提出了一类瓶颈嵌入模块，将教师的高维特征投影到紧凑的一类嵌入空间。 这项创新有助于保留丰富而紧凑的代码，以便在学生处进行无异常表示恢复。</li>\n<li>我们进行了广泛的实验并表明我们的方法实现了新的 SOTA 性能。</li>\n</ul>\n</li>\n</ul></blockquote>\n<h2 id=\"2-Related-Work\"><a href=\"#2-Related-Work\" class=\"headerlink\" title=\"2. Related Work\"></a>2. Related Work</h2><p>This section briefly reviews previous efforts on unsuper- vised anomaly detection. We will highlight the similarity and difference between the proposed method and prior arts.<br> Classical anomaly detection methods focus on defining a compact closed one-class distribution using normal sup- port vectors. The pioneer studies include one-class support vector machine (OC-SVM) [35] and support vector data description (SVDD) [36]. To cope with high-dimensional data, DeepSVDD [31] and PatchSVDD [43] estimate data representations through deep networks.<br> Another unsupervised AD prototype is the use of gener- ative models, such as AutoEncoder (AE) [19] and Genera- tive Adversarial Nets (GAN) [12], for sample reconstruc- tion. These methods rely on the hypothesis that genera- tive models trained on normal samples only can success- fully reconstruct anomaly-free regions, but fail for anoma- lous regions [2, 5, 34]. However, recent studies show that deep models generalize so well that even anomalous re- gions can be well-restored [46]. To address this issue, memory mechanism [11, 16, 26] , image masking strat- egy [42, 46] and pseudo-anomaly [28, 45] are incorporated in reconstruction-based methods. However, these meth- ods still lack a strong discriminating ability for real-world anomaly detection [3, 5]. Recently, Metaformer (MF) [40] proposes the use of meta-learning [9] to bridge model adap- tation and reconstruction gap for reconstruction-based ap- proaches. Notably, the proposed reverse knowledge distil- lation also adopts the encoder-decoder architecture, but it differs from construction-based methods in two-folds. First, the encoder in a generative model is jointly trained with the decoder, while our reverse distillation freezes a pre-trained model as the teacher. Second, instead of pixel-level recon- struction error, it performs anomaly detection on the seman- tic feature space.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>本节简要回顾了以前在无监督异常检测方面的努力。 我们将强调所提出的方法与现有技术之间的相似之处和不同之处。<br>    经典的异常检测方法侧重于使用正态支持向量定义紧凑的封闭一类分布。 先驱研究包括一类支持向量机（OC-SVM）[35]和支持向量数据描述（SVDD）[36]。 为了处理高维数据，DeepSVDD [31] 和 PatchSVDD [43] 通过深度网络估计数据表示。<br>    另一个无监督的 AD 原型是使用生成模型，例如自动编码器 (AE) [19] 和生成对抗网络 (GAN) [12]，用于样本重建。 这些方法依赖于这样一个假设，即在正常样本上训练的生成模型只能成功地重建无异常区域，但对于异常区域则失败 [2, 5, 34]。 然而，最近的研究表明，深度模型的泛化能力非常好，即使是异常区域也可以很好地恢复 [46]。 为了解决这个问题，记忆机制[11、16、26]、图像掩蔽策略[42、46]和伪异常[28、45]被纳入基于重建的方法中。 然而，这些方法对于现实世界的异常检测仍然缺乏很强的辨别能力[3, 5]。 最近，Metaformer (MF) [40] 提出使用元学习 [9] 来弥合基于重建的方法的模型适应和重建差距。 值得注意的是，所提出的反向知识蒸馏也采用了编码器-解码器架构，但它与基于构造的方法有两方面的不同。 首先，生成模型中的编码器与解码器联合训练，而我们的逆向蒸馏将预先训练的模型冻结为教师。 其次，它不是像素级的重建错误，而是对语义特征空间进行异常检测。</p></blockquote>\n<p>Data augmentation strategy is also widely used. By adding pseudo anomalies in the provided anomaly-free samples, the unsupervised task is converted to a supervised learning task [23, 42, 46]. However, these approaches are prone to bias towards pseudo outliers and fail to detect a large variety of anomaly types. For example, CutPaste [23] generates pseudo outliers by adding small patches onto nor- mal images and trains a model to detect these anomalous regions. Since the model focuses on detecting local fea- tures such as edge discontinuity and texture perturbations, it fails to detect and localize large defects and global struc- tural anomalies as shown in Fig. 6.<br>Recently, networks pre-trained on the large dataset are proven to be capable of extracting discriminative features for anomaly detection [7,8,23,25,29,30]. With a pre-trained model, memorizing its anomaly-free features helps to iden- tify anomalous samples [7, 29]. The studies in [8, 30] show that using the Mahalanobis distance to measure the simi- larity between anomalies and anomaly-free features leads to accurate anomaly detection. Since these methods re- quire memorizing all features from training samples, they are computationally expensive.<br>Knowledge distillation from pre-trained models is an- other potential solution to anomaly detection. In the con- text of unsupervised AD, since the student model is ex- posed to anomaly-free samples in knowledge distillation, the T-S model is expected to generate discrepant features on anomalies in inference [4,33,39]. To further increase the discrimnating capability of the T-S model on various types of abnormalities, different strategies are introduced. For in- stance, in order to capture multi-scale anomaly, US [4] en- sembles several models trained on normal data at different scales, and MKD [33] propose to use multi-level features alignment. It should be noted that though the proposed method is also based on knowledge distillation, our reverse distillation is the first to adopt an encoder and a decoder to construct the T-S model. The heterogeneity of the teacher and student networks and reverse data flow in knowledge distillation distinguishes our method from prior arts.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>数据增强策略也被广泛使用。 通过在提供的无异常样本中添加伪异常，将无监督任务转换为监督学习任务 [23,42,46]。 然而，这些方法容易偏向伪异常值，并且无法检测到多种异常类型。 例如，CutPaste [23] 通过在正常图像上添加小块来生成伪异常值，并训练模型来检测这些异常区域。 由于该模型侧重于检测局部特征，例如边缘不连续性和纹理扰动，因此无法检测和定位大缺陷和全局结构异常，如图 6 所示。<br>   最近，在大型数据集上预训练的网络被证明能够提取用于异常检测的判别特征 [7,8,23,25,29,30]。 使用预训练模型，记住其无异常特征有助于识别异常样本 [7, 29]。  [8, 30] 中的研究表明，使用马氏距离来测量异常和无异常特征之间的相似性可以实现准确的异常检测。 由于这些方法需要记住训练样本的所有特征，因此它们的计算成本很高。<br>   来自预训练模型的知识蒸馏是异常检测的另一个潜在解决方案。 在无监督 AD 的背景下，由于学生模型在知识蒸馏中暴露于无异常样本，因此 T-S 模型预计会在推理异常上产生差异特征 [4,33,39]。 为了进一步提高 T-S 模型对各类异常的判别能力，引入了不同的策略。 例如，为了捕获多尺度异常，US [4] 集成了几个在不同尺度的正常数据上训练的模型，MKD [33] 建议使用多级特征对齐。 需要注意的是，虽然所提出的方法也是基于知识蒸馏的，但我们的逆向蒸馏是第一个采用编码器和解码器来构建 T-S 模型的方法。 教师和学生网络的异质性以及知识蒸馏中的反向数据流将我们的方法与现有技术区分开来。</p></blockquote>\n","feature":true,"text":"Anomaly Detection via Reverse Distillation from One-Class Embedding通过一类嵌入的反向蒸馏进行异常检测 AbstractKnowledge distillation (KD) achieves promising ...","link":"","photos":[],"count_time":{"symbolsCount":"15k","symbolsTime":"13 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Anomaly-Detection-via-Reverse-Distillation-from-One-Class-Embedding\"><span class=\"toc-text\">Anomaly Detection via Reverse Distillation from One-Class Embedding</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Abstract\"><span class=\"toc-text\">Abstract</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#1-Introduction\"><span class=\"toc-text\">1. Introduction</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#2-Related-Work\"><span class=\"toc-text\">2. Related Work</span></a></li></ol></li></ol>","author":{"name":"Star","slug":"blog-author","avatar":"https://gitee.com/zyxstar/Pic_bed/raw/master/image/C06BE6D9-B862-4B4B-A025-F273AE06FAF0.jpeg","link":"有棱有角，还会发光；认真生活，星之煌煌","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"DMLNet","uid":"cca6975b74b1218e11c5b9ba5de4d5ef","slug":"DMLNet","date":"2022-05-03T12:07:00.000Z","updated":"2022-05-03T12:24:57.307Z","comments":true,"path":"api/articles/DMLNet.json","keywords":null,"cover":[],"text":"开放世界语义分割 开集语义分割模块 闭集语义分割子模块 异常分割子模块 增量小样本学习模块 我是短小精悍的文章摘要(๑•̀ㅂ•́) ✧ CODEmultiscale 是自己设定的吗 cfg.DATASET.imgSizes = (300, 375, 450, 525, 600) Se...","link":"","photos":[],"count_time":{"symbolsCount":"13k","symbolsTime":"11 mins."},"categories":[],"tags":[],"author":{"name":"Star","slug":"blog-author","avatar":"https://gitee.com/zyxstar/Pic_bed/raw/master/image/C06BE6D9-B862-4B4B-A025-F273AE06FAF0.jpeg","link":"有棱有角，还会发光；认真生活，星之煌煌","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}