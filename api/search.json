[{"id":"082602022df518a2faf6c62dcd303051","title":"ä¸€ç±»åµŒå…¥çš„åå‘è’¸é¦","content":"Anomaly Detection via Reverse Distillation from One-Class Embedding\né€šè¿‡ä¸€ç±»åµŒå…¥çš„åå‘è’¸é¦è¿›è¡Œå¼‚å¸¸æ£€æµ‹\n\nAbstract\nKnowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD.\nHowever, using similar or identical architectures to build the teacher and student models in previous stud- ies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective â€œreverse distillationâ€ paradigm accordingly.\nInstead of receiving raw images directly, the student network takes teacher modelâ€™s one-class embedding as input and targets to restore the teacherâ€™s multi-scale representations.\nInherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features.\nIn addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model.\nThe obtained compact embedding effectively preserves essential information on normal patterns, but abandons anomaly perturbations.\nExtensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approachâ€™s effectiveness and generalizability.\n\nçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰çš„æŒ‘æˆ˜æ€§é—®é¢˜ä¸Šå–å¾—äº†å¯å–œçš„æˆæœã€‚ å¸ˆç”Ÿï¼ˆT-Sï¼‰æ¨¡å‹ä¸­å¼‚å¸¸çš„è¡¨ç¤ºå·®å¼‚ä¸ºADæä¾›äº†å¿…è¦çš„è¯æ®ã€‚\nç„¶è€Œï¼Œåœ¨ä»¥å‰çš„ç ”ç©¶ä¸­ä½¿ç”¨ç›¸ä¼¼æˆ–ç›¸åŒçš„æ¶æ„æ¥æ„å»ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹é˜»ç¢äº†å¼‚å¸¸è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±æ•™å¸ˆç¼–ç å™¨å’Œå­¦ç”Ÿè§£ç å™¨ç»„æˆçš„æ–°å‹ T-S æ¨¡å‹ï¼Œå¹¶ç›¸åº”åœ°å¼•å…¥äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„â€œé€†å‘è’¸é¦â€èŒƒå¼ã€‚\nå­¦ç”Ÿç½‘ç»œä¸æ˜¯ç›´æ¥æ¥æ”¶åŸå§‹å›¾åƒï¼Œè€Œæ˜¯å°†æ•™å¸ˆæ¨¡å‹çš„ä¸€ç±»åµŒå…¥ä½œä¸ºè¾“å…¥å’Œç›®æ ‡ï¼Œä»¥æ¢å¤æ•™å¸ˆçš„å¤šå°ºåº¦è¡¨ç¤ºã€‚\næœ¬è´¨ä¸Šï¼Œæœ¬ç ”ç©¶ä¸­çš„çŸ¥è¯†è’¸é¦ä»æŠ½è±¡çš„é«˜çº§è¡¨ç¤ºå¼€å§‹åˆ°ä½çº§ç‰¹å¾ã€‚\næ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ T-S æ¨¡å‹ä¸­å¼•å…¥äº†å¯è®­ç»ƒçš„ä¸€ç±»ç“¶é¢ˆåµŒå…¥ (OCBE) æ¨¡å—ã€‚\nè·å¾—çš„ç´§å‡‘åµŒå…¥æœ‰æ•ˆåœ°ä¿ç•™äº†æ­£å¸¸æ¨¡å¼çš„åŸºæœ¬ä¿¡æ¯ï¼Œä½†æ”¾å¼ƒäº†å¼‚å¸¸æ‰°åŠ¨ã€‚\nå¯¹ AD å’Œä¸€ç±»æ–°é¢–æ€§æ£€æµ‹åŸºå‡†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº† SOTA æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚\n1. Introduction\nAnomaly detection (AD) refers to identifying and localizing anomalies with limited, even no, prior knowledge of abnormality.\nThe wide applications of AD, such as indus- trial defect detection [3], medical out-of-distribution detection [50], and video surveillance [24], makes it a critical task as well as a spotlight. In the context of unsupervised AD, no prior information on anomalies is available. Instead, a set of normal samples is provided for reference.\nTo tackle this problem, previous efforts attempt to construct various self-supervision tasks on those anomaly-free samples. These tasks include, but not limited to, sample reconstruction [2, 5, 11, 16, 26, 34, 38, 48], pseudo-outlier augmentation [23, 42, 46], knowledge distillation [4, 33, 39], etc.\nIn this study, we tackle the problem of unsupervised anomaly detection from the knowledge distillation-based point of view.\nIn knowledge distillation (KD) [6, 15], knowledge is transferred within a teacher-student (T-S) pair. In the context of unsupervised AD, since the student experiences only normal samples during training, it is likely to generate discrepant representations from the teacher when a query is anomalous. This hypothesis forms the basis of KD-based methods for anomaly detection.\nHowever, this hypothesis is not always true in practice due to\n(1) the identical or similar architectures of the teacher and student networks (i.e., non-distinguishing filters [33])\n(2) the same data flow in the T-S model during knowledge trans- fer/distillation.\nThough the use of a smaller student network partially addresses this issue [33, 39], the weaker represen- tation capability of shallow architectures hinders the model from precisely detecting and localizing anomalies.\n\nå¼‚å¸¸æ£€æµ‹ (AD) æ˜¯æŒ‡åœ¨å¯¹å¼‚å¸¸çš„å…ˆéªŒçŸ¥è¯†æœ‰é™ç”šè‡³æ²¡æœ‰çš„æƒ…å†µä¸‹è¯†åˆ«å’Œå®šä½å¼‚å¸¸ã€‚\nAD çš„å¹¿æ³›åº”ç”¨ï¼Œå¦‚å·¥ä¸šç¼ºé™·æ£€æµ‹ [3]ã€åŒ»ç–—åˆ†å¸ƒå¤–æ£€æµ‹ [50] å’Œè§†é¢‘ç›‘æ§ [24]ï¼Œä½¿å…¶æˆä¸ºä¸€é¡¹å…³é”®ä»»åŠ¡å’Œèšå…‰ç¯ã€‚ åœ¨æ— ç›‘ç£ AD çš„èƒŒæ™¯ä¸‹ï¼Œæ²¡æœ‰å…³äºå¼‚å¸¸çš„å…ˆéªŒä¿¡æ¯å¯ç”¨ã€‚ ç›¸åï¼Œæä¾›äº†ä¸€ç»„æ­£å¸¸æ ·æœ¬ä»¥ä¾›å‚è€ƒã€‚\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥å‰çš„åŠªåŠ›è¯•å›¾åœ¨é‚£äº›æ— å¼‚å¸¸çš„æ ·æœ¬ä¸Šæ„å»ºå„ç§è‡ªæˆ‘ç›‘ç£ä»»åŠ¡ã€‚ è¿™äº›ä»»åŠ¡åŒ…æ‹¬ä½†ä¸é™äºæ ·æœ¬é‡å»º[2ã€5ã€11ã€16ã€26ã€34ã€38ã€48]ã€ä¼ªå¼‚å¸¸å€¼å¢å¼º[23ã€42ã€46]ã€çŸ¥è¯†è’¸é¦[4ã€33ã€  39]ç­‰ã€‚\nåœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»åŸºäºçŸ¥è¯†è’¸é¦çš„è§’åº¦è§£å†³äº†æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚\nåœ¨çŸ¥è¯†è’¸é¦ (KD) [6, 15] ä¸­ï¼ŒçŸ¥è¯†åœ¨å¸ˆç”Ÿ (T-S) å¯¹ä¸­è½¬ç§»ã€‚ åœ¨æ— ç›‘ç£ AD çš„èƒŒæ™¯ä¸‹ï¼Œç”±äºå­¦ç”Ÿåœ¨è®­ç»ƒæœŸé—´åªä½“éªŒåˆ°æ­£å¸¸æ ·æœ¬ï¼Œå› æ­¤å½“æŸ¥è¯¢å¼‚å¸¸æ—¶ï¼Œå®ƒå¯èƒ½ä¼šä»æ•™å¸ˆé‚£é‡Œäº§ç”Ÿä¸ä¸€è‡´çš„è¡¨ç¤ºã€‚ è¯¥å‡è®¾æ„æˆäº†åŸºäº KD çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„åŸºç¡€ã€‚ ç„¶è€Œï¼Œè¿™ä¸ªå‡è®¾åœ¨å®è·µä¸­å¹¶ä¸æ€»æ˜¯æ­£ç¡®çš„ï¼Œå› ä¸º\n\næ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œçš„ç›¸åŒæˆ–ç›¸ä¼¼æ¶æ„ï¼ˆå³éåŒºåˆ†è¿‡æ»¤å™¨ [33]ï¼‰\nT-S æ¨¡å‹ä¸­çš„ç›¸åŒæ•°æ®æµ çŸ¥è¯†è½¬ç§»/è’¸é¦ã€‚\n\nå°½ç®¡ä½¿ç”¨è¾ƒå°çš„å­¦ç”Ÿç½‘ç»œéƒ¨åˆ†è§£å†³äº†è¿™ä¸ªé—®é¢˜ [33, 39]ï¼Œä½†æµ…å±‚æ¶æ„è¾ƒå¼±çš„è¡¨ç¤ºèƒ½åŠ›é˜»ç¢äº†æ¨¡å‹ç²¾ç¡®æ£€æµ‹å’Œå®šä½å¼‚å¸¸ã€‚\n\nTo holistically address the issue mentioned above, we propose a new paradigm of knowledge distillation, namely Reverse Distillation, for anomaly detection. We use sim- ple diagrams in Fig. 2 to highlight the systematic differ- ence between conventional knowledge distillation and the proposed reverse distillation. First, unlike the conventional knowledge distillation framework where both teacher and student adopt the encoder structure, the T-S model in our reverse distillation consists of heterogeneous architectures: a teacher encoder and a student decoder. Second, instead of directly feeding the raw data to the T-S model simulta- neously, the student decoder takes the low-dimensional em- bedding as input, targeting to mimic the teacherâ€™s behavior by restoring the teacher modelâ€™s representations in different scales. From the regression perspective, our reverse distil- lation uses the student network to predict the representa- tion of the teacher model. Therefore, â€reverseâ€ here indi- cates both the reverse shapes of teacher encoder and stu- dent decoder and the distinct knowledge distillation order where high-level representation is first distilled, followed by low-level features. It is noteworthy that our reverse distilla- tion presents two significant advantages: i) Non-similarity structure. In the proposed T-S model, one can consider the teacher encoder as a down-sampling filter and the stu- dent decoder as an up-sampling filter. The â€reverse struc- turesâ€ avoid the confusion caused by non-distinguishing fil- ters [33] as we discussed above. ii) Compactness embed- ding. The low-dimensional embedding fed to the student decoder acts as an information bottleneck for normal pat- tern restoration. Letâ€™s formulate anomaly features as pertur- bations on normal patterns. Then the compact embedding helps to prohibit the propagation of such unusual perturba- tions to the student model and thus boosts the T-S modelâ€™s representation discrepancy on anomalies. Notably, tradi- tional AE-based methods [5, 11, 16, 26] detect anomalies utilising pixel differences, whereas we perform discrimi- nation with dense descriptive features. Deep features as region-aware descriptors provide more effective discrimi- native information than per-pixel in images.\n\n\n\n\n\n\n\n\n\nä¸ºäº†å…¨é¢è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„çŸ¥è¯†è’¸é¦èŒƒå¼ï¼Œå³åå‘è’¸é¦ï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹ã€‚ æˆ‘ä»¬ä½¿ç”¨å›¾ 2 ä¸­çš„ç®€å•å›¾è¡¨æ¥çªå‡ºä¼ ç»ŸçŸ¥è¯†è’¸é¦å’Œæå‡ºçš„é€†å‘è’¸é¦ä¹‹é—´çš„ç³»ç»Ÿå·®å¼‚ã€‚ é¦–å…ˆï¼Œä¸æ•™å¸ˆå’Œå­¦ç”Ÿéƒ½é‡‡ç”¨ç¼–ç å™¨ç»“æ„çš„ä¼ ç»ŸçŸ¥è¯†è’¸é¦æ¡†æ¶ä¸åŒï¼Œæˆ‘ä»¬çš„é€†å‘è’¸é¦ä¸­çš„ T-S æ¨¡å‹ç”±å¼‚æ„æ¶æ„ç»„æˆï¼šæ•™å¸ˆç¼–ç å™¨å’Œå­¦ç”Ÿè§£ç å™¨ã€‚ å…¶æ¬¡ï¼Œå­¦ç”Ÿè§£ç å™¨ä¸æ˜¯ç›´æ¥å°†åŸå§‹æ•°æ®åŒæ—¶é¦ˆé€åˆ° T-S æ¨¡å‹ï¼Œè€Œæ˜¯å°†ä½ç»´åµŒå…¥ä½œä¸ºè¾“å…¥ï¼Œæ—¨åœ¨é€šè¿‡æ¢å¤æ•™å¸ˆæ¨¡å‹åœ¨ä¸åŒå°ºåº¦ä¸Šçš„è¡¨ç¤ºæ¥æ¨¡ä»¿æ•™å¸ˆçš„è¡Œä¸ºã€‚ ä»å›å½’çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çš„åå‘è’¸é¦ä½¿ç”¨å­¦ç”Ÿç½‘ç»œæ¥é¢„æµ‹æ•™å¸ˆæ¨¡å‹çš„è¡¨ç¤ºã€‚ å› æ­¤ï¼Œè¿™é‡Œçš„â€œåå‘â€è¡¨ç¤ºæ•™å¸ˆç¼–ç å™¨å’Œå­¦ç”Ÿè§£ç å™¨çš„åå‘å½¢çŠ¶ä»¥åŠä¸åŒçš„çŸ¥è¯†è’¸é¦é¡ºåºï¼Œå…¶ä¸­é¦–å…ˆè’¸é¦é«˜çº§è¡¨ç¤ºï¼Œç„¶åæ˜¯ä½çº§ç‰¹å¾ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„é€†å‘è’¸é¦å…·æœ‰ä¸¤ä¸ªæ˜¾ç€ä¼˜åŠ¿ï¼šiï¼‰éç›¸ä¼¼æ€§ç»“æ„ã€‚ åœ¨æå‡ºçš„ T-S æ¨¡å‹ä¸­ï¼Œå¯ä»¥å°†æ•™å¸ˆç¼–ç å™¨è§†ä¸ºä¸‹é‡‡æ ·æ»¤æ³¢å™¨ï¼Œå°†å­¦ç”Ÿè§£ç å™¨è§†ä¸ºä¸Šé‡‡æ ·æ»¤æ³¢å™¨ã€‚ æ­£å¦‚æˆ‘ä»¬ä¸Šé¢è®¨è®ºçš„ï¼Œâ€œåå‘ç»“æ„â€é¿å…äº†ç”±éåŒºåˆ†è¿‡æ»¤å™¨[33]å¼•èµ·çš„æ··æ·†ã€‚  ii) ç´§å‡‘æ€§åµŒå…¥ã€‚ é¦ˆé€åˆ°å­¦ç”Ÿè§£ç å™¨çš„ä½ç»´åµŒå…¥å……å½“äº†æ­£å¸¸æ¨¡å¼æ¢å¤çš„ä¿¡æ¯ç“¶é¢ˆã€‚ è®©æˆ‘ä»¬å°†å¼‚å¸¸ç‰¹å¾è¡¨è¿°ä¸ºå¯¹æ­£å¸¸æ¨¡å¼çš„æ‰°åŠ¨ã€‚ ç„¶åç´§å‡‘åµŒå…¥æœ‰åŠ©äºç¦æ­¢è¿™ç§ä¸å¯»å¸¸çš„æ‰°åŠ¨ä¼ æ’­åˆ°å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæé«˜ T-S æ¨¡å‹å¯¹å¼‚å¸¸çš„è¡¨ç¤ºå·®å¼‚ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¼ ç»Ÿçš„åŸºäº AE çš„æ–¹æ³• [5ã€11ã€16ã€26] åˆ©ç”¨åƒç´ å·®å¼‚æ£€æµ‹å¼‚å¸¸ï¼Œè€Œæˆ‘ä»¬ä½¿ç”¨å¯†é›†çš„æè¿°æ€§ç‰¹å¾è¿›è¡ŒåŒºåˆ†ã€‚ ä½œä¸ºåŒºåŸŸæ„ŸçŸ¥æè¿°ç¬¦çš„æ·±åº¦ç‰¹å¾æ¯”å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ æä¾›æ›´æœ‰æ•ˆçš„åˆ¤åˆ«ä¿¡æ¯ã€‚\nIn addition, since the compactness of the bottleneck em- bedding is vital for anomaly detection (as discussed above), we introduce a one-class bottleneck embedding (OCBE) module to condense the feature codes further. Our OCBE module consists of a multi-scale feature fusion (MFF) block and one-class embedding (OCE) block, both jointly opti- mized with the student decoder. Notably, the former aggre- gates low- and high-level features to construct a rich embed- ding for normal pattern reconstruction. The latter targets to retain essential information favorable for the student to de- code out the teacherâ€™s response.\nWe perform extensive experiments on public bench- marks. The experimental results indicate that our re- verse distillation paradigm achieves comparable perfor- mance with prior arts. The proposed OCBE module further improves the performance to a new state-of-the-art (SOTA) record. Our main contributions are summarized as follows:\n\n\nWe introduce a simple, yet effective Reverse Distilla- tion paradigm for anomaly detection. The encoder- decoder structure and reverse knowledge distillation strategy holistically address the non-distinguishing fil- ter problem in conventional KD models, boosting the T-S modelâ€™s discrimination capability on anomalies.\n\n\nWe propose a one-class bottleneck embedding mod- ule to project the teacherâ€™s high-dimensional features to a compact one-class embedding space. This inno- vation facilitates retaining rich yet compact codes for anomaly-free representation restoration at the student.\n\n\nWe perform extensive experiments and show that our approach achieves new SOTA performance.\n\n\n\n\n\n\n\n\n\n\n\næ­¤å¤–ï¼Œç”±äºç“¶é¢ˆåµŒå…¥çš„ç´§å‡‘æ€§å¯¹äºå¼‚å¸¸æ£€æµ‹è‡³å…³é‡è¦ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç±»ç“¶é¢ˆåµŒå…¥ï¼ˆOCBEï¼‰æ¨¡å—æ¥è¿›ä¸€æ­¥å‹ç¼©ç‰¹å¾ä»£ç ã€‚ æˆ‘ä»¬çš„ OCBE æ¨¡å—ç”±å¤šå°ºåº¦ç‰¹å¾èåˆ (MFF) å—å’Œä¸€ç±»åµŒå…¥ (OCE) å—ç»„æˆï¼Œä¸¤è€…éƒ½ä¸å­¦ç”Ÿè§£ç å™¨è”åˆä¼˜åŒ–ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‰è€…èšåˆäº†ä½çº§å’Œé«˜çº§ç‰¹å¾ä»¥æ„å»ºç”¨äºæ­£å¸¸æ¨¡å¼é‡å»ºçš„ä¸°å¯ŒåµŒå…¥ã€‚ åè€…çš„ç›®æ ‡æ˜¯ä¿ç•™æœ‰åˆ©äºå­¦ç”Ÿè§£ç æ•™å¸ˆååº”çš„åŸºæœ¬ä¿¡æ¯ã€‚\næˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åå‘è’¸é¦èŒƒå¼å®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚ æ‰€æå‡ºçš„ OCBE æ¨¡å—è¿›ä¸€æ­¥å°†æ€§èƒ½æé«˜åˆ°æ–°çš„æœ€å…ˆè¿› (SOTA) è®°å½•ã€‚ æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š\n\n\næˆ‘ä»¬ä¸ºå¼‚å¸¸æ£€æµ‹å¼•å…¥äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„é€†å‘è’¸é¦èŒƒå¼ã€‚ ç¼–ç å™¨-è§£ç å™¨ç»“æ„å’Œåå‘çŸ¥è¯†è’¸é¦ç­–ç•¥æ•´ä½“è§£å†³äº†ä¼ ç»Ÿ KD æ¨¡å‹ä¸­çš„éåŒºåˆ†è¿‡æ»¤å™¨é—®é¢˜ï¼Œæé«˜äº† T-S æ¨¡å‹å¯¹å¼‚å¸¸çš„åˆ¤åˆ«èƒ½åŠ›ã€‚\n\næˆ‘ä»¬æå‡ºäº†ä¸€ç±»ç“¶é¢ˆåµŒå…¥æ¨¡å—ï¼Œå°†æ•™å¸ˆçš„é«˜ç»´ç‰¹å¾æŠ•å½±åˆ°ç´§å‡‘çš„ä¸€ç±»åµŒå…¥ç©ºé—´ã€‚ è¿™é¡¹åˆ›æ–°æœ‰åŠ©äºä¿ç•™ä¸°å¯Œè€Œç´§å‡‘çš„ä»£ç ï¼Œä»¥ä¾¿åœ¨å­¦ç”Ÿå¤„è¿›è¡Œæ— å¼‚å¸¸è¡¨ç¤ºæ¢å¤ã€‚\næˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ–°çš„ SOTA æ€§èƒ½ã€‚\n\n\n\n2. Related Work\nThis section briefly reviews previous efforts on unsuper- vised anomaly detection. We will highlight the similarity and difference between the proposed method and prior arts.\nClassical anomaly detection methods focus on defining a compact closed one-class distribution using normal sup- port vectors. The pioneer studies include one-class support vector machine (OC-SVM) [35] and support vector data description (SVDD) [36]. To cope with high-dimensional data, DeepSVDD [31] and PatchSVDD [43] estimate data representations through deep networks.\nAnother unsupervised AD prototype is the use of gener- ative models, such as AutoEncoder (AE) [19] and Genera- tive Adversarial Nets (GAN) [12], for sample reconstruc- tion. These methods rely on the hypothesis that genera- tive models trained on normal samples only can success- fully reconstruct anomaly-free regions, but fail for anoma- lous regions [2, 5, 34]. However, recent studies show that deep models generalize so well that even anomalous re- gions can be well-restored [46]. To address this issue, memory mechanism [11, 16, 26] , image masking strat- egy [42, 46] and pseudo-anomaly [28, 45] are incorporated in reconstruction-based methods. However, these meth- ods still lack a strong discriminating ability for real-world anomaly detection [3, 5]. Recently, Metaformer (MF) [40] proposes the use of meta-learning [9] to bridge model adap- tation and reconstruction gap for reconstruction-based ap- proaches. Notably, the proposed reverse knowledge distil- lation also adopts the encoder-decoder architecture, but it differs from construction-based methods in two-folds. First, the encoder in a generative model is jointly trained with the decoder, while our reverse distillation freezes a pre-trained model as the teacher. Second, instead of pixel-level recon- struction error, it performs anomaly detection on the seman- tic feature space.\n\n\n\n\n\n\n\n\n\næœ¬èŠ‚ç®€è¦å›é¡¾äº†ä»¥å‰åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„åŠªåŠ›ã€‚ æˆ‘ä»¬å°†å¼ºè°ƒæ‰€æå‡ºçš„æ–¹æ³•ä¸ç°æœ‰æŠ€æœ¯ä¹‹é—´çš„ç›¸ä¼¼ä¹‹å¤„å’Œä¸åŒä¹‹å¤„ã€‚\nç»å…¸çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾§é‡äºä½¿ç”¨æ­£æ€æ”¯æŒå‘é‡å®šä¹‰ç´§å‡‘çš„å°é—­ä¸€ç±»åˆ†å¸ƒã€‚ å…ˆé©±ç ”ç©¶åŒ…æ‹¬ä¸€ç±»æ”¯æŒå‘é‡æœºï¼ˆOC-SVMï¼‰[35]å’Œæ”¯æŒå‘é‡æ•°æ®æè¿°ï¼ˆSVDDï¼‰[36]ã€‚ ä¸ºäº†å¤„ç†é«˜ç»´æ•°æ®ï¼ŒDeepSVDD [31] å’Œ PatchSVDD [43] é€šè¿‡æ·±åº¦ç½‘ç»œä¼°è®¡æ•°æ®è¡¨ç¤ºã€‚\nå¦ä¸€ä¸ªæ— ç›‘ç£çš„ AD åŸå‹æ˜¯ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚è‡ªåŠ¨ç¼–ç å™¨ (AE) [19] å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) [12]ï¼Œç”¨äºæ ·æœ¬é‡å»ºã€‚ è¿™äº›æ–¹æ³•ä¾èµ–äºè¿™æ ·ä¸€ä¸ªå‡è®¾ï¼Œå³åœ¨æ­£å¸¸æ ·æœ¬ä¸Šè®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹åªèƒ½æˆåŠŸåœ°é‡å»ºæ— å¼‚å¸¸åŒºåŸŸï¼Œä½†å¯¹äºå¼‚å¸¸åŒºåŸŸåˆ™å¤±è´¥ [2, 5, 34]ã€‚ ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›éå¸¸å¥½ï¼Œå³ä½¿æ˜¯å¼‚å¸¸åŒºåŸŸä¹Ÿå¯ä»¥å¾ˆå¥½åœ°æ¢å¤ [46]ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®°å¿†æœºåˆ¶[11ã€16ã€26]ã€å›¾åƒæ©è”½ç­–ç•¥[42ã€46]å’Œä¼ªå¼‚å¸¸[28ã€45]è¢«çº³å…¥åŸºäºé‡å»ºçš„æ–¹æ³•ä¸­ã€‚ ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¯¹äºç°å®ä¸–ç•Œçš„å¼‚å¸¸æ£€æµ‹ä»ç„¶ç¼ºä¹å¾ˆå¼ºçš„è¾¨åˆ«èƒ½åŠ›[3, 5]ã€‚ æœ€è¿‘ï¼ŒMetaformer (MF) [40] æå‡ºä½¿ç”¨å…ƒå­¦ä¹  [9] æ¥å¼¥åˆåŸºäºé‡å»ºçš„æ–¹æ³•çš„æ¨¡å‹é€‚åº”å’Œé‡å»ºå·®è·ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„åå‘çŸ¥è¯†è’¸é¦ä¹Ÿé‡‡ç”¨äº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œä½†å®ƒä¸åŸºäºæ„é€ çš„æ–¹æ³•æœ‰ä¸¤æ–¹é¢çš„ä¸åŒã€‚ é¦–å…ˆï¼Œç”Ÿæˆæ¨¡å‹ä¸­çš„ç¼–ç å™¨ä¸è§£ç å™¨è”åˆè®­ç»ƒï¼Œè€Œæˆ‘ä»¬çš„é€†å‘è’¸é¦å°†é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å†»ç»“ä¸ºæ•™å¸ˆã€‚ å…¶æ¬¡ï¼Œå®ƒä¸æ˜¯åƒç´ çº§çš„é‡å»ºé”™è¯¯ï¼Œè€Œæ˜¯å¯¹è¯­ä¹‰ç‰¹å¾ç©ºé—´è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚\nData augmentation strategy is also widely used. By adding pseudo anomalies in the provided anomaly-free samples, the unsupervised task is converted to a supervised learning task [23, 42, 46]. However, these approaches are prone to bias towards pseudo outliers and fail to detect a large variety of anomaly types. For example, CutPaste [23] generates pseudo outliers by adding small patches onto nor- mal images and trains a model to detect these anomalous regions. Since the model focuses on detecting local fea- tures such as edge discontinuity and texture perturbations, it fails to detect and localize large defects and global struc- tural anomalies as shown in Fig. 6.\nRecently, networks pre-trained on the large dataset are proven to be capable of extracting discriminative features for anomaly detection [7,8,23,25,29,30]. With a pre-trained model, memorizing its anomaly-free features helps to iden- tify anomalous samples [7, 29]. The studies in [8, 30] show that using the Mahalanobis distance to measure the simi- larity between anomalies and anomaly-free features leads to accurate anomaly detection. Since these methods re- quire memorizing all features from training samples, they are computationally expensive.\nKnowledge distillation from pre-trained models is an- other potential solution to anomaly detection. In the con- text of unsupervised AD, since the student model is ex- posed to anomaly-free samples in knowledge distillation, the T-S model is expected to generate discrepant features on anomalies in inference [4,33,39]. To further increase the discrimnating capability of the T-S model on various types of abnormalities, different strategies are introduced. For in- stance, in order to capture multi-scale anomaly, US [4] en- sembles several models trained on normal data at different scales, and MKD [33] propose to use multi-level features alignment. It should be noted that though the proposed method is also based on knowledge distillation, our reverse distillation is the first to adopt an encoder and a decoder to construct the T-S model. The heterogeneity of the teacher and student networks and reverse data flow in knowledge distillation distinguishes our method from prior arts.\n\n\n\n\n\n\n\n\n\næ•°æ®å¢å¼ºç­–ç•¥ä¹Ÿè¢«å¹¿æ³›ä½¿ç”¨ã€‚ é€šè¿‡åœ¨æä¾›çš„æ— å¼‚å¸¸æ ·æœ¬ä¸­æ·»åŠ ä¼ªå¼‚å¸¸ï¼Œå°†æ— ç›‘ç£ä»»åŠ¡è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ [23,42,46]ã€‚ ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å®¹æ˜“åå‘ä¼ªå¼‚å¸¸å€¼ï¼Œå¹¶ä¸”æ— æ³•æ£€æµ‹åˆ°å¤šç§å¼‚å¸¸ç±»å‹ã€‚ ä¾‹å¦‚ï¼ŒCutPaste [23] é€šè¿‡åœ¨æ­£å¸¸å›¾åƒä¸Šæ·»åŠ å°å—æ¥ç”Ÿæˆä¼ªå¼‚å¸¸å€¼ï¼Œå¹¶è®­ç»ƒæ¨¡å‹æ¥æ£€æµ‹è¿™äº›å¼‚å¸¸åŒºåŸŸã€‚ ç”±äºè¯¥æ¨¡å‹ä¾§é‡äºæ£€æµ‹å±€éƒ¨ç‰¹å¾ï¼Œä¾‹å¦‚è¾¹ç¼˜ä¸è¿ç»­æ€§å’Œçº¹ç†æ‰°åŠ¨ï¼Œå› æ­¤æ— æ³•æ£€æµ‹å’Œå®šä½å¤§ç¼ºé™·å’Œå…¨å±€ç»“æ„å¼‚å¸¸ï¼Œå¦‚å›¾ 6 æ‰€ç¤ºã€‚\næœ€è¿‘ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ç½‘ç»œè¢«è¯æ˜èƒ½å¤Ÿæå–ç”¨äºå¼‚å¸¸æ£€æµ‹çš„åˆ¤åˆ«ç‰¹å¾ [7,8,23,25,29,30]ã€‚ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œè®°ä½å…¶æ— å¼‚å¸¸ç‰¹å¾æœ‰åŠ©äºè¯†åˆ«å¼‚å¸¸æ ·æœ¬ [7, 29]ã€‚  [8, 30] ä¸­çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é©¬æ°è·ç¦»æ¥æµ‹é‡å¼‚å¸¸å’Œæ— å¼‚å¸¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§å¯ä»¥å®ç°å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹ã€‚ ç”±äºè¿™äº›æ–¹æ³•éœ€è¦è®°ä½è®­ç»ƒæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾ï¼Œå› æ­¤å®ƒä»¬çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚\næ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è’¸é¦æ˜¯å¼‚å¸¸æ£€æµ‹çš„å¦ä¸€ä¸ªæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚ åœ¨æ— ç›‘ç£ AD çš„èƒŒæ™¯ä¸‹ï¼Œç”±äºå­¦ç”Ÿæ¨¡å‹åœ¨çŸ¥è¯†è’¸é¦ä¸­æš´éœ²äºæ— å¼‚å¸¸æ ·æœ¬ï¼Œå› æ­¤ T-S æ¨¡å‹é¢„è®¡ä¼šåœ¨æ¨ç†å¼‚å¸¸ä¸Šäº§ç”Ÿå·®å¼‚ç‰¹å¾ [4,33,39]ã€‚ ä¸ºäº†è¿›ä¸€æ­¥æé«˜ T-S æ¨¡å‹å¯¹å„ç±»å¼‚å¸¸çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå¼•å…¥äº†ä¸åŒçš„ç­–ç•¥ã€‚ ä¾‹å¦‚ï¼Œä¸ºäº†æ•è·å¤šå°ºåº¦å¼‚å¸¸ï¼ŒUS [4] é›†æˆäº†å‡ ä¸ªåœ¨ä¸åŒå°ºåº¦çš„æ­£å¸¸æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ŒMKD [33] å»ºè®®ä½¿ç”¨å¤šçº§ç‰¹å¾å¯¹é½ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶æ‰€æå‡ºçš„æ–¹æ³•ä¹Ÿæ˜¯åŸºäºçŸ¥è¯†è’¸é¦çš„ï¼Œä½†æˆ‘ä»¬çš„é€†å‘è’¸é¦æ˜¯ç¬¬ä¸€ä¸ªé‡‡ç”¨ç¼–ç å™¨å’Œè§£ç å™¨æ¥æ„å»º T-S æ¨¡å‹çš„æ–¹æ³•ã€‚ æ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œçš„å¼‚è´¨æ€§ä»¥åŠçŸ¥è¯†è’¸é¦ä¸­çš„åå‘æ•°æ®æµå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æŠ€æœ¯åŒºåˆ†å¼€æ¥ã€‚\n","slug":"ä¸€ç±»åµŒå…¥åå‘è’¸é¦","date":"2022-05-04T03:30:00.000Z","categories_index":"","tags_index":"æ·±åº¦å­¦ä¹ ,å¼‚å¸¸åˆ†å‰²","author_index":"Star"},{"id":"4346091fabb18c4f718acfd51c087897","title":"Pytorch & python","content":"pytorch\nå‚æ•° &amp; å‘½ä»¤è¡Œ &amp; è¾…åŠ©\nlogger\nloggeræ¨¡å—è§£é‡Š â€”â€” CSDN\nloggerä½¿ç”¨æ¡ˆä¾‹\nloggingæ¨¡å—æ˜¯Pythonå†…ç½®çš„æ ‡å‡†æ¨¡å—ï¼Œä¸»è¦ç”¨äºè¾“å‡ºè¿è¡Œæ—¥å¿—ï¼Œå¯ä»¥è®¾ç½®è¾“å‡ºæ—¥å¿—çš„ç­‰çº§ã€æ—¥å¿—ä¿å­˜è·¯å¾„ã€æ—¥å¿—æ–‡ä»¶å›æ»šç­‰\nyacs.config\nyacsä½¿ç”¨ â€”â€” çŸ¥ä¹\nyacsåº“ï¼Œç”¨äºä¸ºä¸€ä¸ªç³»ç»Ÿæ„å»ºconfigæ–‡ä»¶\néœ€è¦åˆ›å»ºCN()è¿™ä¸ªä½œä¸ºå®¹å™¨æ¥è£…è½½æˆ‘ä»¬çš„å‚æ•°ï¼Œè¿™ä¸ªå®¹å™¨å¯ä»¥åµŒå¥—\nè®¾å¤‡ç›¸å…³\ntorch.cuda.synchronize()\nç­‰å¾…å½“å‰è®¾å¤‡ä¸Šæ‰€æœ‰æµä¸­çš„æ‰€æœ‰æ ¸å¿ƒå®Œæˆã€‚\nğŸŒ°ï¼šæµ‹è¯•æ—¶é—´çš„ä»£ç \n# code 1\nstart = time.time()\nresult = model(input)\nend = time.time()\n\n# code 2\ntorch.cuda.synchronize()\nstart = time.time()\nresult = model(input)\ntorch.cuda.synchronize()\nend = time.time()\n\n# code 3\nstart = time.time()\nresult = model(input)\nprint(result)\nend = time.time()\nä»£ç 2æ˜¯æ­£ç¡®çš„ã€‚å› ä¸ºåœ¨pytorché‡Œé¢ï¼Œç¨‹åºçš„æ‰§è¡Œéƒ½æ˜¯å¼‚æ­¥çš„ã€‚\nå¦‚æœé‡‡ç”¨ä»£ç 1ï¼Œæµ‹è¯•çš„æ—¶é—´ä¼šå¾ˆçŸ­ï¼Œå› ä¸ºæ‰§è¡Œå®Œend=time.time()ç¨‹åºå°±é€€å‡ºäº†ï¼Œåå°çš„cuä¹Ÿå› ä¸ºpythonçš„é€€å‡ºé€€å‡ºäº†ã€‚\nå¦‚æœé‡‡ç”¨ä»£ç 2ï¼Œä»£ç ä¼šåŒæ­¥cuçš„æ“ä½œï¼Œç­‰å¾…gpuä¸Šçš„æ“ä½œéƒ½å®Œæˆäº†å†ç»§ç»­æˆå½¢end = time.time()\nä»£ç 3å’Œä»£ç 2çš„æ—¶é—´æ˜¯ç±»ä¼¼çš„ã€‚\nå› ä¸ºä»£ç 3ä¼šç­‰å¾…gpuä¸Šçš„ç»“æœæ‰§è¡Œå®Œä¼ ç»™printå‡½æ•°ï¼Œæ‰€ä»¥æ—¶é—´å°±å’Œä»£ç 2åŒæ­¥çš„æ“ä½œçš„æ—¶é—´åŸºæœ¬ä¸Šæ˜¯ä¸€è‡´çš„äº†ã€‚\nå°†print(result)æ¢æˆresult.cpu()ç»“æœæ˜¯ä¸€è‡´çš„ã€‚\næ•°æ®åŠ è½½\nå›¾åƒæ•°æ®å˜æ¢\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\nNormalizeæ˜¯æŠŠå›¾åƒæ•°æ®ä»[0,1]å˜æˆ[-1,1]ï¼Œå˜æ¢å…¬å¼æ˜¯image=(image-mean)/stdï¼Œé‚£ä¹ˆå…¶ä¸­çš„å‚æ•°å°±åˆ†åˆ«æ˜¯ä¸‰ä¸ªé€šé“çš„meanå’Œstdï¼Œè¿™ä¸ªå‡å€¼å’Œæ ‡å‡†å·®éœ€è¦è‡ªå·±è®¡ç®—ï¼ŒèŒƒå›´å°±æ˜¯è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ‰€æœ‰å›¾åƒã€‚\nDataLoader\nCSDNåŸæ–‡é“¾æ¥\ncollate_fnå‚æ•°ä½¿ç”¨è¯¦è§£ â€”â€” çŸ¥ä¹\nnum_workså‚æ•° â€”â€” CSDN\n\nåŠ è½½ä¸€ä¸ªbatchçš„æ•°æ®è¿™ä¸€æ­¥éœ€è¦ä½¿ç”¨ä¸€ä¸ªtorch.utils.data.DataLoaderå¯¹è±¡ï¼Œå¹¶ä¸”DataLoaderæ˜¯ä¸€ä¸ªåŸºäºæŸä¸ªdatasetçš„iterableï¼Œè¿™ä¸ªiterableæ¯æ¬¡ä»datasetä¸­åŸºäºæŸç§é‡‡æ ·åŸåˆ™å–å‡ºä¸€ä¸ªbatchçš„æ•°æ®ã€‚\nä¹Ÿå¯ä»¥è¿™æ ·è¯´ï¼šTorchä¸­å¯ä»¥åˆ›å»ºä¸€ä¸ªtorch.utils.data.==Dataset==å¯¹è±¡ï¼Œå¹¶ä¸torch.utils.data.==DataLoader==ä¸€èµ·ä½¿ç”¨ï¼Œåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä¸æ–­ä¸ºæ¨¡å‹æä¾›æ•°æ®ã€‚\ntorch.utils.data.DataLoader\nå®šä¹‰ï¼šData loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\næ„é€ å‡½æ•°:\ntorch.utils.data.DataLoader(dataset, \n\t\t\t\t\t\t\tbatch_size=1, \n\t\t\t\t\t\t\tshuffle=False, \n\t\t\t\t\t\t\tsampler=None,\n\t\t\t\t\t\t\tbatch_sampler=None, num_workers=0, collate_fn=None,\n\t\t\t\t\t\t\tpin_memory=False, drop_last=False, timeout=0,\n\t\t\t\t\t\t\tworker_init_fn=None)\n\ndataset: æŠ½è±¡ç±»,åŒ…å«ä¸¤ç§ç±»å‹\n\nmap-style datasets \niterable-style datasets\n\n\nbatch_size : æ¯ä¸€æ¬¡æŠ½æ ·çš„batch-sizeå¤§å°\nshuffle : Trueåˆ™éšæœºæ‰“ä¹±æ•°æ®\nNum_worksï¼šå°†batchåŠ è½½è¿›RAMçš„è¿›ç¨‹æ•°ã€‚å†…å­˜å¼€é”€å¤§ï¼ŒCPUè´Ÿæ‹…å¤§ã€‚å¯èƒ½ä¹‹åå‡ æ¬¡è¿­ä»£çš„æ•°æ®åœ¨æœ¬æ¬¡è¿­ä»£çš„æ—¶å€™å·²ç»åŠ è½½è¿›å†…å­˜ã€‚\ncollate_fnï¼šå¦‚ä½•å–æ ·æœ¬çš„ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰è‡ªå·±çš„å‡½æ•°æ¥å‡†ç¡®åœ°å®ç°æƒ³è¦çš„åŠŸèƒ½ã€‚\ndrop_lastï¼šå‘Šè¯‰å¦‚ä½•å¤„ç†æ•°æ®é›†é•¿åº¦é™¤äºbatch_sizeä½™ä¸‹çš„æ•°æ®ã€‚Trueå°±æŠ›å¼ƒï¼Œå¦åˆ™ä¿ç•™ã€‚\n\nMap-style datasets\næ˜¯ä¸€ä¸ªç±»ï¼Œè¦æ±‚æœ‰ __getitem__()and__len__()è¿™ä¸¤ä¸ªæ„é€ å‡½æ•°ï¼Œä»£è¡¨ä¸€ä¸ªä»ç´¢å¼•æ˜ å°„åˆ°æ•°æ®æ ·æœ¬ã€‚\n\n__getitem__(): æ ¹æ®ç´¢å¼•indexéå†æ•°æ®\n__len__(): è¿”å›æ•°æ®é›†çš„é•¿åº¦\nå¯ç¼–å†™ç‹¬ç«‹çš„æ•°æ®å¤„ç†å‡½æ•°\n\nåœ¨ __getitem()__ å‡½æ•°ä¸­è¿›è¡Œè°ƒç”¨\nç›´æ¥å°†æ•°æ®å¤„ç†å‡½æ•°å†™åœ¨ __getitem()__ æˆ–è€… __init()__ å‡½æ•°ä¸­ï¼Œä½†æ˜¯__getitem()__\nå¿…é¡»æ ¹æ®==index==è¿”å›å“åº”çš„å€¼ï¼Œè¯¥å€¼ä¼šé€šè¿‡indexä¼ åˆ°dataloaderä¸­è¿›è¡Œåç»­çš„batchæ‰¹å¤„ç†ã€‚\n\n\n\nåŸºæœ¬éœ€è¦æ»¡è¶³ï¼š\ndef __getitem__(self, index):\n    return self.src[index], self.trg[index]\n\ndef __len__(self):\n\treturn len(self.src)  \ngetitem()æ–¹æ³•ç”¨æ¥ä»datasetsä¸­è¯»å–ä¸€æ¡æ•°æ®ï¼Œè¿™æ¡æ•°æ®åŒ…å«è®­ç»ƒå›¾ç‰‡ï¼ˆå·²CVè·ç¦»ï¼‰å’Œæ ‡ç­¾ï¼Œå‚æ•°indexè¡¨ç¤ºå›¾ç‰‡å’Œæ ‡ç­¾åœ¨æ€»æ•°æ®é›†ä¸­çš„Indexã€‚\nlen()æ–¹æ³•è¿”å›æ•°æ®é›†çš„æ€»é•¿åº¦ï¼ˆè®­ç»ƒé›†çš„æ€»æ•°ï¼‰ã€‚\nå®ç° MyDatasets ç±»\n\nç®€å•ç›´ç™½\n\næŠŠ x å’Œ label åˆ†åˆ«è£…å…¥ä¸¤ä¸ªåˆ—è¡¨ self.src å’Œ self.trg ï¼Œç„¶åé€šè¿‡ getitem(self, idex) è¿”å›å¯¹åº”å…ƒç´ \nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n \nclass My_dataset(Dataset):\n    def __init__(self):\n        super().__init__()\n        ## ä½¿ç”¨sinå‡½æ•°è¿”å›10000ä¸ªæ—¶é—´åºåˆ—,å¦‚æœä¸è‡ªå·±æ„é€ æ•°æ®ï¼Œå°±ä½¿ç”¨numpy,pandasç­‰è¯»å–è‡ªå·±çš„æ•°æ®ä¸ºxå³å¯ã€‚\n        ## ä»¥ä¸‹æ•°æ®ç»„ç»‡è¿™å—æ—¢å¯ä»¥æ”¾åœ¨initæ–¹æ³•é‡Œï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨getitemæ–¹æ³•é‡Œ\n        self.x = torch.randn(1000,3)\n        self.y = self.x.sum(axis=1)\n        self.src,  self.trg = [], []\n        for i in range(1000):\n            self.src.append(self.x[i])\n            self.trg.append(self.y[i])\n    \n           \n    def __getitem__(self, index):\n        return self.src[index], self.trg[index]\n\n    def __len__(self):\n        return len(self.src) \n        \n ## æˆ–è€…return len(self.trg), srcå’Œtrgé•¿åº¦ä¸€æ ·\n \ndata_train = My_dataset()\ndata_test = My_dataset()\ndata_loader_train = DataLoader(data_train, batch_size=5, shuffle=False)\ndata_loader_test = DataLoader(data_test, batch_size=5, shuffle=False)\n## i_batchçš„å¤šå°‘æ ¹æ®batch sizeå’Œdef __len__(self)è¿”å›çš„é•¿åº¦ç¡®å®š\n## batch_dataè¿”å›çš„å€¼æ ¹æ®def __getitem__(self, index)æ¥ç¡®å®š\n## å¯¹è®­ç»ƒé›†ï¼š(ä¸å¤ªæ¸…æ¥šenumerateè¿”å›ä»€ä¹ˆçš„æ—¶å€™å°±å¤šprintè¯•è¯•)\nfor i_batch, batch_data in enumerate(data_loader_train):\n    print(i_batch)  ## æ‰“å°batchç¼–å·\n    print(batch_data[0])  ## æ‰“å°è¯¥batché‡Œé¢src\n    print(batch_data[1])  ## æ‰“å°è¯¥batché‡Œé¢trg\n## å¯¹æµ‹è¯•é›†ï¼šï¼ˆä¸‹é¢çš„è¯­å¥ä¹Ÿå¯ä»¥ï¼‰\nfor i_batch, (src, trg) in enumerate(data_loader_test):\n    print(i_batch)  ## æ‰“å°batchç¼–å·\n    print(src)  ## æ‰“å°è¯¥batché‡Œé¢srcçš„å°ºå¯¸\n    print(trg)  ## æ‰“å°è¯¥batché‡Œé¢trgçš„å°ºå¯¸    \nç”Ÿæˆçš„data_trainå¯ä»¥é€šè¿‡ data_train[xxx]ç›´æ¥ç´¢å¼•æŸä¸ªå…ƒç´ ï¼Œæˆ–è€…é€šè¿‡next(iter(data_train)) å¾—åˆ°ä¸€æ¡æ¡çš„æ•°æ®ã€‚\n\nå€ŸåŠ©TensorDatasetå°†æ•°æ®åŒ…è£…æˆdataset\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n \nsrc = torch.sin(torch.arange(1, 1000, 0.1))\ntrg = torch.cos(torch.arange(1, 1000, 0.1))\n \ndata = TensorDataset(src, trg)\ndata_loader = DataLoader(data, batch_size=5, shuffle=False)\nfor i_batch, batch_data in enumerate(data_loader):\n    print(i_batch)  ## æ‰“å°batchç¼–å·\n    print(batch_data[0].size())  ## æ‰“å°è¯¥batché‡Œé¢src\n    print(batch_data[1].size())  ## æ‰“å°è¯¥batché‡Œé¢trg\n\nåœ°å€è¯»å–ï¼Œç”Ÿæˆæ•°æ®çš„è·¯å¾„ txtæ–‡ä»¶\n\nimport os\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport matplotlib.image as mpimg\n\n\n\n## å¯¹æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆpath-label map.txt è¿™ä¸ªç¨‹åºå¯æ ¹æ®å®é™…éœ€è¦é€‚å½“ä¿®æ”¹\ndef generate_map(root_dir):\n\t##å¾—åˆ°å½“å‰ç»å¯¹è·¯å¾„\n    current_path = os.path.abspath('.')\n    ##os.path.dirname()å‘å‰é€€ä¸€ä¸ªè·¯å¾„\n    father_path = os.path.abspath(os.path.dirname(current_path) + os.path.sep + \".\")\n\n    with open(root_dir + 'map.txt', 'w') as wfp:\n        for idx in range(10):\n            subdir = os.path.join(root_dir, '%d/' % idx)\n            for file_name in os.listdir(subdir):\n                abs_name = os.path.join(father_path, subdir, file_name)\n                ## linux_abs_name = abs_name.replace(\"\\\\\", '/')\n                wfp.write('&#123;file_dir&#125; &#123;label&#125;\\n'.format(file_dir=linux_abs_name, label=idx))\n\n## å®ç°MyDatasetsç±»\nclass MyDatasets(Dataset):\n\n    def __init__(self, dir):\n        ## è·å–æ•°æ®å­˜æ”¾çš„dir\n        ## ä¾‹å¦‚d:/images/\n        self.data_dir = dir\n        ## ç”¨äºå­˜æ”¾(image,label) tupleçš„list,å­˜æ”¾çš„æ•°æ®ä¾‹å¦‚(d:/image/1.png,4)\n        self.image_target_list = []\n        ## ä»dir--labelçš„mapæ–‡ä»¶ä¸­å°†æ‰€æœ‰çš„tupleå¯¹è¯»å–åˆ°image_target_listä¸­\n        ## map.txtä¸­å…¨éƒ¨å­˜æ”¾çš„æ˜¯d:/.../image_data/1/3.jpg 1 è·¯å¾„æœ€å¥½æ˜¯ç»å¯¹è·¯å¾„\n        with open(os.path.join(dir, 'map.txt'), 'r') as fp:\n            content = fp.readlines()\n            ##s.rstrip()åˆ é™¤å­—ç¬¦ä¸²æœ«å°¾æŒ‡å®šå­—ç¬¦ï¼ˆé»˜è®¤æ˜¯å­—ç¬¦ï¼‰\n            ## å¾—åˆ° [['d:/.../image_data/1/3.jpg', '1'], ...,]\n            str_list = [s.rstrip().split() for s in content]\n            ## å°†æ‰€æœ‰å›¾ç‰‡çš„dir--labelå¯¹éƒ½æ”¾å…¥åˆ—è¡¨ï¼Œå¦‚æœè¦æ‰§è¡Œå¤šä¸ªepochï¼Œå¯ä»¥åœ¨è¿™é‡Œå¤šå¤åˆ¶å‡ éï¼Œç„¶åç»Ÿä¸€shuffleæ¯”è¾ƒå¥½\n            self.image_target_list = [(x[0], int(x[1])) for x in str_list]\n\n    def __getitem__(self, index):\n        image_label_pair = self.image_target_list[index]\n        ## æŒ‰pathè¯»å–å›¾ç‰‡æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸ºå›¾ç‰‡æ ¼å¼ä¾‹å¦‚[3,32,32]\n        ## å¯ä»¥ç”¨åˆ«çš„ä»£æ›¿\n        img = mpimg.imread(image_label_pair[0])\n        return img, image_label_pair[1]\n\n    def __len__(self):\n        return len(self.image_target_list)\n\n\nif __name__ == '__main__':\n    ## ç”Ÿæˆmap.txt\n    ## generate_map('train/')\n\n    train_loader = DataLoader(MyDatasets('train/'), batch_size=128, shuffle=True)\n\n    for step in range(20000):\n        for idx, (img, label) in enumerate(train_loader):\n            print(img.shape)\n            print(label.shape)\nç½‘ç»œæ­å»ºTrick\nwith torch.no_grad()\n\n\n\n\n\n\n\n\n\nå‚è€ƒï¼šhttps://blog.csdn.net/sazass/article/details/116668755\nä½œç”¨ï¼šåœ¨è¯¥æ¨¡å—ä¸‹ï¼Œæ‰€æœ‰è®¡ç®—å¾—å‡ºçš„tensorçš„requires_gradéƒ½è‡ªåŠ¨è®¾ç½®ä¸ºFalseã€‚å½“requires_gradè®¾ç½®ä¸ºFalseæ—¶,åå‘ä¼ æ’­æ—¶å°±ä¸ä¼šè‡ªåŠ¨æ±‚å¯¼äº†ï¼Œå› æ­¤å¤§å¤§èŠ‚çº¦äº†æ˜¾å­˜æˆ–è€…è¯´å†…å­˜ã€‚\nåŸºæœ¬åŠŸèƒ½å‡½æ•°\ntorch.max()\ntorch.max(input) â†’ Tensor:è¿”å›è¾“å…¥tensorä¸­æ‰€æœ‰å…ƒç´ çš„æœ€å¤§å€¼\ntorch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor): æŒ‰ç»´åº¦dim è¿”å›æœ€å¤§å€¼ï¼Œå¹¶ä¸”è¿”å›ç´¢å¼•ã€‚\ntorch.max()[0]ï¼Œ åªè¿”å›æœ€å¤§å€¼çš„æ¯ä¸ªæ•°\n\ntroch.max()[1]ï¼Œ åªè¿”å›æœ€å¤§å€¼çš„æ¯ä¸ªç´¢å¼•\n\ntorch.max()[1].data åªè¿”å›variableä¸­çš„æ•°æ®éƒ¨åˆ†ï¼ˆå»æ‰Variable containing:ï¼‰\n\ntorch.max()[1].data.numpy() æŠŠæ•°æ®è½¬åŒ–æˆnumpy ndarry\n\ntorch.max()[1].data.numpy().squeeze() æŠŠæ•°æ®æ¡ç›®ä¸­ç»´åº¦ä¸º1 çš„åˆ é™¤æ‰\npython\nstr.lower() å…¨éƒ¨è½¬åŒ–ä¸ºå°å†™å­—æ¯\n","slug":"python&pytorchä½¿ç”¨æŒ‡å—","date":"2022-05-03T12:10:00.000Z","categories_index":"","tags_index":"","author_index":"Star"},{"id":"cca6975b74b1218e11c5b9ba5de4d5ef","title":"DMLNet","content":"\nå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²\n\nå¼€é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—\n\né—­é›†è¯­ä¹‰åˆ†å‰²å­æ¨¡å—\nå¼‚å¸¸åˆ†å‰²å­æ¨¡å—\n\n\nå¢é‡å°æ ·æœ¬å­¦ä¹ æ¨¡å—\n\næˆ‘æ˜¯çŸ­å°ç²¾æ‚çš„æ–‡ç« æ‘˜è¦(à¹‘â€¢Ì€ã…‚â€¢Ì) âœ§\n\nCODE\nmultiscale æ˜¯è‡ªå·±è®¾å®šçš„å— cfg.DATASET.imgSizes = (300, 375, 450, 525, 600)\nSeg è½¬åŒ–ä¸ºlong Tensorçš„ç›®çš„æ˜¯ä»€ä¹ˆ\ncolorsçš„ä½œç”¨æ˜¯ä»€ä¹ˆ\nå‡ ä¸ªè¾…åŠ©å‡½æ•°çš„ä½œç”¨ï¼š\nNormalization(x): \nCoefficient_map(x, thre): \nnormfun(x, mu, sigma):\nè®ºæ–‡é˜…è¯»\nå¼•è¨€\nClassical close-set semantic segmentation networks have limited ability to detect out-of-distribution (OOD) objects, which is important for safety-critical applications such as autonomous driving. Incrementally learning these OOD objects with few annotations is an ideal way to enlarge the knowledge base of the deep learning models. In this paper, we propose an open world semantic segmenta- tion system that includes two modules:\n(1) ==an open-set semantic segmentation module to detect both in-distribution and OOD objects==.\n(2) an incremental few-shot learning module to gradually incorporate those OOD objects into its existing knowledge base.\nThis open world semantic segmentation system behaves like a human being, which is able to identify OOD objects and gradually learn them with corresponding supervision.\nWe adopt the ==Deep Metric Learning Network (DMLNet) with contrastive clustering== to implement open-set semantic segmentation. Compared to other open-set semantic segmentation methods, our DMLNet achieves state-of-the-art performance on three challenging open-set semantic segmentation datasets without using additional data or generative models.\nOn this basis, two incremental few-shot learning methods are fur- ther proposed to progressively improve the DMLNet with the annotations of OOD objects\n\nç»å…¸çš„é—­é›†è¯­ä¹‰åˆ†å‰²ç½‘ç»œæ£€æµ‹åˆ†å¸ƒå¤– (OOD) å¯¹è±¡çš„èƒ½åŠ›æœ‰é™ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®å‹åº”ç”¨å¾ˆé‡è¦ã€‚ å¢é‡å­¦ä¹ è¿™äº›å¸¦æœ‰å°‘é‡æ³¨é‡Šçš„ OOD å¯¹è±¡æ˜¯æ‰©å¤§æ·±åº¦å­¦ä¹ æ¨¡å‹çŸ¥è¯†åº“çš„ç†æƒ³æ–¹æ³•ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼š\n(1) ä¸€ä¸ªå¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—ï¼Œç”¨äºæ£€æµ‹å†…åˆ†å¸ƒå’ŒOODå¯¹è±¡ã€‚\n(2) ä¸€ä¸ªå¢é‡çš„å°æ ·æœ¬å­¦ä¹ æ¨¡å—ï¼Œé€æ¸å°†è¿™äº› OOD å¯¹è±¡çº³å…¥å…¶ç°æœ‰çš„çŸ¥è¯†åº“ã€‚\nè¿™ä¸ªå¼€æ”¾ä¸–ç•Œçš„è¯­ä¹‰åˆ†å‰²ç³»ç»Ÿå°±åƒä¸€ä¸ªäººï¼Œèƒ½å¤Ÿè¯†åˆ«OODå¯¹è±¡å¹¶åœ¨ç›¸åº”çš„ç›‘ç£ä¸‹é€æ¸å­¦ä¹ å®ƒä»¬ã€‚\næˆ‘ä»¬é‡‡ç”¨å…·æœ‰==å¯¹æ¯”èšç±»çš„æ·±åº¦åº¦é‡å­¦ä¹ ç½‘ç»œï¼ˆDMLNetï¼‰==æ¥å®ç°å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²ã€‚ ä¸å…¶ä»–å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ DMLNet åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œæ— éœ€ä½¿ç”¨é¢å¤–çš„æ•°æ®æˆ–ç”Ÿæˆæ¨¡å‹ã€‚\nåœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†ä¸¤ç§å¢é‡å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ OOD å¯¹è±¡çš„æ³¨é‡Šé€æ­¥æ”¹è¿› DMLNet\n6. Conclusion\nWe introduce an open world semantic segmentation system which incorporates two modules:\n\nan open-set segmentation module\nan incremental few-shot learning module.\n\nOur proposed open-set segmentation module is based on the deep metric learning network, and it uses the Euclidean distance sum criterion to achieve state-of-the-art performance.\nTwo incremental few-shot learning methods are proposed to broaden the perception knowledge of the network. Both modules of the open world semantic segmentation system can be further studied to improve the performance. We hope our work can draw more researchers to contribute to this practically valuable research direction.\n\næˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿï¼Œå®ƒåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šä¸€ä¸ªå¼€æ”¾é›†åˆ†å‰²æ¨¡å—å’Œä¸€ä¸ªå¢é‡å°æ ·æœ¬å­¦ä¹ æ¨¡å—ã€‚\næˆ‘ä»¬æå‡ºçš„å¼€æ”¾é›†åˆ†å‰²æ¨¡å—åŸºäºæ·±åº¦åº¦é‡å­¦ä¹ ç½‘ç»œï¼Œå®ƒä½¿ç”¨==æ¬§å‡ é‡Œå¾·è·ç¦»å’Œæ ‡å‡†==æ¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚\næå‡ºäº†ä¸¤ç§å¢é‡å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•æ¥æ‹“å®½ç½‘ç»œçš„æ„ŸçŸ¥çŸ¥è¯†ã€‚ å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿçš„ä¸¤ä¸ªæ¨¡å—éƒ½å¯ä»¥è¿›ä¸€æ­¥ç ”ç©¶ä»¥æé«˜æ€§èƒ½ã€‚ æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿå¸å¼•æ›´å¤šçš„ç ”ç©¶äººå‘˜ä¸ºè¿™ä¸ªå…·æœ‰å®é™…ä»·å€¼çš„ç ”ç©¶æ–¹å‘åšå‡ºè´¡çŒ®\n1. ä»‹ç»\nå¾—ç›Šäºé«˜è´¨é‡çš„æ•°æ®é›† [3,4,5]ï¼Œæ·±åº¦å·ç§¯ç½‘ç»œåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ [1, 2] ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ è¿™äº›è¯­ä¹‰åˆ†å‰²ç½‘ç»œåœ¨è®¸å¤šåº”ç”¨ä¸­è¢«ç”¨ä½œæ„ŸçŸ¥ç³»ç»Ÿï¼Œå¦‚è‡ªåŠ¨é©¾é©¶[6]ã€åŒ»ç–—è¯Šæ–­[7]ç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ„ŸçŸ¥ç³»ç»Ÿä¸­çš„å¤§å¤šæ•°éƒ½æ˜¯é—­é›†å’Œé™æ€çš„ã€‚ é—­é›†è¯­ä¹‰åˆ†å‰²å‡è®¾æµ‹è¯•ä¸­çš„æ‰€æœ‰ç±»éƒ½å·²ç»åœ¨è®­ç»ƒæœŸé—´å‚ä¸ï¼Œè¿™åœ¨å¼€æ”¾ä¸–ç•Œä¸­æ˜¯ä¸æ­£ç¡®çš„ã€‚ å¦‚æœé—­é›†ç³»ç»Ÿé”™è¯¯åœ°å°†åˆ†å‘ä¸­æ ‡ç­¾åˆ†é…ç»™ OOD å¯¹è±¡ [8]ï¼Œå®ƒå¯èƒ½ä¼šåœ¨å®‰å…¨å…³é”®å‹åº”ç”¨ç¨‹åºï¼ˆå¦‚è‡ªåŠ¨é©¾é©¶ï¼‰ä¸­é€ æˆç¾éš¾æ€§åæœã€‚ åŒæ—¶ï¼Œé™æ€æ„ŸçŸ¥ç³»ç»Ÿæ— æ³•æ ¹æ®æ‰€è§å†…å®¹æ›´æ–°å…¶çŸ¥è¯†åº“ï¼Œå› æ­¤ï¼Œå®ƒä»…é™äºç‰¹å®šåœºæ™¯ï¼Œéœ€è¦åœ¨ä¸€å®šæ—¶é—´åé‡æ–°è®­ç»ƒã€‚ ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼€æ”¾é›†çš„åŠ¨æ€æ„ŸçŸ¥ç³»ç»Ÿï¼Œç§°ä¸ºå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿã€‚ å®ƒåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼š\nï¼ˆ1ï¼‰ä¸€ä¸ªå¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—ï¼Œç”¨äºæ£€æµ‹OODå¯¹è±¡å¹¶å°†æ­£ç¡®çš„æ ‡ç­¾åˆ†é…ç»™åˆ†å¸ƒä¸­çš„å¯¹è±¡ã€‚\n(2) ä¸€ä¸ªå¢é‡çš„å°æ ·æœ¬å­¦ä¹ æ¨¡å—ï¼Œå°†è¿™äº›æœªçŸ¥å¯¹è±¡é€æ­¥åˆå¹¶åˆ°å…¶ç°æœ‰çš„çŸ¥è¯†åº“ä¸­ã€‚\næˆ‘ä»¬æå‡ºçš„å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿçš„æ•´ä¸ªæµç¨‹å¦‚å›¾ 1 æ‰€ç¤º\nå¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²å’Œå¢é‡å°æ ·æœ¬å­¦ä¹ éƒ½æ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„è§£å†³ã€‚\nå¯¹äºå¼€é›†è¯­ä¹‰åˆ†å‰²ï¼Œæœ€é‡è¦çš„éƒ¨åˆ†æ˜¯åœ¨ä¸€å¼ å›¾åƒçš„æ‰€æœ‰åƒç´ ä¸­è¯†åˆ«OODåƒç´ ï¼Œç§°ä¸ºå¼‚å¸¸åˆ†å‰²ã€‚ å¼‚å¸¸åˆ†å‰²çš„å…¸å‹æ–¹æ³•æ˜¯å°†å›¾åƒçº§çš„å¼€é›†åˆ†ç±»æ–¹æ³•åº”ç”¨äºåƒç´ çº§çš„å¼€é›†åˆ†ç±»ã€‚\nè¿™äº›æ–¹æ³•åŒ…æ‹¬åŸºäºä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–¹æ³• [9, 10, 11, 12] å’ŒåŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æ–¹æ³• [13, 14]ã€‚ ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•å·²è¢«è¯æ˜åœ¨é©¾é©¶åœºæ™¯ä¸­æ— æ•ˆï¼Œå› ä¸ºåŸºäºä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–¹æ³•==ä¼šç»™å‡ºè®¸å¤šå‡é˜³æ€§å¼‚å¸¸å€¼æ£€æµ‹== [15] å¹¶ä¸”è‡ªåŠ¨ç¼–ç å™¨==æ— æ³•é‡æ–°ç”Ÿæˆå¤æ‚çš„åŸå¸‚åœºæ™¯== [16]ã€‚ æœ€è¿‘ï¼ŒåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆåŸºäº GANï¼‰çš„æ–¹æ³• [16, 17] å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»¬è¿œ==éè½»é‡çº§==ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦åœ¨ç®¡é“ä¸­ä½¿ç”¨å¤šä¸ªæ·±åº¦ç½‘ç»œã€‚\nå¯¹äºå¢é‡å°‘æ ·æœ¬å­¦ä¹ ï¼Œæˆ‘ä»¬ä¸ä»…è¦å¤„ç†å¢é‡å­¦ä¹ çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç¾éš¾æ€§é—å¿˜[18]ï¼Œè¿˜è¦å¤„ç†å°‘æ ·æœ¬å­¦ä¹ çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»å°‘é‡æ ·æœ¬ä¸­æå–ä»£è¡¨æ€§ç‰¹å¾[19]\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ DMLNet æ¥è§£å†³å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚ åŸå› æœ‰ä¸‰ï¼š\n(1) DMLNetçš„åˆ†ç±»åŸç†æ˜¯åŸºäºå¯¹æ¯”èšç±»ï¼Œå¯ä»¥æœ‰æ•ˆè¯†åˆ«å¼‚å¸¸ç‰©ä½“ï¼Œå¦‚å›¾2æ‰€ç¤º\n\n\n\n\n\n\n\n\n\n\nåº¦é‡å­¦ä¹ ï¼šä»æ•°æ®ä¸­å­¦ä¹ ä¸€ç§åº¦é‡æ•°æ®å¯¹è±¡é—´è·ç¦»çš„æ–¹æ³•ã€‚å…¶ç›®æ ‡æ˜¯ä½¿å¾—åœ¨å­¦å¾—çš„è·ç¦»åº¦é‡ä¸‹ï¼Œç›¸ä¼¼å¯¹è±¡é—´çš„è·ç¦»å°ï¼Œä¸ç›¸ä¼¼å¯¹è±¡é—´çš„è·ç¦»å¤§ã€‚\nä¼ ç»Ÿçš„åº¦é‡å­¦ä¹ æ–¹æ³•åªèƒ½å­¦ä¹ å‡ºçº¿æ€§ç‰¹å¾ï¼Œè™½ç„¶æœ‰ä¸€äº›èƒ½å¤Ÿæå–éçº¿æ€§ç‰¹å¾çš„æ ¸æ–¹æ³•è¢«æå‡ºï¼Œä½†å¯¹å­¦ä¹ æ•ˆæœä¹Ÿæ²¡æœ‰æ˜æ˜¾æå‡\næ·±åº¦åº¦é‡å­¦ä¹ ï¼šæ·±åº¦å­¦ä¹ çš„æ¿€æ´»å‡½æ•°å­¦ä¹ éçº¿æ€§ç‰¹å¾çš„ä¼˜ç§€èƒ½åŠ›ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨åœ°ä»åŸå§‹æ•°æ®ä¸­å­¦å‡ºé«˜è´¨é‡çš„ç‰¹å¾ã€‚å› æ­¤æ·±åº¦å­¦ä¹ çš„ç½‘ç»œç»“æ„ä¸ä¼ ç»Ÿçš„åº¦é‡å­¦ä¹ æ–¹æ³•ç›¸ç»“åˆèƒ½å¤Ÿå¸¦æ¥ç†æƒ³çš„æ•ˆæœã€‚\n(2) DMLNetç»“åˆåŸå‹éå¸¸é€‚åˆfew-shot ä»»åŠ¡[19]ã€‚\n(3) DMLNet çš„å¢é‡å­¦ä¹ å¯ä»¥é€šè¿‡æ·»åŠ æ–°çš„åŸå‹æ¥å®ç°ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç„¶è€Œæœ‰ç”¨çš„æ–¹æ³• [20]ã€‚\nåŸºäº DMLNet æ¶æ„ï¼Œæˆ‘ä»¬ä¸ºå¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—å¼€å‘äº†ä¸¤ç§æœªçŸ¥è¯†åˆ«æ ‡å‡†ï¼Œä¸ºå¢é‡å°‘æ ·æœ¬å­¦ä¹ æ¨¡å—å¼€å‘äº†ä¸¤ç§æ–¹æ³•ã€‚\næ ¹æ®æˆ‘ä»¬çš„å®éªŒï¼Œè¿™ä¸¤ä¸ªæ¨¡å—éƒ½è¢«éªŒè¯ä¸ºæœ‰æ•ˆä¸”è½»é‡çº§çš„ã€‚ æ€»è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„è´¡çŒ®å¦‚ä¸‹ï¼š\n\næˆ‘ä»¬ç‡å…ˆæ¨å‡ºå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿï¼Œåœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ ç¨³å¥å®ç”¨ã€‚\næˆ‘ä»¬æå‡ºçš„åŸºäº DMLNet çš„å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚\næˆ‘ä»¬æå‡ºçš„few-shot å¢é‡å­¦ä¹ æ¨¡å—æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚\né€šè¿‡ç»“åˆæˆ‘ä»¬æå‡ºçš„å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—å’Œå¢é‡å°‘æ ·æœ¬å­¦ä¹ æ¨¡å—ï¼Œå®ç°äº†ä¸€ä¸ªå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿã€‚\n\n2. Related Work\n2.1 å¼‚å¸¸è¯­ä¹‰åˆ†å‰²\nå¼‚å¸¸è¯­ä¹‰åˆ†å‰²çš„æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸¤ç§è¶‹åŠ¿ï¼š  åŸºäºä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–¹æ³•å’ŒåŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ã€‚\nä¸ç¡®å®šæ€§ä¼°è®¡çš„åŸºçº¿æ˜¯æœ€å¤§softmaxæ¦‚ç‡ï¼ˆMSPï¼‰ï¼Œå®ƒé¦–å…ˆåœ¨[9]ä¸­æå‡ºã€‚  Dan ç­‰äººæ²¡æœ‰ä½¿ç”¨ softmax æ¦‚ç‡ã€‚  [11]æå‡ºä½¿ç”¨æœ€å¤§logitï¼ˆMaxLogitï¼‰å¹¶å–å¾—æ›´å¥½çš„å¼‚å¸¸åˆ†å‰²æ€§èƒ½ã€‚ è´å¶æ–¯ç½‘ç»œé‡‡ç”¨æ·±åº¦å­¦ä¹ ç½‘ç»œçš„æ¦‚ç‡è§‚ç‚¹ï¼Œæ‰€ä»¥å®ƒä»¬çš„æƒé‡å’Œè¾“å‡ºæ˜¯æ¦‚ç‡åˆ†å¸ƒè€Œä¸æ˜¯ç‰¹å®šçš„æ•°å­— [21, 22]ã€‚ åœ¨å®è·µä¸­ï¼ŒDropout [10] æˆ–é›†æˆ [12] é€šå¸¸ç”¨äºè¿‘ä¼¼è´å¶æ–¯æ¨ç†ã€‚\nè‡ªåŠ¨ç¼–ç å™¨ï¼ˆAEï¼‰[23, 13] å’Œ RBM [14] æ˜¯å…¸å‹çš„ç”Ÿæˆæ–¹æ³•ï¼Œå‡è®¾ OOD å›¾åƒçš„é‡å»ºè¯¯å·®å¤§äºåˆ†å¸ƒå†…å›¾åƒ\næœ€è¿‘ï¼Œå¦ä¸€ç§åŸºäº GAN å†åˆæˆçš„ç”Ÿæˆæ¨¡å‹è¢«è¯æ˜å¯ä»¥åŸºäºå…¶å¯é çš„é«˜åˆ†è¾¨ç‡åƒç´ åˆ°åƒç´ è½¬æ¢ç»“æœå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚  SynthCP [17] å’Œ DUIR [16] æ˜¯åŸºäº GAN å†åˆæˆçš„ä¸¤ç§æ–¹æ³•ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œå®ƒä»¬ç¦»è½»é‡çº§è¿˜å¾ˆè¿œï¼Œå› ä¸ºå¿…é¡»ä¾æ¬¡ä½¿ç”¨ä¸¤ä¸ªæˆ–ä¸‰ä¸ªç¥ç»ç½‘ç»œæ¥è¿›è¡Œ OOD æ£€æµ‹ã€‚\nä¸å®ƒä»¬ç›¸æ¯”ï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºå¯¹æ¯”èšç±»çš„ DMLNet å…·æœ‰æ›´å¥½çš„å¼‚å¸¸åˆ†å‰²æ€§èƒ½ï¼Œè€Œåªéœ€è¦æ¨ç†ä¸€æ¬¡\n2.2 æ·±åº¦åº¦é‡å­¦ä¹ ç½‘ç»œ\nDMLNets å·²ç”¨äºå¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬è§†é¢‘ç†è§£ [24] å’Œäººå‘˜é‡æ–°è¯†åˆ« [25]ã€‚  DMLNet ä½¿ç”¨æ¬§å‡ é‡Œå¾—ã€é©¬æ°è·ç¦»æˆ– Matusita è·ç¦» [26] å°†æ­¤ç±»é—®é¢˜è½¬æ¢ä¸ºè®¡ç®—åº¦é‡ç©ºé—´ä¸­çš„åµŒå…¥ç‰¹å¾ç›¸ä¼¼åº¦ã€‚\nå·ç§¯åŸå‹ç½‘ç»œå’Œ DMLNets é€šå¸¸ä¸€èµ·ç”¨äºè§£å†³ç‰¹å®šé—®é¢˜ï¼Œä¾‹å¦‚æ£€æµ‹å›¾åƒçº§ OOD æ ·æœ¬ [27ã€28ã€29] å’Œç”¨äºè¯­ä¹‰åˆ†å‰²çš„å°æ ·æœ¬å­¦ä¹  [19ã€30ã€31]ã€‚ æˆ‘ä»¬ä¹ŸæŒ‰ç…§è¿™ç§ç»„åˆæ„å»ºäº†ç¬¬ä¸€ä¸ªç”¨äºå¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²çš„ DMLNet\n2.3 å¼€æ”¾ä¸–ç•Œåˆ†ç±»å’Œæ£€æµ‹\nå¼€æ”¾ä¸–ç•Œåˆ†ç±»é¦–å…ˆç”± [32] æå‡ºã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†æœ€è¿‘éå¼‚å¸¸å€¼ (NNO) ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨å¢é‡æ·»åŠ å¯¹è±¡ç±»åˆ«ã€æ£€æµ‹å¼‚å¸¸å€¼å’Œç®¡ç†å¼€æ”¾ç©ºé—´é£é™©æ–¹é¢éå¸¸æœ‰æ•ˆã€‚\næœ€è¿‘çº¦ç‘Ÿå¤«ç­‰äººã€‚  [33]æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”èšç±»ã€æœªçŸ¥æ„ŸçŸ¥æè®®ç½‘ç»œå’ŒåŸºäºèƒ½é‡çš„æœªçŸ¥è¯†åˆ«æ ‡å‡†çš„å¼€æ”¾ä¸–ç•Œå¯¹è±¡æ£€æµ‹ç³»ç»Ÿã€‚ æˆ‘ä»¬çš„å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿçš„ç®¡é“ä¸ä»–ä»¬çš„ç›¸ä¼¼ï¼Œé™¤äº†ä¸¤ä¸ªé‡è¦çš„åŒºåˆ«ä½¿æˆ‘ä»¬çš„ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ï¼šï¼ˆ1ï¼‰åœ¨ä»–ä»¬çš„å¼€æ”¾é›†æ£€æµ‹æ¨¡å—ä¸­ï¼Œä»–ä»¬ä¾èµ–äºåŒºåŸŸæè®®ç½‘ç»œï¼ˆRPNï¼‰æ˜¯ ç±»ä¸å¯çŸ¥ï¼Œå› æ­¤ä¹Ÿå¯ä»¥æ£€æµ‹åˆ°æœªæ ‡è®°çš„æ½œåœ¨ OOD å¯¹è±¡ã€‚ è¿™æ ·ï¼ŒOODæ ·æœ¬çš„ä¿¡æ¯å¯¹äºè®­ç»ƒæ˜¯æœ‰æ•ˆçš„ã€‚ ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè¯­ä¹‰åˆ†å‰²ï¼Œå…¶ä¸­è®­ç»ƒä¸­ä½¿ç”¨çš„æ¯ä¸ªåƒç´ éƒ½è¢«åˆ†é…äº†ä¸€ä¸ªåˆ†å¸ƒå†…æ ‡ç­¾ï¼Œå› æ­¤ä¸èƒ½å°† OOD æ ·æœ¬æ·»åŠ åˆ°è®­ç»ƒä¸­ã€‚  (2) åœ¨å¢é‡å­¦ä¹ æ¨¡å—ä¸­ï¼Œä»–ä»¬ä½¿ç”¨æ–°ç±»çš„æ‰€æœ‰æ ‡è®°æ•°æ®ï¼Œè€Œæˆ‘ä»¬ä¸“æ³¨äºè‡ªç„¶æ›´å›°éš¾çš„å°‘æ ·æœ¬æ¡ä»¶ã€‚ å¾ˆå°‘æœ‰ç ”ç©¶é›†ä¸­åœ¨å¢é‡å°æ ·æœ¬å­¦ä¹ ä¸Šï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨äºåˆ†ç±»çš„å¢é‡å°æ ·æœ¬å­¦ä¹ [34]ã€å¯¹è±¡æ£€æµ‹[35]å’Œè¯­ä¹‰åˆ†å‰²[36]\n3. å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºäº†å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿçš„å·¥ä½œæµç¨‹ã€‚ è¯¥ç³»ç»Ÿç”±ä¸€ä¸ªå¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—å’Œä¸€ä¸ªå¢é‡å°æ ·æœ¬å­¦ä¹ æ¨¡å—ç»„æˆã€‚ å‡è®¾  æ˜¯ N ä¸ªåˆ†å¸ƒå†…çš„ç±»ï¼Œå®ƒä»¬éƒ½åœ¨è®­ç»ƒæ•°æ®é›†ä¸­è¿›è¡Œäº†æ³¨é‡Šï¼Œå¹¶ä¸”  æ˜¯è®­ç»ƒæ•°æ®é›†ä¸­æ²¡æœ‰é‡åˆ°çš„ M ä¸ª OOD ç±»\nå¼€é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—åˆåˆ†ä¸ºä¸¤ä¸ªå­æ¨¡å—ï¼šé—­é›†è¯­ä¹‰åˆ†å‰²å­æ¨¡å—å’Œå¼‚å¸¸åˆ†å‰²å­æ¨¡å—ã€‚\n\n æ˜¯é—­é›†è¯­ä¹‰åˆ†å‰²å­æ¨¡å—çš„è¾“å‡ºå›¾ï¼Œæ‰€ä»¥æ¯ä¸ªåƒç´ çš„ç±»åˆ« ã€‚\nå¼‚å¸¸åˆ†å‰²å­æ¨¡å—çš„åŠŸèƒ½æ˜¯è¯†åˆ«OODåƒç´ ï¼Œå…¶è¾“å‡ºç§°ä¸ºå¼‚å¸¸æ¦‚ç‡å›¾ï¼šï¼Œå…¶ä¸­  å’Œ  è¡¨ç¤ºè¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ã€‚\n\nåŸºäº  å’Œ ï¼Œå¼€é›†è¯­ä¹‰åˆ†å‰²å›¾  ç»™å‡ºä¸º:\n ï¼šè¡¨ç¤º OOD ç±»åˆ«\n ï¼šç¡®å®š OOD åƒç´ çš„é˜ˆå€¼ã€‚\nå› æ­¤ï¼Œopensetè¯­ä¹‰åˆ†å‰²æ¨¡å—åº”è¯¥è¯†åˆ«OODåƒç´ å¹¶åˆ†é…æ­£ç¡®çš„åˆ†å¸ƒæ ‡ç­¾ã€‚ç„¶å Yopen å¯ä»¥è½¬å‘ç»™å¯ä»¥ä»  ä¸­è¯†åˆ«  å¹¶ç»™å‡ºæ–°ç±»çš„ç›¸åº”æ³¨é‡Šçš„æ ‡æ³¨è€…  å¢é‡å°‘æ ·æœ¬å­¦ä¹ æ¨¡å—ç”¨äºåœ¨æœ‰æ–°æ ‡ç­¾æ—¶å°†è¿‘é›†åˆ†å‰²å­æ¨¡å—çš„çŸ¥è¯†åº“ä»  ä¸€ä¸ªä¸€ä¸ªæ›´æ–°ä¸º ï¼Œå…¶ä¸­ ã€‚ å›¾ 1 æ˜¾ç¤ºäº†å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿçš„å¾ªç¯å·¥ä½œæµæ°´çº¿\nå›¾ 1. å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ç³»ç»Ÿã€‚ ç¬¬ 1 æ­¥ï¼šè¯†åˆ«å·²çŸ¥å’ŒæœªçŸ¥å¯¹è±¡ï¼ˆè“è‰²ç®­å¤´ï¼‰ã€‚ ç¬¬ 2 æ­¥ï¼šæ³¨é‡ŠæœªçŸ¥å¯¹è±¡ï¼ˆçº¢è‰²ç®­å¤´ï¼‰ã€‚ ç¬¬ 3 æ­¥ï¼šåº”ç”¨å¢é‡å°‘æ ·æœ¬å­¦ä¹ æ¥å¢åŠ ç½‘ç»œçš„åˆ†ç±»èŒƒå›´ï¼ˆç»¿è‰²ç®­å¤´ï¼‰ã€‚ ç¬¬ 4 æ­¥ï¼šåœ¨å¢é‡å°‘æ ·æœ¬å­¦ä¹ ä¹‹åï¼ŒDMLNet å¯ä»¥åœ¨æ›´å¤§çš„åŸŸä¸­è¾“å‡ºç»“æœï¼ˆç´«è‰²ç®­å¤´ï¼‰ã€‚\n\n4. æ–¹æ³•\næˆ‘ä»¬é‡‡ç”¨ DMLNet ä½œä¸ºæˆ‘ä»¬çš„ç‰¹å¾æå–å™¨ï¼Œå¹¶åœ¨ 4.1 èŠ‚è®¨è®ºæ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚ å¼€æ”¾é›†åˆ†å‰²æ¨¡å—å’Œå¢é‡å°‘æ ·æœ¬å­¦ä¹ æ¨¡å—åœ¨ 4.2 å’Œ 4.3 èŠ‚ä¸­è¿›è¡Œäº†è¯´æ˜\n4.1 æ·±åº¦åº¦é‡å­¦ä¹ ç½‘ç»œ\n\n\n\n\n\n\n\n\n\nClassical CNN-based semantic segmentation networks can be disentangled into two parts:\n\na feature extractor  for obtaining the embedding vector of each pixel\na classifier  for generating the decision boundary,\n\nwhere ,  and  denote the input image, parameters of the feature extractor and classifier respectively.\nThis learnable classifier is not suitable for OOD detection because it assigns all feature space to known classes and leaves no space for OOD classes.\nä¼ ç»ŸCNN-basedè¯­ä¹‰åˆ†å‰²ç½‘ç»œï¼š\n\n ç‰¹å¾æå–å™¨ï¼šè·å–æ¯ä¸ªåƒç´ çš„åµŒå…¥å‘é‡\n åˆ†ç±»å™¨ï¼šç”Ÿæˆå†³ç­–è¾¹ç•Œ\n\nè¿™ç§==å¯å­¦ä¹ çš„åˆ†ç±»å™¨ä¸é€‚ç”¨äº OOD æ£€æµ‹==ï¼Œå› ä¸ºå®ƒå°†æ‰€æœ‰ç‰¹å¾ç©ºé—´åˆ†é…ç»™å·²çŸ¥ç±»ï¼Œå¹¶ä¸”æ²¡æœ‰ä¸º OOD ç±»ç•™ä¸‹ç©ºé—´ã€‚\n\n\n\n\n\n\n\n\n\nIn contrast, the classifier is replaced by the Euclidean distance representation with all prototypes  in DMLNet, where  refers to the prototype of class . The feature extractor  learns to map the input  to the feature vector which has the same length as the prototype in metric space. For the close-set segmentation task, the probability of one pixel  belonging to the class  is formulated as:\nDMLNet ä¸­, ==æ‰€æœ‰åŸå‹çš„æ¬§å‡ é‡Œå¾—è·ç¦»==è¡¨ç¤ºä»£æ›¿äº†ä¼ ç»Ÿçš„å¯å­¦ä¹ åˆ†ç±»å™¨\n\n æŒ‡çš„æ˜¯  ç±»çš„åŸå‹ã€‚\n\nç‰¹å¾æå–å™¨ å­¦ä¹ å°†è¾“å…¥ X æ˜ å°„åˆ°ä¸åº¦é‡ç©ºé—´ä¸­çš„åŸå‹é•¿åº¦ç›¸åŒçš„ç‰¹å¾å‘é‡ã€‚\nå¯¹äºé—­é›†åˆ†å‰²ä»»åŠ¡ï¼Œä¸€ä¸ªåƒç´   å±äºç±»  çš„æ¦‚ç‡å…¬å¼ä¸ºï¼š\nåŸºäºè¿™ç§åŸºäºæ¬§å‡ é‡Œå¾·è·ç¦»çš„æ¦‚ç‡ï¼Œåˆ¤åˆ«äº¤å‰ç†µ (DCE) æŸå¤±å‡½æ•°  [27] å®šä¹‰ä¸º:\nï¼šè¾“å…¥å›¾åƒ  çš„æ ‡ç­¾\n çš„åˆ†å­å’Œåˆ†æ¯åˆ†åˆ«æŒ‡å›¾2ä¸­çš„å¸å¼•åŠ›å’Œæ’æ–¥åŠ›ã€‚\n\n\n\n\n\n\n\n\n\næ’æ–¥åŠ›ä¸éœ€è¦é™¤å»æœ¬èº«æ‰€å±çš„ç±»ï¼Œæœ¬èº«ç±»çš„åŸå‹å—ï¼Ÿ\nå›¾ 2. DMLNet çš„å¯¹æ¯”èšç±»ã€‚ åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå·²çŸ¥å¯¹è±¡å°†è¢«åŒä¸€ç±»çš„åŸå‹æ‰€å¸å¼•ï¼Œè€Œè¢«å‰©ä½™çš„åŸå‹æ‰€æ’æ–¥ã€‚ æœ€åï¼Œå®ƒä»¬å°†å›´ç»•ç‰¹å®šçš„åŸå‹è¿›è¡Œèšåˆã€‚ ç›¸åï¼Œå¼‚å¸¸å¯¹è±¡å°†è¢«æ‰€æœ‰åŸå‹æ’æ–¥ï¼Œå› æ­¤å®ƒä»¬å°†èšé›†åœ¨åº¦é‡ç©ºé—´çš„ä¸­é—´ã€‚\n\næˆ‘ä»¬åˆ¶å®šäº†å¦ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œç§°ä¸ºæ–¹å·®æŸå¤± (VL) å‡½æ•° ï¼Œå…¶å®šä¹‰ä¸ºï¼š\n åªæœ‰å¸å¼•åŠ›ä½œç”¨ï¼Œæ²¡æœ‰æ’æ–¥åŠ›ä½œç”¨ã€‚\nä½¿ç”¨ DCE å’Œ VLï¼Œæ··åˆæŸå¤±å®šä¹‰ä¸ºï¼šï¼Œå…¶ä¸­  æ˜¯æƒé‡å‚æ•°\n4.2 å¼€é›†è¯­ä¹‰åˆ†å‰²æ¨¡å‹\nå¼€é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—ç”±é—­é›†è¯­ä¹‰åˆ†å‰²å­æ¨¡å—å’Œå¼‚å¸¸åˆ†å‰²å­æ¨¡å—ç»„æˆã€‚ å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ¨¡å—çš„æµç¨‹å¦‚ å›¾3 æ‰€ç¤ºã€‚\nå›¾3.é—­é›†åˆ†å‰²å­æ¨¡å—åŒ…å«åœ¨è“è‰²è™šçº¿æ¡†å†…ï¼Œå¼‚å¸¸åˆ†å‰²å­æ¨¡å—åŒ…å«åœ¨çº¢è‰²è™šçº¿æ¡†å†…ã€‚ å¼€é›†åˆ†å‰²å›¾æ˜¯è¿™ä¸¤ä¸ªå­æ¨¡å—ç”Ÿæˆçš„ç»“æœçš„ç»„åˆã€‚ åœ¨å¼€æ”¾é›†åˆ†å‰²å›¾ä¸­é¢„æµ‹åˆ†å¸ƒå†…ç±»å’Œ OOD ç±»ã€‚  EDS map å’Œ MMSP map çš„å®šä¹‰è¯·å‚è€ƒ 4.2 èŠ‚ã€‚\n\n\n\né—­é›†è¯­ä¹‰åˆ†å‰²å­æ¨¡å—ä¸ºä¸€å¹…å›¾åƒçš„æ‰€æœ‰åƒç´ åˆ†é…åˆ†å¸ƒæ ‡ç­¾ã€‚ ç”±äºä¸€ä¸ªåƒç´   å±äºç±»  çš„æ¦‚ç‡æ˜¯ç”¨å…¬å¼ 2 è¡¨ç¤ºï¼Œé—­é›†åˆ†å‰²å›¾ä¸ºï¼š\n\n\nå¼‚å¸¸åˆ†å‰²å­æ¨¡å—æ£€æµ‹OODåƒç´ ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªæœªçŸ¥çš„è¯†åˆ«æ ‡å‡†æ¥æµ‹é‡å¼‚å¸¸æ¦‚ç‡ï¼ŒåŒ…æ‹¬_åŸºäºåº¦é‡çš„æœ€å¤§softmaxæ¦‚ç‡ï¼ˆMMSPï¼‰å’Œ_æ¬§å‡ é‡Œå¾—è·ç¦»å’Œï¼ˆEDSï¼‰ã€‚\n\n\nä»¥ä¸‹æ˜¯åŸºäº MMSP çš„å¼‚å¸¸æ¦‚ç‡ï¼š\n\n\nEDS æ˜¯æ ¹æ®ä»¥ä¸‹å‘ç°æå‡ºçš„ï¼šå¦‚æœç‰¹å¾ä½äº OOD åƒç´ èšé›†çš„åº¦é‡ç©ºé—´çš„ä¸­å¿ƒï¼Œåˆ™ä¸æ‰€æœ‰åŸå‹çš„æ¬§å‡ é‡Œå¾—è·ç¦»å’Œæ›´å°ï¼Œå³==å¼‚å¸¸çš„æ¬§å‡ é‡Œå¾—è·ç¦»è¾ƒå°==ã€‚  EDS å®šä¹‰ä¸ºï¼š\nåŸºäº EDS çš„å¼‚å¸¸æ¦‚ç‡è®¡ç®—å¦‚ä¸‹ï¼š\n\n\n\n\nEDS æ˜¯ç±»ç‹¬ç«‹çš„ï¼Œå› æ­¤æ‰€æœ‰ç±»çš„åŸå‹åº”è¯¥å‡åŒ€åˆ†å¸ƒåœ¨åº¦é‡ç©ºé—´ä¸­ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæœŸé—´ä¸ç§»åŠ¨ã€‚ ==å¯å­¦ä¹ çš„åŸå‹ä¼šåœ¨è®­ç»ƒæœŸé—´å¯¼è‡´ä¸ç¨³å®šï¼Œå¹¶ä¸”å¯¹æ›´å¥½çš„æ€§èƒ½æ²¡æœ‰è´¡çŒ®== [37]ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬ä»¥ one-hot å‘é‡å½¢å¼å®šä¹‰åŸå‹ï¼šåªæœ‰  çš„ç¬¬ t ä¸ªå…ƒç´ æ˜¯ ï¼Œè€Œå…¶ä»–å…ƒç´ ä¿æŒä¸ºé›¶ï¼Œå…¶ä¸­ t âˆˆ {1,2,â€¦,N}\n\n\n\n\n\n\n\n\n\nPAnSæ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿ\nEDS æ˜¯ç›¸å¯¹äºæ‰€æœ‰åƒç´ ä¹‹é—´çš„æœ€å¤§è·ç¦»å’Œçš„æ¯”ç‡ï¼Œå³ä½¿å›¾åƒä¸­æ²¡æœ‰ OOD å¯¹è±¡ï¼Œé«˜å¼‚å¸¸åˆ†æ•°åŒºåŸŸè‚¯å®šå­˜åœ¨äºæ¯å¹…å›¾åƒä¸­ã€‚ æ­¤å¤–ï¼Œæ¯ä¸ªåˆ†å¸ƒå†…ç±»åˆ«çš„è·ç¦»æ€»å’Œåˆ†å¸ƒå½¼æ­¤ç•¥æœ‰ä¸åŒï¼Œå¦‚å›¾4æ‰€ç¤ºã€‚\n\nå°†MMSPä¸EDSç›¸ç»“åˆï¼Œä»¥æŠ‘åˆ¶é‚£äº›å®é™…ä¸Šå¤„äºåˆ†å¸ƒçŠ¶æ€çš„å…·æœ‰ä¸­é—´å“åº”çš„åƒç´ ã€‚\næ··åˆå‡½æ•°ä¸ºï¼š\n\n\nÎ± ï¼š\n\nÎ² å’Œ Î³ æ˜¯æ§åˆ¶æŠ‘åˆ¶æ•ˆæœå’Œé˜ˆå€¼çš„è¶…å‚æ•°ã€‚\n\n\n\né€šè¿‡æ–¹ç¨‹ 9 å¾—åˆ°å¼‚å¸¸æ¦‚ç‡å›¾å’Œæ–¹ç¨‹ 5 å¾—åˆ°é—­é›†åˆ†å‰²å›¾åï¼Œæˆ‘ä»¬åº”ç”¨æ–¹ç¨‹ 1 ç”Ÿæˆæœ€ç»ˆçš„å¼€é›†åˆ†å‰²å›¾\n5. å®éªŒ\nOur experiments are divided into three parts.\n\nWe first evaluate our open-set semantic segmentation approach in Section 5.1.\nThen we demonstrate our incremental few-shot learning results in Section 5.2.\nBased on the open-set semantic segmentation module and incremental few-show learning module, the whole open world semantic segmentation is realized in Section 5.3.\n\n5.1 å¼€é›†è¯­ä¹‰åˆ†å‰²\næ•°æ®é›†ã€‚ ä¸‰ä¸ªæ•°æ®é›†åŒ…æ‹¬ StreetHazards [11]ã€Lost and Found [38] å’Œ Road Anomaly [16] ç”¨äºè¯æ˜æˆ‘ä»¬åŸºäº DMLNet çš„å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚\n\nStreetHazards çš„å¤§å¤šæ•°å¼‚å¸¸ç‰©ä½“æ˜¯å¤§å‹ç¨€æœ‰è¿è¾“æœºå™¨ï¼Œä¾‹å¦‚ç›´å‡æœºã€é£æœºå’Œæ‹–æ‹‰æœºã€‚\nLost and Found å«è®¸å¤šå°çš„å¼‚å¸¸ç‰©å“ï¼Œå¦‚è´§ç‰©ã€ç©å…·å’Œç›’å­ã€‚\nRoad Anomaly æ•°æ®é›†ä¸å†é™åˆ¶åŸå¸‚åœºæ™¯ä¸­çš„åœºæ™¯ï¼Œè¿˜åŒ…å«æ‘åº„å’Œå±±è„‰çš„å›¾åƒã€‚\n\næŒ‡æ ‡ã€‚ å¼€æ”¾é›†è¯­ä¹‰åˆ†å‰²æ˜¯å°é—­é›†åˆ†å‰²å’Œå¼‚å¸¸åˆ†å‰²çš„ç»„åˆï¼Œå¦‚ 4.2 èŠ‚æ‰€è¿°ã€‚\n\nå¯¹äºé—­é›†è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ mIoU æ¥è¯„ä¼°æ€§èƒ½ã€‚\nå¯¹äºå¼‚å¸¸åˆ†å‰²ä»»åŠ¡ï¼Œæ ¹æ® [11] ä½¿ç”¨ä¸‰ä¸ªæŒ‡æ ‡ï¼ŒåŒ…æ‹¬ ROC æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ã€95% å¬å›çš„è¯¯æŠ¥ç‡ï¼ˆFPR95ï¼‰å’Œç²¾ç¡®å¬å›æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUPRï¼‰ã€‚\n\nå®æ–½ç»†èŠ‚ã€‚\n\n\nå¯¹äº StreetHazardsï¼Œæˆ‘ä»¬éµå¾ªä¸ [11] ç›¸åŒçš„è®­ç»ƒç¨‹åºï¼Œåœ¨ StreetHazards çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒ PSPNet [2]ã€‚\n\n\n\n\n\n\n\n\n\n[11]: Scaling out-of-distribution detection for real-world settings.\n\n\nå¯¹äºLost and Foundå’Œ Road Anomalyï¼Œæˆ‘ä»¬æŒ‰ç…§ [16] ä½¿ç”¨ BDD-100k [39] æ¥è®­ç»ƒ PSPNetã€‚ è¯·æ³¨æ„ï¼ŒPSPNet ä»…ç”¨äºæå–æˆ‘ä»¬åœ¨ 4.1 èŠ‚ä¸­è®¨è®ºçš„ç‰¹å¾ï¼ˆè·å¾—æ¯ä¸ªåƒç´ çš„åµŒå…¥å‘é‡ï¼‰ã€‚ æ··åˆæŸå¤±çš„  ä¸º 0.01ã€‚   æ‰€æœ‰åŸå‹ä¸­éé›¶å…ƒç´   ä¸º 3ã€‚ç­‰å¼ 10 ä¸­çš„ Î² å’Œ Î³ åˆ†åˆ«ä¸º 20 å’Œ 0.8ã€‚\n\n\n\n\n\n\n\n\n\n[16]: Detecting the unexpected via image resynthesis\n\n\nåŸºçº¿ã€‚\n\nStreetHazards:  MSP [9]ã€Dropout [10]ã€AE [13]ã€MaxLogit [11] å’Œ SynthCP [17]ã€‚\nLost and Found å’Œ Road Anomaly:  MSPã€MaxLogitã€Ensemble [12]ã€RBM [14] å’Œ DUIR [16]ã€‚\n\nç»“æœã€‚\nStreetHazards çš„ç»“æœå¦‚è¡¨ 1 æ‰€ç¤ºã€‚\n\nå¯¹äº Lost and Found å’Œ Road Anomalyï¼ŒmIoU æ˜¯æ— æ•ˆçš„ï¼Œå› ä¸ºå®ƒä»¬åªæä¾› OOD ç±»æ ‡ç­¾ï¼Œä½†æ²¡æœ‰ç‰¹å®šçš„åˆ†å¸ƒå†…ç±»æ ‡ç­¾ã€‚ ç»“æœåœ¨è¡¨ 2 ä¸­ã€‚\n\næˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼š\n\nåŸºäº DMLNet çš„æ–¹æ³•åœ¨æ‰€æœ‰ä¸‰ä¸ªå¼‚å¸¸åˆ†å‰²ç›¸å…³æŒ‡æ ‡ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚\nä¸æœ€è¿‘æå‡ºçš„åŸºäº GAN çš„æ–¹æ³•ï¼ˆåŒ…æ‹¬ DUIR å’Œ SynthCPï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼‚å¸¸åˆ†å‰²è´¨é‡æ–¹é¢ä¼˜äºå®ƒä»¬ï¼Œç»“æ„æ›´è½»é‡çº§ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ•´ä¸ªæµç¨‹ä¸­éœ€è¦ä¸¤ä¸ªæˆ–ä¸‰ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼Œè€Œæˆ‘ä»¬åªéœ€è¦æ¨ç†ä¸€æ¬¡ã€‚\nStreetHazards ä¸­é—­é›†åˆ†å‰²çš„ mIoU å€¼è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯¹é—­é›†åˆ†å‰²æ²¡æœ‰å±å®³ã€‚\n\nä¸€äº›å®šæ€§ç»“æœå¦‚å›¾ 8 æ‰€ç¤º\n\næ¶ˆèç ”ç©¶ã€‚ æˆ‘ä»¬ä»”ç»†è¿›è¡Œäº†æ¶ˆèå®éªŒï¼Œç ”ç©¶äº†ä¸åŒæŸå¤±å‡½æ•°ï¼ˆVL å’Œ DCEï¼‰å’Œå¼‚å¸¸åˆ¤æ–­æ ‡å‡†ï¼ˆEDS å’Œ MMSPï¼‰çš„å½±å“ï¼Œå¦‚è¡¨ 3 æ‰€ç¤ºã€‚\n\n\nDCE åœ¨ mIoU ä¸Šçš„æ€§èƒ½ä¼˜äº VL çš„äº‹å®è¡¨æ˜äº† æ’æ–¥åŠ›ã€‚\nEDS åœ¨æ‰€æœ‰æŸå¤±ä¸‹éƒ½ä¼˜äº MMSP å‡½æ•°ï¼Œè¿™æ„å‘³ç€==ä¸ç±»æ— å…³çš„æ ‡å‡†æ›´é€‚åˆäºå¼‚å¸¸åˆ†å‰²ä»»åŠ¡==\n\n","slug":"DMLNet","date":"2022-05-03T12:07:00.000Z","categories_index":"","tags_index":"æ·±åº¦å­¦ä¹ ,å¼‚å¸¸åˆ†å‰²,åº¦é‡å­¦ä¹ ","author_index":"Star"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\n\nQuick Start\nCreate a new post\n$ hexo new \"My New Post\"\nMore info: Writing\nRun server\n$ hexo server\nMore info: Server\nGenerate static files\n$ hexo generate\nMore info: Generating\nDeploy to remote sites\n$ hexo deploy\nMore info: Deployment\n","slug":"hello-world","date":"2022-05-03T10:31:15.256Z","categories_index":"","tags_index":"","author_index":"Star"}]