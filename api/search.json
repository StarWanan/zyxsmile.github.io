[{"id":"283a70296507847edcff51a8a9a4f224","title":"SML","content":"Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation标准max logit：在城市场景分割中识别意外道路障碍的一种简单但有效的方法\n论文翻译AbstractIdentifying unexpected objects on roads in semantic seg- mentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or re- quire additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying un- expected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes* the max logits in order to align the different distributions and reflect the relative meanings of max log- its within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic in- formation. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to ex-isting pre-trained segmentation models. Such a straightfor- ward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost &amp; Found leader- board with a large margin. Our code is publicly available at this link 1.\n在语义分割中识别道路上的意外物体（例如，识别道路上的狗）在安全关键型应用中至关重要。 \n现有方法：\n\n使用来自外部数据集的意外对象的图像或需要额外的训练（重新训练分割网络或训练额外的网络）\n缺点：需要大量的劳动强度或冗长的推理时间\n\n预训练网络的预测分数（例如maxlogit，最终 softmax 层之前的类之间的最大值）\n缺点：每个预测类别的最大 logits 分布彼此之间存在显着差异，这降低了在城市场景分割中识别非预期对象的性能\n\n\n改进：\n\n标准化 max logits：对齐不同的分布并反映每个预测类别中 max logits 的相对含义。 \n基于相邻像素共享相似语义信息的直觉，从两个不同的角度考虑局部区域。 \n\n标准maxlogit优势：不使用任何外部数据集或需要额外的训练，广泛适用于现有的预训练分割模型。 \n结果：在公开可用的 Fishyscapes Lost &amp; Found 排行榜上实现了新的最先进的性能，并有很大的优势。 \n1. IntroductionRecent studies [8, 9, 22, 40, 43, 44, 12] in semantic segmentation focus on improving the segmentation per- formance on urban-scene images. Despite such recent ad- vances, these approaches cannot identify unexpected ob- jects (i.e., objects not included in the pre-defined classes during training), mainly because they predict all the pixels as one of the pre-defined classes. Addressing such an is- sue is critical especially for safety-critical applications such as autonomous driving. As shown in Fig. 1, wrongly pre- dicting a dog (i.e., an unexpected object) on the road as the road does not stop the autonomous vehicle, which may lead to roadkill. In this safety-critical point of view, the dog should be detected as an unexpected object which works as the starting point of the autonomous vehicle to handle these objects differently (e.g., whether to stop the car or circumvent the dog).\n近期提升城市场景图像语义分割性能的研究：8, 9,22, 40, 43, 44, 12\n\n8. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. &#8617;\n\n\n9. Cars can’t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks. &#8617;\n\n\n22. Foveanet: Perspective-aware urban scene parsing. &#8617;\n\n\n40. Denseaspp for semantic segmentation in street scenes. &#8617;\n\n\n43. Improving semantic segmentation via video propagation and label relaxation &#8617;\n\n\n44. Asymmetric non-local neural networks for semantic segmentation &#8617;\n\n\n12. Dual attention network for scene segmentation &#8617;\n\n这些方法不足之处：无法识别意外对象（即训练期间未包含在预定义类中的对象）\n原因：将所有像素预测为预定义类之一\n解决此类问题至关重要，尤其是对于自动驾驶等安全关键型应用。 如图 1 所示，错误地将道路上的狗（即意外物体）预测为道路不会停止自动驾驶汽车，这可能会导致道路死亡。 从这个安全关键的角度来看，狗应该被检测为一个意外的物体，它作为自动驾驶汽车的起点，以不同的方式处理这些物体（例如，是停下汽车还是绕过狗）。\n\n图1：我们识别道路上意外障碍物的方法的结果。(a) 先前的分割网络将意外的障碍物（例如狗）分类为预定义的类别（例如道路）之一，这从安全关键的角度来看可能是有害的。(b) 通过我们的方法，我们检测到了意外的障碍。(c) 最后，我们可以获得识别出意外障碍物（青色物体）的分割标签的最终预测。\nSeveral studies [3, 26, 25, 5, 35, 2, 14] tackle the problem of detecting such unexpected objects on roads. Some approaches [2, 5] utilize external datasets [36, 24] as samples of unexpected objects while others [26, 39, 25, 33] leverage image resynthesis models for erasing the regions of such objects. However, such approaches require a considerable amount of labor intensity or necessitate a lengthy inference time. On the other hand, simple approaches which leverage only a pre-trained model [18, 23, 20] are proposed for out-of-distribution (OoD) detection in image classification, the task of detecting images from a different distribution compared to that of the train set. Based on the intuition that a correctly classified image generally has a higher maximum softmax probability (MSP) than an OoD image [18], MSP is used as the anomaly score (i.e., the value used for detecting OoD samples). Alternatively, utilizing the max logit [16] (i.e., maximum values among classes before the final soft- max layer) as the anomaly score is proposed, which out- performs using MSP for detecting anomalous objects in se- mantic segmentation. Note that high prediction scores (e.g., MSP and max logit) indicate low anomaly scores and vice versa.\n在道路上检测此类意外物体的问题的几项研究 [3, 26, 25, 5, 35, 2, 14] 。 \n\n[2, 5] 利用外部数据集 [36, 24] 作为意外对象的样本，\n\n[26, 39, 25, 33] 利用图像再合成模型来擦除这些对象的区域。 然而，这种方法需要大量的劳动强度或需要很长的推理时间。 \n\n仅利用预训练模型 [18,23,20] 的简单方法用于图像分类中的分布外 (OoD) 检测，检测与训练集相比不同分布的图像。 \n\n基于正确分类的图像通常具有比 OoD 图像 [18] 更高的最大 softmax 概率 (MSP) 的直觉，MSP 被用作异常分数（用于检测 OoD 样本的值)\n\n提出了利用最大 logit [16]（最终 softmax 层之前的类之间的最大值）作为异常分数，其在语义分割中使用 MSP 检测异常对象的性能优于使用 MSP 检测异常对象。 \n\n请注意，高预测分数（例如， MSP 和最大 logit）表示低异常分数，反之亦然。\nHowever, directly using the MSP [18] or the max logit [16] as the anomaly score has the following limita- tions. Regarding the MSP [18], the softmax function has the fast-growing exponential property which produces highly confident predictions. Pre-trained networks may be highly confident with OoD samples which limits the performance of using MSPs for detecting the anomalous samples [23]. In the case of the max logit [16], as shown in Fig. 2, the val- ues of the max logit have their own ranges in each predicted class. Due to this fact, the max logits of the unexpected ob- jects predicted as particular classes (e.g., road) exceed those of other classes (e.g., train) in the in-distribution objects. This can degrade the performance of detecting unexpected objects on evaluation metrics (e.g., AUROC and AUPRC) that use the same threshold for all classes.\n直接使用 MSP [18] 或最大 logit [16] 作为异常分数的限制：\n\nMSP [18]：softmax 函数具有快速增长的指数特性，可以产生高度可信的预测。 预训练的网络可能对 OoD 样本高度自信，这限制了使用 MSP 检测异常样本的性能 [23]。 \n\n最大 logit [16] ：如图 2 所示，最大 logit 的值在每个预测类别中都有自己的范围。 由于这个事实，预测为特定类（例如，道路）的意外对象的最大 logits 超过了分布对象中其他类（例如，火车）的最大 logits。 这会降低在对所有类使用相同阈值的评估指标（例如 AUROC 和 AUPRC）上检测意外对象的性能。\n\n\n\n图 2：Fishyscapes Static 中的 MSP、最大 logit 和标准化最大 logit 的箱线图。  X 轴表示在训练阶段按像素出现次数排序的类。  Y 轴表示每种方法的值。 红色和蓝色分别代表分布内像素和意外像素中的值分布。 每个条的下限和上限表示 Q1 和 Q3，而点表示其预测类别的平均值。 灰色表示两组的重叠区域。 灰色区域的不透明度与 TPR 95% 时的 FPR 成正比。 以分类方式标准化最大 logits 明显降低了 FPR。\nIn this work, inspired by this finding, we propose standardizing the max logits in a class-wise manner, termed standardized max logits (SML). Standardizing the max log- its aligns the distributions of max logits in each predicted class, so it enables to reflect the relative meanings of val- ues within a class. This reduces the false positives (i.e., in-distribution objects detected as the unexpected objects, highlighted as gray regions in Fig. 2) when using a single threshold.\n在这项工作中，受这一发现的启发，我们建议以分类方式对最大 logits 进行标准化，称为 standardized max logits (SML)。 标准化最大对数会对齐每个预测类中最大对数的分布，因此它能够反映类内值的相对含义。 当使用单个阈值时，这减少了误报（被检测为意外对象的分布中对象，在图 2 中突出显示为灰色区域）。\nMoreover, we further improve the performance of identifying unexpected obstacles using the local semantics from two different perspectives. First, we remove the false positives in boundary regions where predicted class changes from one to another. Due to the class changes, the boundary pixels tend to have low prediction scores (i.e., high anomaly scores) compared to the non-boundary pixels [38, 1]. In this regard, we propose a novel iterative boundary sup- pression to remove such false positives by replacing the high anomaly scores of boundary regions with low anomaly scores of neighboring non-boundary pixels. Second, in order to remove the remaining false positives in both bound- ary and non-boundary regions, we smooth them using the neighboring pixels based on the intuition that local consistency exists among the pixels in a local region. We term this process as dilated smoothing.\n此外，我们从两个不同的角度进一步提高了使用局部语义识别意外障碍物的性能。\n\n在预测的类从一个到另一个变化的边界区域中删除了误报。 由于类别变化，与非边界像素相比，边界像素往往具有较低的预测分数（即高异常分数）[38, 1]。 在这方面，我们提出了一种新的迭代边界抑制，通过用相邻非边界像素的低异常分数替换边界区域的高异常分数来消除这种误报。 \n\n\n\n\n\n\n\n\n\n怎么确定边界的？\n\n为了消除边界和非边界区域中剩余的误报，根据局部区域中像素之间存在局部一致性的直觉，使用相邻像素对它们进行平滑处理。 将此过程称为扩张平滑。\n\n\n\n\n\n\n\n\n\n\n\n相邻像素，好像非极大抑制啊\nThe main contributions of our work are as follows: \n\nWe propose a simple yet effective approach for identi- fying unexpected objects on roads in urban-scene se- mantic segmentation.\nOur proposed approach can easily be applied to vari- ous existing models since our method does not require additional training or external datasets.\nWe achieve a new state-of-the-art performance on the publicly available Fishyscapes Lost &amp; Found Leader- board2 among the previous approaches with a large margin and negligible computation overhead while not requiring additional training and OoD data.\n\n主要贡献：\n\n提出了一种简单而有效的方法，用于在城市场景语义分割中识别道路上的意外物体。\n本方法可以很容易地应用于各种现有模型，因为不需要额外的训练或外部数据集。\n在公开可用的 Fishyscapes Lost &amp; Found Leaderboard2 上实现了新的最先进的性能（以前的方法中具有很大的余量和可忽略的计算开销）同时不需要额外的训练和 OoD 数据。\n\n2. Related Work2.1. Semantic segmentation on urban driving scenesRecent studies [8, 9, 22, 40, 43, 44, 12, 6, 34, 32] have strived to enhance the semantic segmentation performance on urban scenes. The studies [22, 40] consider diverse scale changes in urban scenes or leverage the innate geometry and positional patterns found in urban-scene images [9]. Moreover, several studies [6, 34, 32] have proposed more efficient architectures to improve the inference time, which is critical for autonomous driving. Despite the advances, unex- pected objects cannot be identified by these models, which is another important task for safetycritical applications. Regarding the importance of such a task from the safetycritical perspective, we focus on detecting unexpected ob- stacles in urban-scene segmentation.\n最近的研究 [8, 9, 22, 40, 43, 44, 12, 6, 34, 32] 致力于提高城市场景的语义分割性能。 研究 [22, 40] 考虑了城市场景中的不同尺度变化或利用城市场景图像中发现的先天几何和位置模式 [9]。 此外，一些研究 [6, 34, 32] 提出了更有效的架构来改善推理时间，这对于自动驾驶至关重要。 尽管取得了进步，但这些模型无法识别意外对象，这是安全关键应用程序的另一项重要任务。 从安全关键的角度来看，这项任务的重要性，我们专注于检测城市场景分割中的意外障碍。\n2.2. Detecting unexpected objects in semantic segmentation 在语义分割中检测意外对象Several studies [2, 5, 3] utilize samples of unexpected objects from external datasets during the training phase. For example, by assuming that the objects cropped from the Im- ageNet dataset [36] are anomalous objects, they are overlaid on original training images [2] (e.g., Cityscapes) to provide samples of unexpected objects. Similarly, another previous work [5] utilizes the objects from the COCO dataset [24] as samples of unexpected objects. However, such meth- ods require retraining the network by using the additional datasets, which hampers to utilize a given pre-trained seg- mentation network directly.\nOther work [26, 39, 25, 33] exploits the image resynthesis (i.e., reconstructing images from segmentation predic- tions) for detecting unexpected objects. Based on the intu- ition that image resynthesis models fail to reconstruct the regions with unexpected objects, these studies use the dis- crepancy between an original image and the resynthesized image with such objects excluded. However, utilizing an ex- tra image resynthesis model to detect unexpected objects necessitates a lengthy inference time that is critical in semantic segmentation. In the real-world application of se- mantic segmentation (e.g., autonomous driving), detecting unexpected objects should be finalized in real-time. Consid- ering such issues, we propose a simple yet effective method that can be applied to a given segmentation model without requiring additional training or external datasets.\n一些研究[2,5,3]在训练阶段利用来自外部数据集的意外对象样本。 例如，假设从 ImageNet 数据集 [36] 中裁剪的对象是异常对象，它们将覆盖在原始训练图像 [2]（例如 Cityscapes）上以提供意外对象的样本。 同样，之前的另一项工作 [5] 利用 COCO 数据集 [24] 中的对象作为意外对象的样本。 然而，这种方法需要通过使用额外的数据集来重新训练网络，这阻碍了直接利用给定的预训练分割网络。\n其他工作 [26, 39, 25, 33] 利用图像重新合成（即从分割预测重建图像）来检测意外对象。 基于图像再合成模型无法重建具有意外物体的区域的直觉，这些研究使用了原始图像与排除这些物体的再合成图像之间的差异。 然而，利用额外的图像再合成模型来检测意外对象需要很长的推理时间，这在语义分割中至关重要。 在语义分割的实际应用中（例如，自动驾驶），应实时完成对意外对象的检测。 考虑到这些问题，我们提出了一种简单而有效的方法，可以应用于给定的分割模型，而不需要额外的训练或外部数据集。\n3. Proposed MethodThis section presents our approach for detecting unex- pected road obstacles. We first present how we standardize the max logits in Section 3.2 and explain how we consider the local semantics in Section 3.3.\n本节介绍了我们检测意外道路障碍物的方法。 我们首先在 3.2 节介绍我们如何标准化最大 logits，并在 3.3 节解释我们如何考虑局部语义。\n3.1. Method OverviewAs our method overview is illustrated in Fig. 3, we first obtain the max logits and standardize them, based on the finding that the max logits have their own ranges according to the predicted classes. These different ranges cause unex- pected objects (pixels in blue boxes) predicted as a certain class to have higher max logit values (i.e., lower anomaly scores) than in-distribution pixels in other classes. This is- sue is addressed by standardizing the max logits in a class- wise manner since it enables to reflect the relative meanings within each predicted class.\n由于我们的方法概述如图 3 所示，我们首先获得最大 logits 并标准化它们，基于最大 logits 根据预测类别有自己的范围的发现。 这些不同的范围导致预测为某个类别的意外对象（蓝色框中的像素）比其他类别中的分布内像素具有更高的最大 logit 值（较低的异常分数）。 这个问题是通过以类别方式标准化最大 logits 来解决的，因为它能够反映每个预测类别中的相对含义。\n图 3：我们的方法概述。 我们从分割网络中获得最大 logits，并 (a) 使用从训练样本中获得的统计数据对其进行标准化。  (b) 然后，我们用周围的非边界像素迭代地替换边界区域的标准化最大 logits。  (c) 最后，我们应用扩张平滑来考虑广泛接受域中的局部语义。\nThen, we remove the false positives (pixels in green boxes) in boundary regions. Generally, false positives in boundary pixels have lower prediction scores than neigh- boring in-distribution pixels. We reduce such false posi- tives by iteratively updating boundary pixels using anomaly scores of neighboring non-boundary pixels. Additionally, there exist a non-trivial number of pixels that have signif- icantly different anomaly scores compared to their neigh- boring pixels, which we term as irregulars (pixels in yellow boxes). Based on the intuition that local consistency (i.e., neighboring pixels sharing similar semantics) exists among pixels in a local region, we apply the smoothing filter with broad receptive fields. Note that we use the negative value of the final SML as the anomaly score.\n然后，我们删除边界区域中的误报（绿色框中的像素） 通常，边界像素中的误报比相邻的分布内像素具有更低的预测分数。 我们通过使用相邻非边界像素的异常分数迭代更新边界像素来减少此类误报。 此外，存在大量像素与其相邻像素相比具有显着不同的异常分数，我们将其称为不规则像素（黄色框中的像素）。 基于局部一致性（相邻像素共享相似语义）存在于局部区域中的像素之间的直觉，我们应用具有广泛感受野的平滑滤波器。 请注意，我们使用最终 SML 的负值作为异常分数。\nThe following describes the process of how we obtain the max logit and the prediction at each pixel with a given image and the number of pre-defined classes. Let $X \\in \\R^{3\\times H\\times W}$ and $C$ denote the input image and the number of pre-defined classes, where H and W are the image height, and width, respectively. The logit output $F \\in \\R^{3\\times H \\times W}$ can be obtained from the segmentation network before the softmaxlayer.Then,themaxlogit $L \\in \\R^{H×W}$ andprediction $\\hat{Y} \\in \\R^{H×W}$ at each location h, w are defined as\n获得最大 logit 以及使用给定图像和预定义类的数量在每个像素处进行预测的过程: \n$X \\in \\R^{3\\times H\\times W}$  输入图像$C$ 预定义类的数量$H$ 和 $W$ 分别是图像的高度和宽度。  \nlogit 输出 $F \\in \\R^{3\\times H \\times W}$ 可以在 softmax 层之前从分割网络中获得。\n在每个位置 h，w 处的 maxlogit $L \\in \\R^{H×W}$ 和预测 $\\hat{Y} \\in \\R^{H×W}$ 定义为\n\nL_{h,w} = max_c \\ F_{c,h,w} \\tag{1}\n\\hat{Y}_{h,w} = arg max_c \\ F_{c,h,w} \\tag{2}$c \\in \\{1,…,C\\}$\n3.2 Standardized Max Logits (SML)As described in Fig. 2, standardizing the max logits aligns the distributions of max logits in a class-wise manner. For the standardization, we obtain the mean $\\mu_c$ and variance $\\sigma_c^2$ of class $c$ from the training samples. With the max logit $L_{h,w}$ and the predicted class $\\hat{Y}_{h,w}$  from the Eqs. (1) and (2), we compute the mean $\\mu_c$ and variance $\\sigma_c^2$ by\n如图 2 所述，标准化最大 logits 以按类别方式对齐最大 logits 的分布。 对于标准化，我们从训练样本中获得类 $c$ 的均值 $\\mu_c$ 和方差 $\\sigma_c^2$。 使用最大 logit $L_{h,w}$ 和预测类别 $\\hat{Y}_{h,w}$，我们计算均值 $\\mu_c$ 和方差 $\\sigma_c^2$\n\n\\mu_c = \\dfrac{\\sum_i\\sum_{h,w}\\mathbb{1}(\\hat{y}^{(i)}_{h,w}=c)\\cdot L_{h,w}^{(i)}}{\\sum_i\\sum_{h,w}\\mathbb{1}(\\hat{y}^{(i)}_{h,w}=c)} \\tag{3}\n\\sigma_c^2 = \\dfrac{\\sum_i\\sum_{h,w}\\mathbb{1}(\\hat{y}^{(i)}_{h,w}=c)\\cdot (L_{h,w}^{(i)} - \\mu_c)^2}{\\sum_i\\sum_{h,w}\\mathbb{1}(\\hat{y}^{(i)}_{h,w}=c)} \\tag{4}where $i$ indicates the $i_{th}$ training sample and $\\mathbb{1}(·)$ represents the indicator function.\n其中$i$ 表示$i_{th}$个训练样本，$\\mathbb{1}(·)$ 表示指示函数。\n\n\n\n\n\n\n\n\n\n指示函数：指示函数是定义在某集合X上的函数，表示其中有哪些元素属于某一子集A\nNext, we standardize the max logits by the obtained statistics. The SML $S \\in \\R^{H×W}$ in a test image at each location $h, w$ is defined as\n接下来，我们通过获得的统计数据对最大 logits 进行标准化。 测试图像中每个位置 $h, w$ 的 SML $S \\in \\R^{H×W}$ 定义为 \n\nS_{h,w} = \\dfrac{L_{h,w} - \\mu\\hat{Y}_{h,w}}{\\sigma\\hat{Y}_{h,w}} \\tag{5}3.3 Enhancing with Local Semantics 使用局部语义增强We explain how we apply iterative boundary suppression and dilated smoothing by utilizing the local semantics.\n我们解释了如何通过利用局部语义来应用迭代边界抑制和扩张平滑。 \n3.3.1 Iterative boundary suppression 迭代边界抑制To address the problem of wrongly predicting the boundary regions as false positives and false negatives, we iteratively suppress the boundary regions. Fig. 4 illustrates the process of iterative boundary suppression. We gradually propagate the SMLs of the neighboring non-boundary pixels to the boundary regions, starting from the outer areas of the boundary (green-colored pixels) to inner areas (gray-colored pixels). To be specific, we assume the boundary width as a particular value and update the boundaries by iteratively reducing the boundary width at each iteration. This process is defined as follows. With a given boundary width ri at the i-th iteration and the semantic segmentation output $\\hat{Y}$ , we obtain the non-boundary mask $M^{(i)}_{h,w} \\in \\R^{H\\times W}$ each pixel $h, w$ as\n图 4：迭代边界抑制的工作原理。 在标准化最大 logits 之后，我们通过仅使用非边界像素的 SML（即边界感知平均池)进行多次迭代来应用平均池。 边界掩码是从分割网络的预测输出中获得的。\n为了解决错误地将边界区域预测为假阳性和假阴性的问题，我们迭代地抑制边界区域。 图 4 说明了迭代边界抑制的过程。 我们逐渐将相邻非边界像素的 SML 传播到边界区域，从边界的外部区域（绿色像素）到内部区域（灰色像素）。 具体来说，我们假设边界宽度为一个特定值，并通过在每次迭代中迭代地减小边界宽度来更新边界。 该过程定义如下。 在第 $i$ 次迭代给定边界宽度 $r_i$ 和语义分割输出 $\\hat{Y}$ ，我们得到在每个位置为 $h, w$ 像素的非边界掩码 $M^{(i)}_{h,w} \\in \\R  ^{H\\times W}$ ：\n\nM^{(i)}_{h,w} = \n\\begin{cases}\n0, \\qquad if \\ \\  \\exists h',w' \\quad s.t. \\hat{Y}_{h,w} \\ne \\hat{Y}_{h',w'} \\\\\n1, \\qquad otherwise \n\\end{cases}  \\tag{6}for $\\forall h’,w’$ that satisfies $|h−h’|+|w−w’| \\le r_i$\nNext, we apply the boundary-aware average pooling on the boundary pixels as shown in Fig. 4. This applies average pooling on a boundary pixel only with the SMLs of neighboring non-boundary pixels. With the boundary pixel $b$ and its receptive field $\\mathcal{R}$, the boundary-aware average pooling (BAP) is defined as\nwhere $S^{(i)}_{\\mathcal{R}}$ and $M^{(i)}_{\\mathcal{R}}$ denote the patch of receptive field $\\mathcal{R}$ on $S^{(i)}_{\\mathcal{R}}$ and $M^{(i)}_{\\mathcal{R}}$ , and $(h,w) \\in \\mathcal{R}$ enumerates the pixels in $\\mathcal{R}$. Then, we replace the original value at the boundary pixel $b$ using the newly obtained one. We iteratively apply this process for $n$ times by reducing the boundary width by $\\Delta r = 2$ at each iteration. We also set the size of receptive field $\\mathcal{R}$ as $3 × 3$. In addition, we empirically set the number of iterations n and initial boundary width $r_0$ as 4 and 8.\n接下来，我们在边界像素上应用边界感知平均池化，如图 4 所示。这仅在边界像素上应用平均池化，并使用相邻非边界像素的 SML。 使用边界像素 $b$ 及其感受野 $\\mathcal{R}$，边界感知平均池化 (BAP) 定义为\n\nBAP(S_{\\mathcal{R}}^{(i)},M_{\\mathcal{R}}^{(i)}) = \\dfrac{\\sum_{h,w}S_{\\mathcal{R}}^{(i)}\\times M_{\\mathcal{R}}^{(i)}}{\\sum_{h,w}M_{\\mathcal{R}}^{(i)}} \\tag{7}$S^{(i)}_{\\mathcal{R}}$ 和 $M^{(i)}_{\\mathcal{R}}$ 表示 $S^{(i)}_{\\mathcal{R}}$ 和 $M^{(i)}_{\\mathcal{R}}$ 上的感受野 $\\mathcal{R}$ 的补丁 和 $(h,w) \\in \\mathcal{R}$ 枚举 $\\mathcal{R}$ 中的像素。 然后，我们使用新获得的值替换边界像素 $b$ 处的原始值。 我们通过在每次迭代中将边界宽度减小 $\\Delta r = 2$ 来迭代地应用这个过程 $n$ 次。 我们还将感受野 $\\mathcal{R}$ 的大小设置为 $3 × 3$。 此外，我们根据经验将迭代次数 n 和初始边界宽度 $r_0$ 设置为 4 和 8。\n3.3.2 Dilated smoothing 扩张平滑Since iterative boundary suppression only updates boundary pixels, the irregulars in the non-boundary regions are not addressed. Hence, we address these pixels by smoothing them using the neighboring pixels based on the intuition that the local consistency exists among the pixels in a local region. In addition, if the adjacent pixels used for iterative boundary suppression do not have sufficiently low or high anomaly scores, there may still exist boundary pixels that remain as false positives or false negatives even after the process. In this regard, we broaden the receptive fields of the smoothing filter using dilation [41] to reflect the anomaly scores beyond boundary regions.\n由于迭代边界抑制只更新边界像素，非边界区域中的不规则不被处理。 因此，我们基于局部区域中像素之间存在局部一致性的直觉，通过使用相邻像素对它们进行平滑来处理这些像素。 此外，如果用于迭代边界抑制的相邻像素没有足够低或足够高的异常分数，则即使在处理之后仍可能存在边界像素保持为假阳性或假阴性。 在这方面，我们使用膨胀 [41] 扩大了平滑滤波器的感受野，以反映边界区域之外的异常分数。\n\n\n\n\n\n\n\n\n\n什么是非边界区域中的不规则？ \n会不会处理了小的异常？会，已经在结论部分给出解释，是不足之处。\nFor the smoothing filter, we leverage the Gaussian kernel since it is widely known that the Gaussian kernel removes noises [13]. With a given standard deviation σ and convolution filter size k, the kernel weight $K \\in \\R^{k×k}$ at location $i, j$ is defined as\n对于平滑滤波器，我们利用高斯核，因为众所周知，高斯核可以去除噪声 [13]。 在给定标准偏差 σ 和卷积滤波器大小 k 的情况下，位置 $i, j$ 处的核权重 $K \\in \\R^{k×k}$ 定义为\n\nK_{i,j} = \\frac{1}{2\\pi \\sigma^2}exp(-\\frac{\\Delta i^2 + \\Delta j^2}{2 \\sigma^2}) \\tag{8}where $∆i = i − (k−1)$ and $∆j = j − (k−1)$ are the displacements of location $i, j$ from the center. In our setting, we set the kernel size $k$ and $σ$ to 7 and 1, respectively. Moreover, we empirically set the dilation rate as 6.\n其中 $Δi = i - (k-1)$ 和 $Δj = j - (k-1)$ 是位置 $i, j$ 从中心的位移。 在我们的设置中，我们将内核大小 $k$ 和 $σ$ 分别设置为 7 和 1。 此外，我们根据经验将膨胀率设置为 6。\n4. ExperimentsThis section describes the datasets, experimental setup, and quantitative and qualitative results.\n本节介绍数据集、实验设置以及定量和定性结果。\n4.1. DatasetsFishyscapes Lost &amp; Found [3] is a high-quality image dataset containing real obstacles on the road. This dataset is based on the original Lost &amp; Found [35] dataset. The original Lost &amp; Found is collected with the same setup as Cityscapes [10], which is a widely used dataset in urban- scene segmentation. It contains real urban images with 37 types of unexpected road obstacles and 13 different street scenarios (e.g., different road surface appearances, strong illumination changes, and etc). Fishyscapes Lost &amp; Found further provides the pixel-wise annotations for 1) unexpected objects, 2) objects with pre-defined classes of Cityscapes [10], and 3) void (i.e., objects neither in pre-defined classes nor unexpected objects) regions. This dataset includes a public validation set of 100 images and a hidden test set of 275 images for the benchmarking\nFishyscapes Lost &amp; Found [3] 是一个高质量的图像数据集，其中包含道路上的真实障碍物。 该数据集基于原始的 Lost &amp; Found [35] 数据集。 原始 Lost &amp; Found 的收集设置与 Cityscapes [10] 相同，Cityscapes [10] 是城市场景分割中广泛使用的数据集。 它包含真实的城市图像，包含 37 种意外道路障碍和 13 种不同的街道场景（例如不同的路面外观、强烈的光照变化等） \nFishyscapes Lost &amp; Found 进一步为 1) 意外对象、2) 具有预先定义的 Cityscapes [10] 类的对象和 3) void（既不在预定义类中也不在意外对象中的对象提供了像素级注释 ) 地区。 该数据集包括 100 张图像的公共验证集和 275 张图像的隐藏测试集，用于基准测试\nFishyscapes Static [3] is constructed based on the validation set of Cityscapes [10]. Regarding the objects in the PASCAL VOC [11] as unexpected objects, they are overlaid on the Cityscapes validation images by using various blending techniques to match the characteristics of Cityscapes. This dataset contains 30 publicly available validation samples and 1,000 test images that are hidden for benchmarking.\nFishyscapes Static [3] 是基于 Cityscapes [10] 的验证集构建的。 将 PASCAL VOC [11] 中的对象视为意外对象，通过使用各种混合技术将它们叠加在 Cityscapes 验证图像上以匹配 Cityscapes 的特征。 该数据集包含 30 个公开可用的验证样本和 1,000 个隐藏用于基准测试的测试图像。\nRoad Anomaly [26] contains images of unusual dangers which vehicles confront on roads. It consists of 60 web-collected images with anomalous objects ( animals, rocks, and etc.) on roads with a resolution of 1280 × 720. This dataset is challenging since it contains various driving circumstances such as diverse scales of anomalous objects and adverse road conditions.\nRoad Anomaly [26] 包含车辆在道路上遇到的异常危险的图像。 它由 60 个网络收集的图像组成，在道路上具有异常物体（动物、岩石等），分辨率为 1280 × 720。该数据集具有挑战性，因为它包含各种驾驶环境，例如不同尺度的异常物体和不利的道路条件。\n4.2 Experimental SetupImplementation Details We adopt DeepLabv3+ [7] with ResNet101 [15] backbone for our segmentation architecture with the output stride set to 8. We train our segmentation networks on Cityscapes [10] which is one of the widely used datasets for urban-scene segmentation. We use the same pre-trained network for all experiments.\nEvaluation Metrics For the quantitative results, we compare the performance by the area under receiver operating characteristics (AUROC) and average precision (AP). In addition, we measure the false positive rate at a true positive rate of 95% (FPR95) since the rate of false positives in high-recall areas is crucial for safety-critical applications. For the qualitative analysis, we visualize the prediction results using the threshold at a true positive rate of 95% (TPR95).\nBaselines We compare ours with the various approaches reported in the Fishyscapes leaderboard. We also report results on the Fishyscapes validation sets and Road Anomaly with previous approaches that do not utilize external datasets or require additional training for fair comparisons. Additionally, we compare our method with approaches that are not reported in the Fishyscapes leaderboard. Thus, we include the previous method using max logit [16] and SynthCP [39] that leverages an image resynthesis model for such comparison. Note that SynthCP requires training of additional networks.\n实现细节 我们采用 DeepLabv3+ [7] 和 ResNet101 [15] 主干作为我们的分割架构，输出步幅设置为 8。我们在 Cityscapes [10] 上训练我们的分割网络，这是广泛使用的城市数据集之一 -场景分割。 我们对所有实验使用相同的预训练网络。\n评估指标 对于定量结果，我们按接收器操作特性 (AUROC) 和平均精度 (AP) 下的区域比较性能。 此外，我们以 95% 的真阳性率 (FPR95) 测量误报率，因为高召回率区域的误报率对于安全关键型应用至关重要。 对于定性分析，我们使用阈值以 95% 的真阳性率 (TPR95) 可视化预测结果。\n基线 我们将我们的方法与 Fishyscapes 排行榜中报告的各种方法进行比较。 我们还报告了 Fishyscapes 验证集和 Road Anomaly 的结果，这些方法不使用外部数据集或需要额外培训以进行公平比较。 此外，我们将我们的方法与 Fishyscapes 排行榜中未报告的方法进行比较。 因此，我们使用了之前使用最大 logit [16] 和 SynthCP [39] 的方法，该方法利用图像再合成模型进行此类比较。 请注意，SynthCP 需要训练额外的网络。\n4.3 Evaluation ResultsThis section provides the quantitative and qualitative re- sults. We first show the results on Fishyscapes datasets and Road Anomaly, and then present the comparison results with various backbone networks. Additionally, we report the computational cost and the qualitative results by com- paring with previous approaches.\n本节提供定量和定性结果。 我们首先展示了 Fishyscapes 数据集和 Road Anomaly 的结果，然后展示了与各种骨干网络的比较结果。 此外，我们通过与以前的方法进行比较来报告计算成本和定性结果。\n4.3.1 Comparison on Fishyscapes Leaderboard 排行榜比较Table 1 shows the leaderboard result on the test sets of Fishyscapes Lost &amp; Found and Fishyscapes Static. The Fishyscapes Leaderboard categorizes approaches by check- ing whether they require retraining of segmentation net- works or utilize OoD data. In this work, we add the Ex- tra Network column under the Additional Training category. Extra networks refer to the extra learnable parameters that need to be trained using a particular objective function other than the one for the main segmentation task. Utilizing extra networks may require a lengthy inference time, which could be critical for real-time applications such as autonomous driving. Considering such importance, we add this category for the evaluation.\nAs shown in Table 1, we achieve a new state-of-the-art performance on the Fishyscapes Lost &amp; Found dataset with a large margin, compared to the previous models that do not require additional training of the segmentation network and external datasets. Additionally, we even outperform 6 previous approaches in Fishyscapes Lost &amp; Found and 5 models in Fishyscapes Static which fall into at least one of the two categories. Moreover, as discussed in the previous work [3], retraining the segmentation network with addi- tional loss terms impair the original segmentation perfor- mance(i.e., mIoU) as can be shown in the cases of Bayesian Deeplab [31], Dirichlet Deeplab [29], and OoD Training with void class in Table 1. This result is publicly available on the Fishyscapes benchmark website.\n表 1 显示了 Fishyscapes Lost &amp; Found 和 Fishyscapes Static 测试集的排行榜结果。  Fishyscapes 排行榜通过检查它们是否需要重新训练分割网络或利用 OoD 数据来对方法进行分类。 在这项工作中，我们在 Additional Training 类别下添加了 Extra Network 列。 额外网络是指需要使用特定目标函数训练的额外可学习参数，而不是用于主要分割任务的目标函数。 利用额外的网络可能需要很长的推理时间，这对于自动驾驶等实时应用至关重要。 考虑到这样的重要性，我们添加了这个类别进行评估。\n表 1：与 Fishyscapes 排行榜中报告的先前方法的比较。 模型按 Fishyscapes Lost &amp; Found 测试集中的 AP 分数排序。 我们在不需要对分割网络或 Fishyscapes Lost &amp; Found 数据集的 OoD 数据进行额外训练的方法中实现了新的最先进的性能。 粗体表示其评估指标在方法网络中的最高性能，并且 3)利用 OoD 数据。\n如表 1 所示，与不需要对分割网络和外部数据集进行额外训练的先前模型相比，我们在 Fishyscapes Lost &amp; Found 数据集上实现了新的最先进的性能。 此外，我们甚至超过了 Fishyscapes Lost &amp; Found 中的 6 个先前方法和 Fishyscapes Static 中的 5 个模型，它们至少属于这两个类别中的一个。 此外，正如在之前的工作 [3] 中所讨论的，使用额外的损失项重新训练分割网络会损害原始分割性能（即 mIoU），这可以在贝叶斯 Deeplab [31]、Dirichlet Deeplab 的案例中得到证明 [29]，以及表 1 中带有 void 类的 OoD 训练。该结果可在 Fishyscapes 基准网站上公开获得。\n4.3.2 Comparison on Fishyscapes validation sets and Road AnomalyFor a fair comparison, we compare our method on Fishyscapes validation sets and Road Anomaly with pre- vious approaches which do not require additional training and OoD data. As shown in Table 2, our method outper- forms other previous methods in the three datasets with a large margin. Additionally, our method achieves a signifi- cantly lower FPR95 compared to previous approaches.\n表 2：与 Fishyscapes 验证集和 Road Anomaly 数据集中其他基线的比较。  † 表示结果是使用我们预先训练的主干从官方代码中获得的，∗ 表示该模型需要额外的可学习参数。 请注意，kNN Embedding - Density 的性能由 Fishyscapes [3] 团队提供。\n为了公平比较，我们将 Fishyscapes 验证集和 Road Anomaly 的方法与之前不需要额外训练和 OoD 数据的方法进行比较。 如表 2 所示，我们的方法在三个数据集中以较大的优势优于其他先前的方法。 此外，与以前的方法相比，我们的方法实现了显着降低的 FPR95。\n4.3.3 Qualitative AnalysisFig. 5 visualizes the pixels detected as unexpected objects (i.e., white regions) with the TPR at 95%. While previous approaches using MSP [18] and max logit [16] require nu- merous in-distribution pixels to be detected as unexpected, our method does not. To be more specific, regions that are less confident (e.g., boundary pixels) are detected as un- expected in MSP [18] and max logit [16]. However, our method clearly reduces such false positives which can be confirmed by the significantly reduced number of white regions.\n\n图 5：使用 TPR95 检测到的意外对象。 我们将我们的方法与 MSP [18] 和 max logit [16] 进行比较。 白色像素表示被识别为意外对象的对象。 与这两种方法相比，我们的方法显着减少了误报像素的数量。\n图 5 将检测到的像素可视化为意外物体（即白色区域），TPR 为 95%。 虽然以前使用 MSP [18] 和 max logit [16] 的方法需要将大量分布内像素检测为意外，但我们的方法不需要。 更具体地说，在 MSP [18] 和最大 logit [16] 中检测到的置信度较低的区域（例如边界像素)是出乎意料的。 然而，我们的方法明显减少了这种误报，这可以通过显着减少的白色区域数量来证实。\n5.DiscussionIn this section, we conduct an in-depth analysis on the effects of our proposed method along with the ablation studies.\n在本节中，我们对我们提出的方法以及消融研究的效果进行了深入分析。\n5.1. Ablation StudyTable 3 describes the effect of each proposed method in our work with the Fishyscapes Lost &amp; Found validation set. SML achieves a significant performance gain over using the max logit [16]. Performing iterative boundary suppression on SMLs improves the overall performance (i.e., 4% increase in AP and 1% decrease in FPR95). On the other hand, despite the increase in AP, performing dilated smoothing on SMLs without iterative boundary suppression results in an unwanted slight increase in FPR95. The following is the possible reason for the result. When dilated smoothing is applied without iterative boundary suppression, the anomaly scores of non-boundary pixels may be updated with those of boundary pixels. Since the non-boundary pixels of in-distribution objects have low anomaly scores compared to the boundaries, it may increase false positives. Such an issue is addressed by performing iterative boundary suppression before applying dilated smoothing. After the boundary regions are updated with neighboring non-boundary regions, dilated smoothing increases the overall performance without such error propagation.\n表 3 描述了在我们使用 Fishyscapes Lost &amp; Found 验证集的工作中每种建议方法的效果。 与使用最大 logit [16] 相比，SML 获得了显着的性能提升。 对 SML 执行迭代边界抑制可提高整体性能（即 AP 增加 4%，FPR95 减少 1%）。 另一方面，尽管 AP 增加，但在没有迭代边界抑制的情况下对 SML 执行扩张平滑会导致 FPR95 略有增加。 以下是该结果的可能原因。 当在没有迭代边界抑制的情况下应用扩张平滑时，非边界像素的异常分数可能会随着边界像素的异常分数而更新。 由于分布对象的非边界像素与边界相比具有较低的异常分数，因此可能会增加误报。 通过在应用扩张平滑之前执行迭代边界抑制来解决此类问题。 在使用相邻的非边界区域更新边界区域后，扩张平滑提高了整体性能，而不会出现这种错误传播。\n\n5.2 AnalysisThis section provides an indepth analysis on the effects on segmentation performance, comparison with various backbones, and comparison on computational costs.\n本节深入分析了对分割性能的影响，与各种骨干网的比较，以及计算成本的比较。\n5.2.1 Effects on the segmentation performanceTable 4 shows the mIoU on the Cityscapes validation set with the detection threshold at TPR95. By applying the detection threshold, the segmentation model predicts a nontrivial amount of in-distribution pixels as the unexpected ones. Due to such false positives, the mIoU of all meth- ods decreased from the original mIoU of 80.33%. To be more specific, using MSP [18] and max logit [16] result in significant performance degradation. On the other hand, our approach maintains a reasonable performance of mIoU even with outstanding unexpected obstacle detection perfor- mance. This table again demonstrates the practicality of our work since it both shows reasonable performance in the seg- mentation task and the unexpected obstacle detection task.\n\n\n\n\n\n\n\n\n\n通过mIoU发现：概率置信度预测的方法容易出现假阳性。并展现了本文方法的有效性\n表4显示了检测阈值为TPR95的城市景观验证集上的mIoU。通过应用检测阈值，分割模型将大量的分布内像素预测为意外的像素。由于这种假阳性，所有方法的mIoU从原来的80.33%下降。更具体地说，使用MSP[18]和max logit[16]会导致性能显著下降。另一方面，我们的方法保持了合理的mIoU的性能，甚至有突出的意外障碍物检测性能。这个表格再次证明了我们工作的实用性，因为它同时显示了在隔离任务和意外障碍物检测任务中的合理性能。\n\n5.2.2 Comparison with various backbonesSince our method does not require additional training or extra OoD datasets, our method can be adopted and used easily on any existing pretrained segmentation networks. To verify the wide applicability of our approach, we report the performance of identifying anomalous objects with various backbone networks including MobileNetV2 [37], ShuffleNetV2 [28], and ResNet50 [28]. As shown in Table 5, our method significantly outperforms the other approaches [18, 16] using the same backbone network with a large improvement in AP. This result clearly demonstrates that our method is applicable widely regardless of the backbone network.\n由于我们的方法不需要额外的训练或额外的OoD数据集，我们的方法可以很容易地在任何现有的预训练的分割网络上采用和使用。为了验证我们方法的广泛适用性，我们报告了用各种骨干网络识别异常对象的性能，包括MobileNetV2[37]、ShuffleNetV2[28]和ResNet50[28]。如表5所示，我们的方法在使用相同的骨干网络时明显优于其他方法[18, 16]，在AP方面有很大的改进。这一结果清楚地表明，无论骨干网络如何，我们的方法都能广泛适用。\n\n5.2.3 Comparison on computational costTo demonstrate that our method requires a negligible amount of computation cost, we report GFLOPs (i.e., the number of floating-point operations used for computation) and the inference time. As shown in Table 6, our method requires only a minimal amount of computation cost regarding both GFLOPs and the inference time compared to the original segmentation network, ResNet-101 [15]. Also, among several studies which utilize additional networks, we compare with a recently proposed approach [39] that leverages an image resynthesis model. Our approach requires substantially less amount of computation cost compared to SynthCP [39].\n为了证明我们的方法需要的计算成本可以忽略不计，我们报告了GFLOPs（即用于计算的浮点运算次数）和推理时间。如表6所示，与原始分割网络ResNet-101[15]相比，我们的方法在GFLOPs和推理时间方面都只需要极少的计算成本。另外，在一些利用额外网络的研究中，我们与最近提出的利用图像再合成模型的方法[39]进行比较。与SynthCP[39]相比，我们的方法需要的计算成本大大减少。\n5.3. Effects of Standardized Max LogitTable 7 describes how SML enables applying iterative boundary suppression and dilated smoothing. Applying iterative boundary suppression and dilated smoothing on other approaches does not improve the performance or even aggravates in the cases of MSP [18] and max logit [16]. On the other hand, it significantly enhances the performance when applied to SML. The following are the possible reasons for such observation. As aforementioned, the overconfidence of the softmax layer elevates the MSPs of anomalous objects. Since the MSPs of anomalous objects and in-distribution objects are not distinguishable enough, applying iterative boundary suppression and dilated smoothing may not improve the performance.\n表7描述了SML如何实现迭代边界抑制和扩张平滑的应用。在其他方法上应用迭代边界抑制和扩张平滑并不能提高性能，甚至在MSP[18]和max logit[16]的情况下会恶化。另一方面，当应用于SML时，它明显提高了性能。以下是这种观察的可能原因。如前所述，softmax层的过度自信提高了异常对象的MSP。由于异常对象的MSP和分布中的对象没有足够的区别，应用迭代边界抑制和扩张平滑可能不会提高性能。\n\nAdditionally, iterative boundary suppression and dilated smoothing require the values to be scaled since it performs certain computations with the values. In the case of using max logits, the values of each predicted class differ accord- ing to the predicted class. Performing the iterative bound- ary suppression and dilated smoothing in such a case aggra- vates the performance because the same max logit values in different classes represent different meanings according to their predicted class. SML aligns the differently formed dis- tributions of max logits which enables to utilize the values of neighboring pixels with certain computations.\n此外，迭代边界抑制和扩张平滑需要对数值进行缩放，因为它对数值进行了某些计算。在使用最大对数的情况下，每个预测类的值根据预测类的不同而不同。在这种情况下，执行迭代边界抑制和扩张平滑会提高性能，因为不同类别中相同的最大对数值根据其预测类别代表不同的意义。SML对不同的最大对数分布进行了调整，这使得在某些计算中可以利用邻近像素的值。\n6. ConclusionsIn this work, we proposed a simple yet effective method for identifying unexpected obstacles on roads that do not require external datasets or additional training. Since max logits have their own ranges in each predicted class, we aligned them via standardization, which improves the performance of detecting anomalous objects. Additionally, based on the intuition that pixels in a local region share local semantics, we iteratively suppressed the boundary regions and re- moved irregular pixels that have distinct values compared to neighboring pixels via dilated smoothing. With such a straightforward approach, we achieved a new state-of-the-art performance on Fishyscapes Lost &amp; Found benchmark. Additionally, extensive experiments with diverse datasets demonstrate the superiority of our method to other previous approaches. Through the visualizations and in-depth analysis, we verified our intuition and rationale that standardizing max logit and considering the local semantics of neighbor- ing pixels indeed enhance the performance of identifying unexpected obstacles on roads. However, there still remains room for improvements; 1) dilated smoothing might remove unexpected obstacles that are as small as noises, and 2) the performance depends on the distribution of max logits ob- tained from the main segmentation networks.\n在这项工作中，我们提出了一个简单而有效的方法来识别道路上的意外障碍物，该方法不需要重新引用外部数据集或额外的训练。由于最大对数在每个预测类中都有自己的范围，我们通过标准化将其对齐，这提高了检测异常物体的性能。此外，基于局部区域的像素共享局部语义的直觉，我们迭代地抑制了边界区域，并通过扩张平滑重新移动了与相邻像素相比具有明显价值的不规则像素。通过这种直接的方法，我们在Fishyscapes Lost &amp; Found基准测试中取得了最先进的性能。此外，在不同的数据集上进行的广泛实验证明了我们的方法比以前的其他方法更有优势。通过可视化和深入分析，我们验证了我们的直觉和理由，即标准化的最大对数和考虑相邻像素的局部语义确实提高了识别道路上意外障碍物的性能。然而，仍有改进的余地；1）扩张平滑可能会去除像噪音一样小的意外障碍物；2）性能取决于从主要分割网络中获得的最大对数的分布。\nWe hope our work inspires the following researchers to investigate such practical methods for identifying anomalous objects in urban-scene segmentation which is crucial in safety-critical applications.\n我们希望我们的工作能激励以下研究人员研究这种实用的方法，以识别城市场景分割中的异常物体，这对安全关键应用至关重要。\n","slug":"SML","date":"2022-05-06T17:03:26.000Z","categories_index":"","tags_index":"深度学习,异常分割","author_index":"Star"},{"id":"082602022df518a2faf6c62dcd303051","title":"一类嵌入的反向蒸馏","content":"Anomaly Detection via Reverse Distillation from One-Class Embedding通过一类嵌入的反向蒸馏进行异常检测\n\nAbstractKnowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. \nHowever, using similar or identical architectures to build the teacher and student models in previous stud- ies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective “reverse distillation” paradigm accordingly. \nInstead of receiving raw images directly, the student network takes teacher model’s one-class embedding as input and targets to restore the teacher’s multi-scale representations. \nInherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. \nIn addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. \nThe obtained compact embedding effectively preserves essential information on normal patterns, but abandons anomaly perturbations. \nExtensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach’s effectiveness and generalizability.\n\n知识蒸馏（KD）在无监督异常检测（AD）的挑战性问题上取得了可喜的成果。 师生（T-S）模型中异常的表示差异为AD提供了必要的证据。 \n然而，在以前的研究中使用相似或相同的架构来构建教师和学生模型阻碍了异常表示的多样性。 为了解决这个问题，我们提出了一种由教师编码器和学生解码器组成的新型 T-S 模型，并相应地引入了一种简单而有效的“逆向蒸馏”范式。\n学生网络不是直接接收原始图像，而是将教师模型的一类嵌入作为输入和目标，以恢复教师的多尺度表示。 \n本质上，本研究中的知识蒸馏从抽象的高级表示开始到低级特征。 \n此外，我们在 T-S 模型中引入了可训练的一类瓶颈嵌入 (OCBE) 模块。 \n获得的紧凑嵌入有效地保留了正常模式的基本信息，但放弃了异常扰动。 \n对 AD 和一类新颖性检测基准的广泛实验表明，我们的方法超越了 SOTA 性能，证明了我们提出的方法的有效性和普遍性。\n1. IntroductionAnomaly detection (AD) refers to identifying and localizing anomalies with limited, even no, prior knowledge of abnormality. \nThe wide applications of AD, such as indus- trial defect detection [3], medical out-of-distribution detection [50], and video surveillance [24], makes it a critical task as well as a spotlight. In the context of unsupervised AD, no prior information on anomalies is available. Instead, a set of normal samples is provided for reference. \nTo tackle this problem, previous efforts attempt to construct various self-supervision tasks on those anomaly-free samples. These tasks include, but not limited to, sample reconstruction [2, 5, 11, 16, 26, 34, 38, 48], pseudo-outlier augmentation [23, 42, 46], knowledge distillation [4, 33, 39], etc.\nIn this study, we tackle the problem of unsupervised anomaly detection from the knowledge distillation-based point of view. \nIn knowledge distillation (KD) [6, 15], knowledge is transferred within a teacher-student (T-S) pair. In the context of unsupervised AD, since the student experiences only normal samples during training, it is likely to generate discrepant representations from the teacher when a query is anomalous. This hypothesis forms the basis of KD-based methods for anomaly detection. \nHowever, this hypothesis is not always true in practice due to \n(1) the identical or similar architectures of the teacher and student networks (i.e., non-distinguishing filters [33]) \n(2) the same data flow in the T-S model during knowledge trans- fer/distillation. \nThough the use of a smaller student network partially addresses this issue [33, 39], the weaker represen- tation capability of shallow architectures hinders the model from precisely detecting and localizing anomalies.\n\n异常检测 (AD) 是指在对异常的先验知识有限甚至没有的情况下识别和定位异常。  \nAD 的广泛应用，如工业缺陷检测 [3]、医疗分布外检测 [50] 和视频监控 [24]，使其成为一项关键任务和聚光灯。 在无监督 AD 的背景下，没有关于异常的先验信息可用。 相反，提供了一组正常样本以供参考。 \n为了解决这个问题，以前的努力试图在那些无异常的样本上构建各种自我监督任务。 这些任务包括但不限于样本重建[2、5、11、16、26、34、38、48]、伪异常值增强[23、42、46]、知识蒸馏[4、33、  39]等。\n在这项研究中，我们从基于知识蒸馏的角度解决了无监督异常检测的问题。 \n在知识蒸馏 (KD) [6, 15] 中，知识在师生 (T-S) 对中转移。 在无监督 AD 的背景下，由于学生在训练期间只体验到正常样本，因此当查询异常时，它可能会从教师那里产生不一致的表示。 该假设构成了基于 KD 的异常检测方法的基础。 然而，这个假设在实践中并不总是正确的，因为 \n\n教师和学生网络的相同或相似架构（即非区分过滤器 [33]）\nT-S 模型中的相同数据流 知识转移/蒸馏。 \n\n尽管使用较小的学生网络部分解决了这个问题 [33, 39]，但浅层架构较弱的表示能力阻碍了模型精确检测和定位异常。\n\nTo holistically address the issue mentioned above, we propose a new paradigm of knowledge distillation, namely Reverse Distillation, for anomaly detection. We use sim- ple diagrams in Fig. 2 to highlight the systematic differ- ence between conventional knowledge distillation and the proposed reverse distillation. First, unlike the conventional knowledge distillation framework where both teacher and student adopt the encoder structure, the T-S model in our reverse distillation consists of heterogeneous architectures: a teacher encoder and a student decoder. Second, instead of directly feeding the raw data to the T-S model simulta- neously, the student decoder takes the low-dimensional em- bedding as input, targeting to mimic the teacher’s behavior by restoring the teacher model’s representations in different scales. From the regression perspective, our reverse distil- lation uses the student network to predict the representa- tion of the teacher model. Therefore, ”reverse” here indi- cates both the reverse shapes of teacher encoder and stu- dent decoder and the distinct knowledge distillation order where high-level representation is first distilled, followed by low-level features. It is noteworthy that our reverse distilla- tion presents two significant advantages: i) Non-similarity structure. In the proposed T-S model, one can consider the teacher encoder as a down-sampling filter and the stu- dent decoder as an up-sampling filter. The ”reverse struc- tures” avoid the confusion caused by non-distinguishing fil- ters [33] as we discussed above. ii) Compactness embed- ding. The low-dimensional embedding fed to the student decoder acts as an information bottleneck for normal pat- tern restoration. Let’s formulate anomaly features as pertur- bations on normal patterns. Then the compact embedding helps to prohibit the propagation of such unusual perturba- tions to the student model and thus boosts the T-S model’s representation discrepancy on anomalies. Notably, tradi- tional AE-based methods [5, 11, 16, 26] detect anomalies utilising pixel differences, whereas we perform discrimi- nation with dense descriptive features. Deep features as region-aware descriptors provide more effective discrimi- native information than per-pixel in images.\n\n\n\n\n\n\n\n\n\n为了全面解决上述问题，我们提出了一种新的知识蒸馏范式，即反向蒸馏，用于异常检测。 我们使用图 2 中的简单图表来突出传统知识蒸馏和提出的逆向蒸馏之间的系统差异。 首先，与教师和学生都采用编码器结构的传统知识蒸馏框架不同，我们的逆向蒸馏中的 T-S 模型由异构架构组成：教师编码器和学生解码器。 其次，学生解码器不是直接将原始数据同时馈送到 T-S 模型，而是将低维嵌入作为输入，旨在通过恢复教师模型在不同尺度上的表示来模仿教师的行为。 从回归的角度来看，我们的反向蒸馏使用学生网络来预测教师模型的表示。 因此，这里的“反向”表示教师编码器和学生解码器的反向形状以及不同的知识蒸馏顺序，其中首先蒸馏高级表示，然后是低级特征。 值得注意的是，我们的逆向蒸馏具有两个显着优势：i）非相似性结构。 在提出的 T-S 模型中，可以将教师编码器视为下采样滤波器，将学生解码器视为上采样滤波器。 正如我们上面讨论的，“反向结构”避免了由非区分过滤器[33]引起的混淆。  ii) 紧凑性嵌入。 馈送到学生解码器的低维嵌入充当了正常模式恢复的信息瓶颈。 让我们将异常特征表述为对正常模式的扰动。 然后紧凑嵌入有助于禁止这种不寻常的扰动传播到学生模型，从而提高 T-S 模型对异常的表示差异。 值得注意的是，传统的基于 AE 的方法 [5、11、16、26] 利用像素差异检测异常，而我们使用密集的描述性特征进行区分。 作为区域感知描述符的深度特征比图像中的每个像素提供更有效的判别信息。\nIn addition, since the compactness of the bottleneck em- bedding is vital for anomaly detection (as discussed above), we introduce a one-class bottleneck embedding (OCBE) module to condense the feature codes further. Our OCBE module consists of a multi-scale feature fusion (MFF) block and one-class embedding (OCE) block, both jointly opti- mized with the student decoder. Notably, the former aggre- gates low- and high-level features to construct a rich embed- ding for normal pattern reconstruction. The latter targets to retain essential information favorable for the student to de- code out the teacher’s response. We perform extensive experiments on public bench- marks. The experimental results indicate that our re- verse distillation paradigm achieves comparable perfor- mance with prior arts. The proposed OCBE module further improves the performance to a new state-of-the-art (SOTA) record. Our main contributions are summarized as follows:  \n\nWe introduce a simple, yet effective Reverse Distilla- tion paradigm for anomaly detection. The encoder- decoder structure and reverse knowledge distillation strategy holistically address the non-distinguishing fil- ter problem in conventional KD models, boosting the T-S model’s discrimination capability on anomalies.\n\nWe propose a one-class bottleneck embedding mod- ule to project the teacher’s high-dimensional features to a compact one-class embedding space. This inno- vation facilitates retaining rich yet compact codes for anomaly-free representation restoration at the student.\n\nWe perform extensive experiments and show that our approach achieves new SOTA performance.\n\n\n\n\n\n\n\n\n\n\n此外，由于瓶颈嵌入的紧凑性对于异常检测至关重要（如上所述），我们引入了一类瓶颈嵌入（OCBE）模块来进一步压缩特征代码。 我们的 OCBE 模块由多尺度特征融合 (MFF) 块和一类嵌入 (OCE) 块组成，两者都与学生解码器联合优化。 值得注意的是，前者聚合了低级和高级特征以构建用于正常模式重建的丰富嵌入。 后者的目标是保留有利于学生解码教师反应的基本信息。    我们在公共基准上进行了广泛的实验。 实验结果表明，我们的反向蒸馏范式实现了与现有技术相当的性能。 所提出的 OCBE 模块进一步将性能提高到新的最先进 (SOTA) 记录。 我们的主要贡献总结如下： \n\n我们为异常检测引入了一种简单而有效的逆向蒸馏范式。 编码器-解码器结构和反向知识蒸馏策略整体解决了传统 KD 模型中的非区分过滤器问题，提高了 T-S 模型对异常的判别能力。\n\n我们提出了一类瓶颈嵌入模块，将教师的高维特征投影到紧凑的一类嵌入空间。 这项创新有助于保留丰富而紧凑的代码，以便在学生处进行无异常表示恢复。\n我们进行了广泛的实验并表明我们的方法实现了新的 SOTA 性能。\n\n\n\n2. Related WorkThis section briefly reviews previous efforts on unsuper- vised anomaly detection. We will highlight the similarity and difference between the proposed method and prior arts. Classical anomaly detection methods focus on defining a compact closed one-class distribution using normal sup- port vectors. The pioneer studies include one-class support vector machine (OC-SVM) [35] and support vector data description (SVDD) [36]. To cope with high-dimensional data, DeepSVDD [31] and PatchSVDD [43] estimate data representations through deep networks. Another unsupervised AD prototype is the use of gener- ative models, such as AutoEncoder (AE) [19] and Genera- tive Adversarial Nets (GAN) [12], for sample reconstruc- tion. These methods rely on the hypothesis that genera- tive models trained on normal samples only can success- fully reconstruct anomaly-free regions, but fail for anoma- lous regions [2, 5, 34]. However, recent studies show that deep models generalize so well that even anomalous re- gions can be well-restored [46]. To address this issue, memory mechanism [11, 16, 26] , image masking strat- egy [42, 46] and pseudo-anomaly [28, 45] are incorporated in reconstruction-based methods. However, these meth- ods still lack a strong discriminating ability for real-world anomaly detection [3, 5]. Recently, Metaformer (MF) [40] proposes the use of meta-learning [9] to bridge model adap- tation and reconstruction gap for reconstruction-based ap- proaches. Notably, the proposed reverse knowledge distil- lation also adopts the encoder-decoder architecture, but it differs from construction-based methods in two-folds. First, the encoder in a generative model is jointly trained with the decoder, while our reverse distillation freezes a pre-trained model as the teacher. Second, instead of pixel-level recon- struction error, it performs anomaly detection on the seman- tic feature space.\n\n\n\n\n\n\n\n\n\n本节简要回顾了以前在无监督异常检测方面的努力。 我们将强调所提出的方法与现有技术之间的相似之处和不同之处。    经典的异常检测方法侧重于使用正态支持向量定义紧凑的封闭一类分布。 先驱研究包括一类支持向量机（OC-SVM）[35]和支持向量数据描述（SVDD）[36]。 为了处理高维数据，DeepSVDD [31] 和 PatchSVDD [43] 通过深度网络估计数据表示。    另一个无监督的 AD 原型是使用生成模型，例如自动编码器 (AE) [19] 和生成对抗网络 (GAN) [12]，用于样本重建。 这些方法依赖于这样一个假设，即在正常样本上训练的生成模型只能成功地重建无异常区域，但对于异常区域则失败 [2, 5, 34]。 然而，最近的研究表明，深度模型的泛化能力非常好，即使是异常区域也可以很好地恢复 [46]。 为了解决这个问题，记忆机制[11、16、26]、图像掩蔽策略[42、46]和伪异常[28、45]被纳入基于重建的方法中。 然而，这些方法对于现实世界的异常检测仍然缺乏很强的辨别能力[3, 5]。 最近，Metaformer (MF) [40] 提出使用元学习 [9] 来弥合基于重建的方法的模型适应和重建差距。 值得注意的是，所提出的反向知识蒸馏也采用了编码器-解码器架构，但它与基于构造的方法有两方面的不同。 首先，生成模型中的编码器与解码器联合训练，而我们的逆向蒸馏将预先训练的模型冻结为教师。 其次，它不是像素级的重建错误，而是对语义特征空间进行异常检测。\nData augmentation strategy is also widely used. By adding pseudo anomalies in the provided anomaly-free samples, the unsupervised task is converted to a supervised learning task [23, 42, 46]. However, these approaches are prone to bias towards pseudo outliers and fail to detect a large variety of anomaly types. For example, CutPaste [23] generates pseudo outliers by adding small patches onto nor- mal images and trains a model to detect these anomalous regions. Since the model focuses on detecting local fea- tures such as edge discontinuity and texture perturbations, it fails to detect and localize large defects and global struc- tural anomalies as shown in Fig. 6.Recently, networks pre-trained on the large dataset are proven to be capable of extracting discriminative features for anomaly detection [7,8,23,25,29,30]. With a pre-trained model, memorizing its anomaly-free features helps to iden- tify anomalous samples [7, 29]. The studies in [8, 30] show that using the Mahalanobis distance to measure the simi- larity between anomalies and anomaly-free features leads to accurate anomaly detection. Since these methods re- quire memorizing all features from training samples, they are computationally expensive.Knowledge distillation from pre-trained models is an- other potential solution to anomaly detection. In the con- text of unsupervised AD, since the student model is ex- posed to anomaly-free samples in knowledge distillation, the T-S model is expected to generate discrepant features on anomalies in inference [4,33,39]. To further increase the discrimnating capability of the T-S model on various types of abnormalities, different strategies are introduced. For in- stance, in order to capture multi-scale anomaly, US [4] en- sembles several models trained on normal data at different scales, and MKD [33] propose to use multi-level features alignment. It should be noted that though the proposed method is also based on knowledge distillation, our reverse distillation is the first to adopt an encoder and a decoder to construct the T-S model. The heterogeneity of the teacher and student networks and reverse data flow in knowledge distillation distinguishes our method from prior arts.\n\n\n\n\n\n\n\n\n\n数据增强策略也被广泛使用。 通过在提供的无异常样本中添加伪异常，将无监督任务转换为监督学习任务 [23,42,46]。 然而，这些方法容易偏向伪异常值，并且无法检测到多种异常类型。 例如，CutPaste [23] 通过在正常图像上添加小块来生成伪异常值，并训练模型来检测这些异常区域。 由于该模型侧重于检测局部特征，例如边缘不连续性和纹理扰动，因此无法检测和定位大缺陷和全局结构异常，如图 6 所示。   最近，在大型数据集上预训练的网络被证明能够提取用于异常检测的判别特征 [7,8,23,25,29,30]。 使用预训练模型，记住其无异常特征有助于识别异常样本 [7, 29]。  [8, 30] 中的研究表明，使用马氏距离来测量异常和无异常特征之间的相似性可以实现准确的异常检测。 由于这些方法需要记住训练样本的所有特征，因此它们的计算成本很高。   来自预训练模型的知识蒸馏是异常检测的另一个潜在解决方案。 在无监督 AD 的背景下，由于学生模型在知识蒸馏中暴露于无异常样本，因此 T-S 模型预计会在推理异常上产生差异特征 [4,33,39]。 为了进一步提高 T-S 模型对各类异常的判别能力，引入了不同的策略。 例如，为了捕获多尺度异常，US [4] 集成了几个在不同尺度的正常数据上训练的模型，MKD [33] 建议使用多级特征对齐。 需要注意的是，虽然所提出的方法也是基于知识蒸馏的，但我们的逆向蒸馏是第一个采用编码器和解码器来构建 T-S 模型的方法。 教师和学生网络的异质性以及知识蒸馏中的反向数据流将我们的方法与现有技术区分开来。\n","slug":"一类嵌入反向蒸馏","date":"2022-05-04T03:30:00.000Z","categories_index":"","tags_index":"深度学习,异常分割","author_index":"Star"},{"id":"4346091fabb18c4f718acfd51c087897","title":"Pytorch & python","content":"pytorch参数 &amp; 命令行 &amp; 辅助loggerlogger模块解释 —— CSDNlogger使用案例\nlogging模块是Python内置的标准模块，主要用于输出运行日志，可以设置输出日志的等级、日志保存路径、日志文件回滚等\nyacs.configyacs使用 —— 知乎\nyacs库，用于为一个系统构建config文件\n需要创建CN()这个作为容器来装载我们的参数，这个容器可以嵌套\n设备相关torch.cuda.synchronize()等待当前设备上所有流中的所有核心完成。\n🌰：测试时间的代码\n# code 1\nstart = time.time()\nresult = model(input)\nend = time.time()\n\n# code 2\ntorch.cuda.synchronize()\nstart = time.time()\nresult = model(input)\ntorch.cuda.synchronize()\nend = time.time()\n\n# code 3\nstart = time.time()\nresult = model(input)\nprint(result)\nend = time.time()\n代码2是正确的。因为在pytorch里面，程序的执行都是异步的。如果采用代码1，测试的时间会很短，因为执行完end=time.time()程序就退出了，后台的cu也因为python的退出退出了。如果采用代码2，代码会同步cu的操作，等待gpu上的操作都完成了再继续成形end = time.time()\n代码3和代码2的时间是类似的。因为代码3会等待gpu上的结果执行完传给print函数，所以时间就和代码2同步的操作的时间基本上是一致的了。将print(result)换成result.cpu()结果是一致的。\n数据加载图像数据变换transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\nNormalize是把图像数据从[0,1]变成[-1,1]，变换公式是image=(image-mean)/std，那么其中的参数就分别是三个通道的mean和std，这个均值和标准差需要自己计算，范围就是训练集和验证集的所有图像。\nDataLoaderCSDN原文链接collate_fn参数使用详解 —— 知乎num_works参数 —— CSDN\n\n加载一个batch的数据这一步需要使用一个torch.utils.data.DataLoader对象，并且DataLoader是一个基于某个dataset的iterable，这个iterable每次从dataset中基于某种采样原则取出一个batch的数据。也可以这样说：Torch中可以创建一个torch.utils.data.==Dataset==对象，并与torch.utils.data.==DataLoader==一起使用，在训练模型时不断为模型提供数据。\ntorch.utils.data.DataLoader\n定义：Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.构造函数:torch.utils.data.DataLoader(dataset, \n\t\t\t\t\t\t\tbatch_size=1, \n\t\t\t\t\t\t\tshuffle=False, \n\t\t\t\t\t\t\tsampler=None,\n\t\t\t\t\t\t\tbatch_sampler=None, num_workers=0, collate_fn=None,\n\t\t\t\t\t\t\tpin_memory=False, drop_last=False, timeout=0,\n\t\t\t\t\t\t\tworker_init_fn=None)\n\ndataset: 抽象类,包含两种类型\nmap-style datasets\niterable-style datasets\n\n\nbatch_size : 每一次抽样的batch-size大小\nshuffle : True则随机打乱数据\nNum_works：将batch加载进RAM的进程数。内存开销大，CPU负担大。可能之后几次迭代的数据在本次迭代的时候已经加载进内存。\ncollate_fn：如何取样本的，我们可以定义自己的函数来准确地实现想要的功能。\ndrop_last：告诉如何处理数据集长度除于batch_size余下的数据。True就抛弃，否则保留。\n\nMap-style datasets\n是一个类，要求有 __getitem__()and__len__()这两个构造函数，代表一个从索引映射到数据样本。\n\n__getitem__(): 根据索引index遍历数据\n__len__(): 返回数据集的长度\n可编写独立的数据处理函数\n在 __getitem()__ 函数中进行调用\n直接将数据处理函数写在 __getitem()__ 或者 __init()__ 函数中，但是__getitem()__必须根据==index==返回响应的值，该值会通过index传到dataloader中进行后续的batch批处理。\n\n\n\n基本需要满足：def __getitem__(self, index):\n    return self.src[index], self.trg[index]\n\ndef __len__(self):\n\treturn len(self.src)  \ngetitem()方法用来从datasets中读取一条数据，这条数据包含训练图片（已CV距离）和标签，参数index表示图片和标签在总数据集中的Index。\nlen()方法返回数据集的总长度（训练集的总数）。\n实现 MyDatasets 类\n\n简单直白\n\n把 x 和 label 分别装入两个列表 self.src 和 self.trg ，然后通过 getitem(self, idex) 返回对应元素import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n \nclass My_dataset(Dataset):\n    def __init__(self):\n        super().__init__()\n        ## 使用sin函数返回10000个时间序列,如果不自己构造数据，就使用numpy,pandas等读取自己的数据为x即可。\n        ## 以下数据组织这块既可以放在init方法里，也可以放在getitem方法里\n        self.x = torch.randn(1000,3)\n        self.y = self.x.sum(axis=1)\n        self.src,  self.trg = [], []\n        for i in range(1000):\n            self.src.append(self.x[i])\n            self.trg.append(self.y[i])\n    \n           \n    def __getitem__(self, index):\n        return self.src[index], self.trg[index]\n\n    def __len__(self):\n        return len(self.src) \n        \n ## 或者return len(self.trg), src和trg长度一样\n \ndata_train = My_dataset()\ndata_test = My_dataset()\ndata_loader_train = DataLoader(data_train, batch_size=5, shuffle=False)\ndata_loader_test = DataLoader(data_test, batch_size=5, shuffle=False)\n## i_batch的多少根据batch size和def __len__(self)返回的长度确定\n## batch_data返回的值根据def __getitem__(self, index)来确定\n## 对训练集：(不太清楚enumerate返回什么的时候就多print试试)\nfor i_batch, batch_data in enumerate(data_loader_train):\n    print(i_batch)  ## 打印batch编号\n    print(batch_data[0])  ## 打印该batch里面src\n    print(batch_data[1])  ## 打印该batch里面trg\n## 对测试集：（下面的语句也可以）\nfor i_batch, (src, trg) in enumerate(data_loader_test):\n    print(i_batch)  ## 打印batch编号\n    print(src)  ## 打印该batch里面src的尺寸\n    print(trg)  ## 打印该batch里面trg的尺寸    \n生成的data_train可以通过 data_train[xxx]直接索引某个元素，或者通过next(iter(data_train)) 得到一条条的数据。\n\n借助TensorDataset将数据包装成dataset\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n \nsrc = torch.sin(torch.arange(1, 1000, 0.1))\ntrg = torch.cos(torch.arange(1, 1000, 0.1))\n \ndata = TensorDataset(src, trg)\ndata_loader = DataLoader(data, batch_size=5, shuffle=False)\nfor i_batch, batch_data in enumerate(data_loader):\n    print(i_batch)  ## 打印batch编号\n    print(batch_data[0].size())  ## 打印该batch里面src\n    print(batch_data[1].size())  ## 打印该batch里面trg\n\n地址读取，生成数据的路径 txt文件\n\nimport os\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport matplotlib.image as mpimg\n\n\n\n## 对所有图片生成path-label map.txt 这个程序可根据实际需要适当修改\ndef generate_map(root_dir):\n\t##得到当前绝对路径\n    current_path = os.path.abspath('.')\n    ##os.path.dirname()向前退一个路径\n    father_path = os.path.abspath(os.path.dirname(current_path) + os.path.sep + \".\")\n\n    with open(root_dir + 'map.txt', 'w') as wfp:\n        for idx in range(10):\n            subdir = os.path.join(root_dir, '%d/' % idx)\n            for file_name in os.listdir(subdir):\n                abs_name = os.path.join(father_path, subdir, file_name)\n                ## linux_abs_name = abs_name.replace(\"\\\\\", '/')\n                wfp.write('&#123;file_dir&#125; &#123;label&#125;\\n'.format(file_dir=linux_abs_name, label=idx))\n\n## 实现MyDatasets类\nclass MyDatasets(Dataset):\n\n    def __init__(self, dir):\n        ## 获取数据存放的dir\n        ## 例如d:/images/\n        self.data_dir = dir\n        ## 用于存放(image,label) tuple的list,存放的数据例如(d:/image/1.png,4)\n        self.image_target_list = []\n        ## 从dir--label的map文件中将所有的tuple对读取到image_target_list中\n        ## map.txt中全部存放的是d:/.../image_data/1/3.jpg 1 路径最好是绝对路径\n        with open(os.path.join(dir, 'map.txt'), 'r') as fp:\n            content = fp.readlines()\n            ##s.rstrip()删除字符串末尾指定字符（默认是字符）\n            ## 得到 [['d:/.../image_data/1/3.jpg', '1'], ...,]\n            str_list = [s.rstrip().split() for s in content]\n            ## 将所有图片的dir--label对都放入列表，如果要执行多个epoch，可以在这里多复制几遍，然后统一shuffle比较好\n            self.image_target_list = [(x[0], int(x[1])) for x in str_list]\n\n    def __getitem__(self, index):\n        image_label_pair = self.image_target_list[index]\n        ## 按path读取图片数据，并转换为图片格式例如[3,32,32]\n        ## 可以用别的代替\n        img = mpimg.imread(image_label_pair[0])\n        return img, image_label_pair[1]\n\n    def __len__(self):\n        return len(self.image_target_list)\n\n\nif __name__ == '__main__':\n    ## 生成map.txt\n    ## generate_map('train/')\n\n    train_loader = DataLoader(MyDatasets('train/'), batch_size=128, shuffle=True)\n\n    for step in range(20000):\n        for idx, (img, label) in enumerate(train_loader):\n            print(img.shape)\n            print(label.shape)\n网络搭建Trickwith torch.no_grad()\n\n\n\n\n\n\n\n\n参考：https://blog.csdn.net/sazass/article/details/116668755\n作用：在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。当requires_grad设置为False时,反向传播时就不会自动求导了，因此大大节约了显存或者说内存。\n基本功能函数torch.max()torch.max(input) → Tensor:返回输入tensor中所有元素的最大值\ntorch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor): 按维度dim 返回最大值，并且返回索引。\ntorch.max()[0]， 只返回最大值的每个数\n\ntroch.max()[1]， 只返回最大值的每个索引\n\ntorch.max()[1].data 只返回variable中的数据部分（去掉Variable containing:）\n\ntorch.max()[1].data.numpy() 把数据转化成numpy ndarry\n\ntorch.max()[1].data.numpy().squeeze() 把数据条目中维度为1 的删除掉\npythonstr.lower() 全部转化为小写字母\n","slug":"python&pytorch使用指南","date":"2022-05-03T12:10:00.000Z","categories_index":"","tags_index":"","author_index":"Star"},{"id":"cca6975b74b1218e11c5b9ba5de4d5ef","title":"DMLNet","content":"\n\n开放世界语义分割  \n\n开集语义分割模块   \n闭集语义分割子模块   \n异常分割子模块 \n\n\n增量小样本学习模块\n\n我是短小精悍的文章摘要(๑•̀ㅂ•́) ✧\n\nCODEmultiscale 是自己设定的吗 cfg.DATASET.imgSizes = (300, 375, 450, 525, 600)\nSeg 转化为long Tensor的目的是什么\ncolors的作用是什么\n几个辅助函数的作用：\nNormalization(x): $\\dfrac{x - min(x)}{max(x) - min(x)}$ \nCoefficient_map(x, thre): $\\dfrac{1}{1 + exp(50*(x - thre))}$\nnormfun(x, mu, sigma):$\\dfrac{exp(-\\frac{(x - mu)^2}{2  \\sigma^2})}{\\sigma  \\sqrt{2*\\pi}}$\n论文阅读引言Classical close-set semantic segmentation networks have limited ability to detect out-of-distribution (OOD) objects, which is important for safety-critical applications such as autonomous driving. Incrementally learning these OOD objects with few annotations is an ideal way to enlarge the knowledge base of the deep learning models. In this paper, we propose an open world semantic segmenta- tion system that includes two modules: \n(1) ==an open-set semantic segmentation module to detect both in-distribution and OOD objects==.\n(2) an incremental few-shot learning module to gradually incorporate those OOD objects into its existing knowledge base. \nThis open world semantic segmentation system behaves like a human being, which is able to identify OOD objects and gradually learn them with corresponding supervision. \nWe adopt the ==Deep Metric Learning Network (DMLNet) with contrastive clustering== to implement open-set semantic segmentation. Compared to other open-set semantic segmentation methods, our DMLNet achieves state-of-the-art performance on three challenging open-set semantic segmentation datasets without using additional data or generative models. \nOn this basis, two incremental few-shot learning methods are fur- ther proposed to progressively improve the DMLNet with the annotations of OOD objects\n\n经典的闭集语义分割网络检测分布外 (OOD) 对象的能力有限，这对于自动驾驶等安全关键型应用很重要。 增量学习这些带有少量注释的 OOD 对象是扩大深度学习模型知识库的理想方法。 在本文中，我们提出了一个开放世界语义分割系统，包括两个模块：\n(1) 一个开放集语义分割模块，用于检测内分布和OOD对象。  \n(2) 一个增量的小样本学习模块，逐渐将这些 OOD 对象纳入其现有的知识库。 \n这个开放世界的语义分割系统就像一个人，能够识别OOD对象并在相应的监督下逐渐学习它们。 \n我们采用具有==对比聚类的深度度量学习网络（DMLNet）==来实现开放集语义分割。 与其他开放集语义分割方法相比，我们的 DMLNet 在三个具有挑战性的开放集语义分割数据集上实现了最先进的性能，而无需使用额外的数据或生成模型。 \n在此基础上，进一步提出了两种增量少样本学习方法，通过 OOD 对象的注释逐步改进 DMLNet\n6. ConclusionWe introduce an open world semantic segmentation system which incorporates two modules: \n\nan open-set segmentation module \nan incremental few-shot learning module.\n\nOur proposed open-set segmentation module is based on the deep metric learning network, and it uses the Euclidean distance sum criterion to achieve state-of-the-art performance. \nTwo incremental few-shot learning methods are proposed to broaden the perception knowledge of the network. Both modules of the open world semantic segmentation system can be further studied to improve the performance. We hope our work can draw more researchers to contribute to this practically valuable research direction.\n\n我们介绍了一个开放世界语义分割系统，它包含两个模块：一个开放集分割模块和一个增量小样本学习模块。 \n我们提出的开放集分割模块基于深度度量学习网络，它使用==欧几里德距离和标准==来实现最先进的性能。 \n提出了两种增量少样本学习方法来拓宽网络的感知知识。 开放世界语义分割系统的两个模块都可以进一步研究以提高性能。 我们希望我们的工作能够吸引更多的研究人员为这个具有实际价值的研究方向做出贡献\n1. 介绍得益于高质量的数据集 [3,4,5]，深度卷积网络在语义分割任务 [1, 2] 中取得了巨大成功。 这些语义分割网络在许多应用中被用作感知系统，如自动驾驶[6]、医疗诊断[7]等。然而，这些感知系统中的大多数都是闭集和静态的。 闭集语义分割假设测试中的所有类都已经在训练期间参与，这在开放世界中是不正确的。 如果闭集系统错误地将分发中标签分配给 OOD 对象 [8]，它可能会在安全关键型应用程序（如自动驾驶）中造成灾难性后果。 同时，静态感知系统无法根据所见内容更新其知识库，因此，它仅限于特定场景，需要在一定时间后重新训练。 为了解决这些问题，我们提出了一种开放集的动态感知系统，称为开放世界语义分割系统。 它包含两个模块：  \n（1）一个开放集语义分割模块，用于检测OOD对象并将正确的标签分配给分布中的对象。\n  (2) 一个增量的小样本学习模块，将这些未知对象逐步合并到其现有的知识库中。\n  我们提出的开放世界语义分割系统的整个流程如图 1 所示  \n开放集语义分割和增量小样本学习都没有得到很好的解决。 \n对于开集语义分割，最重要的部分是在一张图像的所有像素中识别OOD像素，称为异常分割。 异常分割的典型方法是将图像级的开集分类方法应用于像素级的开集分类。 \n这些方法包括基于不确定性估计的方法 [9, 10, 11, 12] 和基于自动编码器的方法 [13, 14]。 然而，这两种方法已被证明在驾驶场景中无效，因为基于不确定性估计的方法==会给出许多假阳性异常值检测== [15] 并且自动编码器==无法重新生成复杂的城市场景== [16]。 最近，基于生成对抗网络（基于 GAN）的方法 [16, 17] 已被证明是有效的，但它们远==非轻量级==，因为它们需要在管道中使用多个深度网络。 \n对于增量少样本学习，我们不仅要处理增量学习的挑战，例如灾难性遗忘[18]，还要处理少样本学习的挑战，包括从少量样本中提取代表性特征[19]  \n在本文中，我们建议使用 DMLNet 来解决开放世界语义分割问题。 原因有三：  \n(1) DMLNet的分类原理是基于对比聚类，可以有效识别异常物体，如图2所示\n\n\n\n\n\n\n\n\n\n度量学习：从数据中学习一种度量数据对象间距离的方法。其目标是使得在学得的距离度量下，相似对象间的距离小，不相似对象间的距离大。\n传统的度量学习方法只能学习出线性特征，虽然有一些能够提取非线性特征的核方法被提出，但对学习效果也没有明显提升\n深度度量学习：深度学习的激活函数学习非线性特征的优秀能力，深度学习方法能够自动地从原始数据中学出高质量的特征。因此深度学习的网络结构与传统的度量学习方法相结合能够带来理想的效果。\n(2) DMLNet结合原型非常适合few-shot 任务[19]。    \n(3) DMLNet 的增量学习可以通过添加新的原型来实现，这是一种自然而有用的方法 [20]。\n 基于 DMLNet 架构，我们为开放集语义分割模块开发了两种未知识别标准，为增量少样本学习模块开发了两种方法。\n   根据我们的实验，这两个模块都被验证为有效且轻量级的。 总而言之，我们的贡献如下：    \n\n我们率先推出开放世界语义分割系统，在实际应用中更加稳健实用。\n我们提出的基于 DMLNet 的开放集语义分割模块在三个具有挑战性的数据集上实现了最先进的性能。\n我们提出的few-shot 增量学习模块方法在很大程度上缓解了灾难性遗忘问题。\n通过结合我们提出的开放集语义分割模块和增量少样本学习模块，实现了一个开放世界语义分割系统。\n\n2. Related Work2.1 异常语义分割异常语义分割的方法可以分为两种趋势：  基于不确定性估计的方法和基于生成模型的方法。\n不确定性估计的基线是最大softmax概率（MSP），它首先在[9]中提出。  Dan 等人没有使用 softmax 概率。  [11]提出使用最大logit（MaxLogit）并取得更好的异常分割性能。 贝叶斯网络采用深度学习网络的概率观点，所以它们的权重和输出是概率分布而不是特定的数字 [21, 22]。 在实践中，Dropout [10] 或集成 [12] 通常用于近似贝叶斯推理。 \n自动编码器（AE）[23, 13] 和 RBM [14] 是典型的生成方法，假设 OOD 图像的重建误差大于分布内图像  \n最近，另一种基于 GAN 再合成的生成模型被证明可以基于其可靠的高分辨率像素到像素转换结果实现最先进的性能。  SynthCP [17] 和 DUIR [16] 是基于 GAN 再合成的两种方法。 不幸的是，它们离轻量级还很远，因为必须依次使用两个或三个神经网络来进行 OOD 检测。 \n与它们相比，我们证明了基于对比聚类的 DMLNet 具有更好的异常分割性能，而只需要推理一次  \n2.2 深度度量学习网络DMLNets 已用于多种应用，包括视频理解 [24] 和人员重新识别 [25]。  DMLNet 使用欧几里得、马氏距离或 Matusita 距离 [26] 将此类问题转换为计算度量空间中的嵌入特征相似度。\n卷积原型网络和 DMLNets 通常一起用于解决特定问题，例如检测图像级 OOD 样本 [27、28、29] 和用于语义分割的小样本学习 [19、30、31]。 我们也按照这种组合构建了第一个用于开放世界语义分割的 DMLNet  \n2.3 开放世界分类和检测开放世界分类首先由 [32] 提出。这项工作提出了最近非异常值 (NNO) 算法，该算法在增量添加对象类别、检测异常值和管理开放空间风险方面非常有效。    最近约瑟夫等人。  [33]提出了一种基于对比聚类、未知感知提议网络和基于能量的未知识别标准的开放世界对象检测系统。 我们的开放世界语义分割系统的管道与他们的相似，除了两个重要的区别使我们的任务更具挑战性：（1）在他们的开放集检测模块中，他们依赖于区域提议网络（RPN）是 类不可知，因此也可以检测到未标记的潜在 OOD 对象。 这样，OOD样本的信息对于训练是有效的。 但是，我们专注于语义分割，其中训练中使用的每个像素都被分配了一个分布内标签，因此不能将 OOD 样本添加到训练中。  (2) 在增量学习模块中，他们使用新类的所有标记数据，而我们专注于自然更困难的少样本条件。 很少有研究集中在增量小样本学习上，其中包括用于分类的增量小样本学习[34]、对象检测[35]和语义分割[36]\n3. 开放世界语义分割在本节中，我们给出了开放世界语义分割系统的工作流程。 该系统由一个开放集语义分割模块和一个增量小样本学习模块组成。 假设$\\mathcal{C}_{in} = \\{\\mathcal{C}_{in,1}, \\mathcal{C}_{in,2},…,\\mathcal{C}_{in,N} \\}$  是 N 个分布内的类，它们都在训练数据集中进行了注释，并且 $\\mathcal{C}_{out} = \\{\\mathcal{C}_{out,1},\\mathcal{C}_{out,2},…,\\mathcal{C}_{out,M} \\}$ 是训练数据集中没有遇到的 M 个 OOD 类  \n开集语义分割模块又分为两个子模块：闭集语义分割子模块和异常分割子模块。 \n\n$\\hat{Y}^{close}$ 是闭集语义分割子模块的输出图，所以每个像素的类别 $\\hat{Y}^{close}_{i,j} ∈ C_{in}$。 \n异常分割子模块的功能是识别OOD像素，其输出称为异常概率图：$\\hat{P} \\in [1,0]^{H \\times W}$，其中 $H$ 和 $W$ 表示输入图像的高度和宽度。 \n\n基于 $\\hat{Y}_{close}$ 和 $\\hat{P}$，开集语义分割图 $\\hat{Y}^{open}$ 给出为:\n\n\\hat{Y}^{open}_{i,j} = \n\\begin{cases}\n\\mathcal{C}_{anomaly} \\quad \\     \\hat{P}_{i,j} > \\lambda_{out} \\\\\n\\hat{Y}_{i,j}^{close} \\quad \\quad \\hat{P}_{i,j} \\le \\lambda_{out}\n\\end{cases} \\tag{1}$\\mathcal{C}_{anomaly}$ ：表示 OOD 类别$λ_{out}$ ：确定 OOD 像素的阈值。\n因此，openset语义分割模块应该识别OOD像素并分配正确的分布标签。然后 Yopen 可以转发给可以从 $C_{out}$ 中识别 $C_{anomaly}$ 并给出新类的相应注释的标注者  增量少样本学习模块用于在有新标签时将近集分割子模块的知识库从 $C_{in}$ 一个一个更新为 $C_{in+M}$，其中 $C_{in+t} = Cin \\cup \\{C_{out,1},C_{out,2},…,C_{out,t}\\},t ∈{1,2,…,M}$。 图 1 显示了开放世界语义分割系统的循环工作流水线 \n图 1. 开放世界语义分割系统。 第 1 步：识别已知和未知对象（蓝色箭头）。 第 2 步：注释未知对象（红色箭头）。 第 3 步：应用增量少样本学习来增加网络的分类范围（绿色箭头）。 第 4 步：在增量少样本学习之后，DMLNet 可以在更大的域中输出结果（紫色箭头）。\n4. 方法我们采用 DMLNet 作为我们的特征提取器，并在 4.1 节讨论架构和损失函数。 开放集分割模块和增量少样本学习模块在 4.2 和 4.3 节中进行了说明\n4.1 深度度量学习网络\n\n\n\n\n\n\n\n\nClassical CNN-based semantic segmentation networks can be disentangled into two parts: \n\na feature extractor $f(X;θ_f)$ for obtaining the embedding vector of each pixel \na classifier $g(f(X;θ_f);θ_g)$ for generating the decision boundary, \n\nwhere $X$, $θ_f$ and $θ_g$ denote the input image, parameters of the feature extractor and classifier respectively. \nThis learnable classifier is not suitable for OOD detection because it assigns all feature space to known classes and leaves no space for OOD classes. \n传统CNN-based语义分割网络：\n\n$f(X;\\theta_f)$ 特征提取器：获取每个像素的嵌入向量\n$g(f(X;\\theta_f);\\theta_g)$ 分类器：生成决策边界\n\n这种==可学习的分类器不适用于 OOD 检测==，因为它将所有特征空间分配给已知类，并且没有为 OOD 类留下空间。 \n\n\n\n\n\n\n\n\n\nIn contrast, the classifier is replaced by the Euclidean distance representation with all prototypes $\\mathcal{M}_{in} = \\{ m_t \\in \\mathbb{R}^{1 \\times N}|t \\in \\{1,2,…,N\\}  \\}$ in DMLNet, where $m_t$ refers to the prototype of class $\\mathcal{C}_{in,t}$. The feature extractor $f(X;θ_f)$ learns to map the input $X$ to the feature vector which has the same length as the prototype in metric space. For the close-set segmentation task, the probability of one pixel $X_{i,j}$ belonging to the class $\\mathcal{C}_{in,t}$ is formulated as:\nDMLNet 中, ==所有原型的欧几里得距离==表示代替了传统的可学习分类器\n\n$m_t$ 指的是 $\\mathcal{C}_{in,t}$ 类的原型。\n\n特征提取器 $f(X;θ_f)$学习将输入 X 映射到与度量空间中的原型长度相同的特征向量。 \n对于闭集分割任务，一个像素 $X_{i,j}$ 属于类 $\\mathcal{C}_{in,t}$ 的概率公式为： \n\np_t(X_{i,j}) = \\frac{exp(-||f(X;\\theta_f)_{i,j} - m_t||^2)}{\\sum^N_{t'=1} exp(-||f(X;\\theta_f)_{i,j} - m_{t'}||^2)} \\tag{2}基于这种基于欧几里德距离的概率，判别交叉熵 (DCE) 损失函数 $\\mathcal{L}_{DCE}(X_{i,j},Y_{i,j};θ_f,M_{in})$ [27] 定义为:\n\n\\mathcal{L}_{DCE} = \\sum_{i,j} -log (\\frac{exp(-||f(X;\\theta_f)_{i,j} - m_{Y_{i,j}}||^2)}{\\sum^N_{k=1} exp(-||f(X;\\theta_f)_{i,j} - m_{k}||^2)} \\tag{3}$Y$：输入图像 $X$ 的标签$\\mathcal{L}_{DCE}$ 的分子和分母分别指图2中的吸引力和排斥力。 \n\n\n\n\n\n\n\n\n\n排斥力不需要除去本身所属的类，本身类的原型吗？\n图 2. DMLNet 的对比聚类。 在推理过程中，已知对象将被同一类的原型所吸引，而被剩余的原型所排斥。 最后，它们将围绕特定的原型进行聚合。 相反，异常对象将被所有原型排斥，因此它们将聚集在度量空间的中间。\n\n我们制定了另一个损失函数，称为方差损失 (VL) 函数 $\\mathcal{L}_{VL}(X_{i,j},Y_{i,j};θ_f,M_{in})$，其定义为：\n\n\\mathcal{L}_{VL} = \\sum_{i,j} ||f(X;\\theta_f)_{i,j} - m_{Y_{i,j}}||^2 \\tag{4}$\\mathcal{L}_{VL}$ 只有吸引力作用，没有排斥力作用。 \n使用 DCE 和 VL，混合损失定义为：$\\mathcal{L}= \\mathcal{L}_{DCE} + λ_{VL}\\mathcal{L}_{VL}$，其中 $λ_{VL}$ 是权重参数\n4.2 开集语义分割模型开集语义分割模块由闭集语义分割子模块和异常分割子模块组成。 开放集语义分割模块的流程如 图3 所示。\n图3.闭集分割子模块包含在蓝色虚线框内，异常分割子模块包含在红色虚线框内。 开集分割图是这两个子模块生成的结果的组合。 在开放集分割图中预测分布内类和 OOD 类。  EDS map 和 MMSP map 的定义请参考 4.2 节。\n\n闭集语义分割子模块为一幅图像的所有像素分配分布标签。 由于一个像素 $X_{i,j}$ 属于类 $\\mathcal{C}_{in,t}$ 的概率是用公式 2 表示，闭集分割图为：\n\n\\hat{Y}_{i,j}^{close} = argmax_t \\ p_t(X_{i,j}) \\tag{5}\n异常分割子模块检测OOD像素。 我们提出了两个未知的识别标准来测量异常概率，包括_基于度量的最大softmax概率（MMSP）_和_欧几里得距离和（EDS）_。 \n\n以下是基于 MMSP 的异常概率：\n\n\\hat{P}^{MMSP}_{i,j} = 1 - max \\ p_t(X_{i,j}),\\ t \\in \\{ 1,2,3...,N \\} \\tag{6}\nEDS 是根据以下发现提出的：如果特征位于 OOD 像素聚集的度量空间的中心，则与所有原型的欧几里得距离和更小，即==异常的欧几里得距离较小==。  EDS 定义为：\n\nS(X_{i,j}) = \\sum_{t=1}^N ||f(X;\\theta_f)_{i,j} - m_t||^2 \\tag{7}基于 EDS 的异常概率计算如下：\n\n\\hat{P}^{EDS}_{i,j} = 1- \\frac{S(X_{i,j})}{maxS(X)} \\tag{8}\n\n\n\nEDS 是类独立的，因此所有类的原型应该均匀分布在度量空间中，并且在训练期间不移动。 ==可学习的原型会在训练期间导致不稳定，并且对更好的性能没有贡献== [37]。 因此，我们以 one-hot 向量形式定义原型：只有 $m_t$ 的第 t 个元素是 $T$，而其他元素保持为零，其中 t ∈ {1,2,…,N}\n\n\n\n\n\n\n\n\n\nPAnS是什么情况？\nEDS 是相对于所有像素之间的最大距离和的比率，即使图像中没有 OOD 对象，高异常分数区域肯定存在于每幅图像中。 此外，每个分布内类别的距离总和分布彼此略有不同，如图4所示。\n将MMSP与EDS相结合，以抑制那些实际上处于分布状态的具有中间响应的像素。\n混合函数为： \n\n\\hat{P} = \\alpha \\hat{P}^{EDS} + (1-\\alpha)\\hat{P}^{MMSP} \\tag{9}\nα ：\n\n\\alpha = \\frac{1}{1 + exp(-\\beta(\\hat{P}^{EDS} - \\gamma))} \\tag{10}\nβ 和 γ 是控制抑制效果和阈值的超参数。\n\n\n\n通过方程 9 得到异常概率图和方程 5 得到闭集分割图后，我们应用方程 1 生成最终的开集分割图\n5. 实验Our experiments are divided into three parts. \n\nWe first evaluate our open-set semantic segmentation approach in Section 5.1. \nThen we demonstrate our incremental few-shot learning results in Section 5.2. \nBased on the open-set semantic segmentation module and incremental few-show learning module, the whole open world semantic segmentation is realized in Section 5.3.\n\n5.1 开集语义分割 数据集。 三个数据集包括 StreetHazards [11]、Lost and Found [38] 和 Road Anomaly [16] 用于证明我们基于 DMLNet 的开放集语义分割方法的稳健性和有效性。  \n\nStreetHazards 的大多数异常物体是大型稀有运输机器，例如直升机、飞机和拖拉机。 \nLost and Found 含许多小的异常物品，如货物、玩具和盒子。  \nRoad Anomaly 数据集不再限制城市场景中的场景，还包含村庄和山脉的图像。\n指标。 开放集语义分割是封闭集分割和异常分割的组合，如 4.2 节所述。 \n\n对于闭集语义分割任务，我们使用 mIoU 来评估性能。\n\n对于异常分割任务，根据 [11] 使用三个指标，包括 ROC 曲线下面积（AUROC）、95% 召回的误报率（FPR95）和精确召回曲线下面积（AUPR）。\n\n实施细节。 \n\n对于 StreetHazards，我们遵循与 [11] 相同的训练程序，在 StreetHazards 的训练集上训练 PSPNet [2]。 \n\n\n\n\n\n\n\n\n\n[11]: Scaling out-of-distribution detection for real-world settings.\n\n对于Lost and Found和 Road Anomaly，我们按照 [16] 使用 BDD-100k [39] 来训练 PSPNet。 请注意，PSPNet 仅用于提取我们在 4.1 节中讨论的特征（获得每个像素的嵌入向量）。 混合损失的 $λ_{VL}$ 为 0.01。   所有原型中非零元素 $T$ 为 3。等式 10 中的 β 和 γ 分别为 20 和 0.8。\n\n\n\n\n\n\n\n\n\n[16]: Detecting the unexpected via image resynthesis\n\n\n基线。 \n\nStreetHazards:  MSP [9]、Dropout [10]、AE [13]、MaxLogit [11] 和 SynthCP [17]。 \nLost and Found 和 Road Anomaly:  MSP、MaxLogit、Ensemble [12]、RBM [14] 和 DUIR [16]。\n\n结果。\nStreetHazards 的结果如表 1 所示。\n对于 Lost and Found 和 Road Anomaly，mIoU 是无效的，因为它们只提供 OOD 类标签，但没有特定的分布内类标签。 结果在表 2 中。\n我们的实验表明：\n\n基于 DMLNet 的方法在所有三个异常分割相关指标中都达到了最先进的性能。 \n与最近提出的基于 GAN 的方法（包括 DUIR 和 SynthCP）相比，我们的方法在异常分割质量方面优于它们，结构更轻量级，因为它们在整个流程中需要两个或三个深度神经网络，而我们只需要推理一次。  \nStreetHazards 中闭集分割的 mIoU 值表明我们的方法对闭集分割没有危害。\n\n一些定性结果如图 8 所示\n消融研究。 我们仔细进行了消融实验，研究了不同损失函数（VL 和 DCE）和异常判断标准（EDS 和 MMSP）的影响，如表 3 所示。\n\nDCE 在 mIoU 上的性能优于 VL 的事实表明了 排斥力。  \nEDS 在所有损失下都优于 MMSP 函数，这意味着==与类无关的标准更适合于异常分割任务==   \n\n","slug":"DMLNet","date":"2022-05-03T12:07:00.000Z","categories_index":"","tags_index":"深度学习,异常分割,度量学习","author_index":"Star"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\n\nQuick StartCreate a new post$ hexo new \"My New Post\"\nMore info: Writing\nRun server$ hexo server\nMore info: Server\nGenerate static files$ hexo generate\nMore info: Generating\nDeploy to remote sites$ hexo deploy\nMore info: Deployment\n","slug":"hello-world","date":"2022-05-03T10:31:15.256Z","categories_index":"","tags_index":"","author_index":"Star"}]